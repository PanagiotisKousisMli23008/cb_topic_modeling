While Vehicular Ad-hoc Network (VANET) is developed to enable effective vehicle communication and traffic information exchange, VANET is also vulnerable to different security attacks, such as DOS attacks. The usage of an intrusion detection system (IDS) is one possible solution for preventing attacks in VANET. However, dealing with a large amount of vehicular data that keep growing in the urban environment is still an critical challenge for IDSs. This paper, therefore, proposes a new machine learning model to improve the performance of IDSs by using Random Forest and a posterior detection based on coresets to improve the detection accuracy and increase detection efficiency. The experimental results show that the proposed machine learning model can significantly enhance the detection accuracy compared to classical application of machine learning models.
With the advent of state-of-the-art models based on Neural Networks, the need for vast corpora of accurately labeled data has become fundamental. However, building such datasets is a very resource-consuming task that additionally requires domain expertise. The present work seeks to alleviate this limitation by proposing an interactive semi-automatic annotation tool using an incremental learning approach to reduce human effort. The automatic models used to assist the annotation are incrementally improved based on user corrections to better annotate the next data. To demonstrate the effectiveness of the proposed method, we build a dataset with named entities and relations between them related to the crime field with the help of the tool. Analysis results show that annotation effort is considerably reduced while still maintaining the annotation quality compared to fully manual labeling.
This study explores the internal dynamics and networks of terrorist groups in cyberspace-in particular, Al-Qaeda and the Islamic State of Iraq and al-Sham (ISIS). Using a Global Cyberterrorism Dataset that features data on cyberterror attacks between 2011 and 2016, this research analyzes these two terrorist groups through the lens of a cyber-conflict theory that integrates conflict theory with Jaishankar's space transition theory. Through a network analysis methodology, we examine the invisible relationships and connections between the national origins and target countries of cyberterror attacks. The analysis focuses on the networks of national origins of terrorists and victims, network structures of Al-Qaeda and ISIS actors, and clustering networks of Al-Qaeda and ISIS cyberterrorists. Results indicate that terror in cyberspace is ubiquitous, more flexible than traditional terrorism, and that cyberattacks mostly occurred within the countries of origin. We conclude by discussing the complex features of cyberterror networks and identify some of the geostrategic implications of the divergent cyber strategies adopted by Al-Qaeda and ISIS.
"The subject of the study is the state and prospects of development of legal and economic foundations of regulation of legal relations in the sphere of national security of Ukraine. Methodology. In the process of research were used general scientific and special legal methods. Dialectical method made it possible to compare the level of legal protection of private and public interests as a type of national interests in comparison with the objects of national security. By means of analysis were determined quantitative and qualitative parameters of measures implemented within the framework of ensuring national security of Ukraine in the sphere of protected national interests of legal and economic nature. Synthesis provided the formation of common features that mediate the existence of the main categories in the sphere of national security, taking into account the time period in which the relevant measures are implemented. Comparative-legal method allowed us to identify common and distinctive features in the implementation of measures of legal and economic nature to ensure national security at different times and in the future. The purpose of the article is to establish the state and determine the prospects of development of legal and economic provision of regulation of social relations in the sphere of national security, taking into account geopolitical and internal challenges of the corresponding period of the existence of Ukrainian statehood. The results of the study showed that the state of legal and economic security of national security of Ukraine needs to be adjusted and improved taking into account the needs of the time in the geopolitical space, which is reflected in the organizational, legal and legislative work of the Verkhovna Rada of Ukraine and the research of scientists. Conclusion. The legal and economic foundations of Ukraine's national security developed under the influence of a number of factors, among which the determining ones are: international standards; creation of a legal regulation mechanism harmonizing private and public interests as part of national interests; prompt response to external and internal challenges, creation of the most effective conditions for the existence of civil society, including economic, involvement of institutions of democratic civil control in this activity. It is established that the formation of legal and economic security of national security of Ukraine took place in three stages: the initial period, when the general rules of normalization of legal relations in this area were laid down; the period of unification and systematization of normative regulations in this sphere, taking into account the proportionality of public and private national interests included in the content of national security, with the priority of the latter, in particular in the field of economic security; the period of adaptation of the regulatory framework in the field of national security to the geopolitical challenges that Ukraine faced after the Revolution of Dignity. The defining features of these stages are characterized. It is established that the legal regulation and economic provision of national security of Ukraine today is determined by the following features: in the first place among the national interests are the state sovereignty, territorial integrity, democratic constitutional order, which are associated with a real threat to these interests due to the existing aggression; a number of strategic documents in the relevant areas of national security, the content of which contains a qualification of threats, directions of state policy and appropriate response measures, in particular to ensure economic security, were categorically defined; the legal status of subjects of national security has been normalized and significantly expanded; the content of the powers and functions of these subjects is reduced, and the content of the activities of citizens and public organizations is ignored; for the first time elements of democratic civil control in the sphere of national security are introduced into the legal field. Analyzed modern scientific developments on the national security of Ukraine, which allowed to highlight promising directions of development of measures of legal and economic nature aimed at ensuring national security, including the prevention of threats in the sphere of influence of geopolitical vectors of the modern world system on the formation of Ukrainian statehood, preventing the impact of negative factors of internal and external origin, in particular: separatism; cybercrime; corruption; economic and financial threats; energy crisis; COVID-19 pandemics, etc. In addition, a characteristic feature of such measures should be their ex-ordinary effectiveness, which is mediated by the involvement of all branches of government and institutions of civil society through deep coordination of such activities, the use of international experience, cooperation with international organizations of a military-economic type, the dissemination of the jurisprudence of international courts, using the example of the European Court of Human Rights, etc."
Hands-on cybersecurity training allows students and professionals to practice various tools and improve their technical skills. The training occurs in an interactive learning environment that enables completing sophisticated tasks in full-fledged operating systems, networks, and applications. During the training, the learning environment allows collecting data about trainees' interactions with the environment, such as their usage of command-line tools. These data contain patterns indicative of trainees' learning processes, and revealing them allows to assess the trainees and provide feedback to help them learn. However, automated analysis of these data is challenging. The training tasks feature complex problem-solving, and many different solution approaches are possible. Moreover, the trainees generate vast amounts of interaction data. This paper explores a dataset from 18 cybersecurity training sessions using data mining and machine learning techniques. We employed pattern mining and clustering to analyze 8834 commands collected from 113 trainees, revealing their typical behavior, mistakes, solution strategies, and difficult training stages. Pattern mining proved suitable in capturing timing information and tool usage frequency. Clustering underlined that many trainees often face the same issues, which can be addressed by targeted scaffolding. Our results show that data mining methods are suitable for analyzing cybersecurity training data. Educational researchers and practitioners can apply these methods in their contexts to assess trainees, support them, and improve the training design. Artifacts associated with this research are publicly available.
With the advancing digitization of our society, network security has become one of the critical concerns for most organizations. In this paper, we present CopAS, a system targeted at Big Data forensics analysis, allowing network operators to comfortably analyze and correlate large amounts of network data to get insights about potentially malicious and suspicious events. We demonstrate the practical usage of CopAS for insider attack detection on a publicly available PCAP dataset and show how the system can be used to detect insiders hiding their malicious activity in the large amounts of data streams generated during the operations of an organization within the network.
This study examines videos produced by the al-Hayat Media Center, a branch of the Islamic State's (IS) larger media campaign aimed more specifically at Western audiences. Using a thematic analysis approach, recurring themes of 10 al-Hayat videos were identified with conclusions made regarding the specificities of the message and the target audience. It was found that al-Hayat videos cater to potential Western recruits and sympathizers by portraying life in the IS as spiritually and existentially fulfilling, while simultaneously decrying the West as secular, immoral, and criminal. By utilizing well-produced propaganda videos that tap into the dissatisfactions of Western Muslims, al-Hayat was shown to deliver a sophisticated and legitimate message that may play a role in the larger radicalization process.
Side-Channel Analysis (SCA) requires the detection of the specific time frame Cryptographic Operations (COs) take place in the side-channel signal. Under laboratory conditions with full control over the Device under Test (DuT), dedicated trigger signals can be implemented to indicate the start and end of COs. For real-world scenarios, waveform-matching techniques have been established which compare the side-channel signal with a template of the CO's pattern in real time to detect the CO in the side channel. State-of-the-Art approaches describe implementations based on Field-Programmable Gate Arrays (FPGAs). However, the maximal length of the template is restricted by the resources available on an FPGAs. Particularly, for high sampling rates the recording of an entire CO may need more samples than the maximum template length supported by a waveform-matching system. Consequently, the template has to be reduced such that it fits the resources while still containing all features relevant for detecting the COs via waveform matching. In this paper, we introduce a generic interval-matching technique which provides several degrees of freedom for fine-tuning it to the statistical deviations of waveform measurements of COs. Moreover, we introduce a novel calibration method that finds the best parameters automatically based on statistical analysis of training data. Furthermore, we investigate a technique to reduce the number of features used for the interval matching by utilizing machine-learning-based feature extraction to find the most important samples in a template. Finally, we evaluate the state-of-the-art interval matching and our expansions during calibration and during the application on a test set. The results show, that a reliable reduction to 10% of the original template size is possible with a reduction method from literature for our example. However, the combination of our proposed methods can reliably work with only 15% of the original size and is less volatile than the state-of-the-art approach for reducing the number of features.
Case law analysis is a significant component of research on almost any legal issue and understanding which agents are involved and mentioned in a decision is integral part of the analysis. In this paper we present a first experiment in detecting mentions of different agents in court decisions automatically. We defined a light-weight and easily extensible hierarchy of agents that play important roles in the decisions. We used the types from the hierarchy to annotate a corpus of US court decisions. The resulting data set enabled us to test the hypothesis that the mentions of agents in the decisions could be detected automatically. Conditional random fields models trained on the data set were shown to be very promising in this respect. To support research in automatic case-law analysis we release the agent mentions data set with this paper.
In this work we address the main issues of IT consumerisation that are related to security risks, and propose a 'soft' mitigation strategy for user actions based on nudging, widely applied to health and social behaviour influence. In particular, we propose a complementary, less strict, more flexible Information Security policies, based on risk assessment of device vulnerabilities and threats to corporate data and devices, combined with a strategy of influencing security behaviour by nudging. We argue that nudging, by taking into account the context of the decision-making environment, and the fact that the employee may be in better position to make a more appropriate decision, may be more suitable than strict policies in situations of uncertainty of security-related decisions. (C) 2014 The Authors. Published by Elsevier Ltd.
The U.S., Israel, China, and Iran are funding acts of hacker aggression, and the targets are as good as sitting ducks [GRI 13]. This statement describes the current reality in cyberspace. The activity in which a certain country or organization attacks other countries or organizations has long been topical, but unnoticed by the general public, which gives us the feeling that these are just isolated cases and that individuals who point out the burning issues are too pessimistic regarding warfare in cyberspace. However, cyberwarfare occurs constantly, and many reports published in recent months even recount of a war between countries.
The preceding decade has seen an explosive growth in illicit online commerce, and scholars are increasingly occupied with the study of illicit products, their buyers and sellers, and the platforms and markets in which they are found. Despite sharing one basic characteristic, the exchange of illicit goods and services, there is extensive variation between illicit online markets: Some markets employ currencies, others are based on barter. Some markets exhibit strong social control, others appear anarchic. Traditional theoretical explanations to the organization of illicit markets, which assume that optimization or evolution drives their organization, find it difficult to explain these variations. I propose a typology of illicit online markets that appreciates this heterogeneity and uses it as a theoretical point of departure. Drawing on social control theory and economic sociology, I argue that illicit online markets can be separated across two axes: administrative governance and marketness. This conceptual framework is not prone to functionalist inference and instead implies a less deterministic understanding of how illicit online markets come to be. In turn, this framework encourages the classification and comparison of individual markets, as well as comparative analyses of marketplaces.
In common law jurisdictions, legal research often involves an analysis of relevant case law. Court opinions comprise several high-level parts with different functions. A statement's membership in one of the parts is a key factor influencing how the statement should be understood. In this paper we present a number of experiments in automatically segmenting court opinions into the functional and the issue specific parts. We defined a set of seven types including Background, Analysis, and Conclusions. We used the types to annotate a sizable corpus of US trade secret and cyber crime decisions. We used the data set to investigate the feasibility of recognizing the parts automatically. The proposed framework based on conditional random fields proved to be very promising in this respect. To support research in automatic case law analysis we plan to release the data set to the public.
Side-Channel Analysis (SCA) requires the detection of the specific time frame within which Cryptographic Operations (COs) take place in the side-channel signal. In laboratory conditions with full control over the Device under Test (DuT), dedicated trigger signals can be implemented to indicate the start and end of COs. For real-world scenarios, waveform-matching techniques have been established which compare the side-channel signal with a template of the CO's pattern in real time to detect the CO in the side channel. State-of-the-art approaches are implemented on Field-Programmable Gate Arrays (FPGAs). However, current waveform-matching designs process the samples from Analog-to-Digital Converters (ADCs) sequentially and can only work with low sampling rates due to the limited clock speed of FPGAs. This makes it increasingly difficult to apply existing techniques on modern DuTs that operate with clock speeds in the GHz range. In this paper, we present a parallel waveform-matching architecture that is capable of performing waveform matching at the speed of fast ADCs. We implement the proposed architecture in a high-end FPGA-based digitizer and deploy it to detect AES COs from the side channel of a single-board computer operating at 1 GHz. Our implementation allows for waveform matching at 10 GS/s with high accuracy, thus offering a speedup of 50x compared to the fastest state-of-the-art implementation known to us.
Telecommunications wiretaps are commonly used by law enforcement in criminal investigations. While phone-based wiretapping has seen considerable success, the same cannot be said for Internet taps. Large portions of intercepted Internet traffic are often encrypted, making it difficult to obtain useful information. The advent of the Internet of Things further complicates network wiretapping. In fact, the current level of complexity of intercepted network traffic is almost at the point where data cannot be analyzed without the active involvement of experts. Additionally, investigations typically focus on analyzing traffic in chronological order and predominately examine the data content of the intercepted traffic. This approach is overly arduous when the amount of data to be analyzed is very large. This chapter describes a novel approach for analyzing large amounts of intercepted network traffic based on traffic metadata. The approach significantly reduces the analysis time and provides useful insights and information to non-technical investigators. The approach is evaluated using a large sample of network traffic data.
Implanted microchips can store users' medical, financial, and other personal information, and provide users with easy and quick access to various locations and items. While adopted for their convenience outside of the healthcare sector, these invasive, semi-permanent implantable devices create augmented bodies that can be subject to ubiquitous surveillance. Situating human microchip implantations within surveillance literature, we draw from neoliberal perspectives of surveillance to examine augmented bodies, particularly as sources for market activity and as subjects of social control and sorting when these bodies are used as access control mechanisms, payment methods, and tracking means in employment, residential, commercial, and transportation sectors. History has demonstrated time and time again how unfettered technology applications and uses have led to real and/or perceived misuse by private and public sectors. Through the lens of function creep, we identify a pattern of expansion of applications and uses of technology beyond those originally intended across new technologies, such as DNA genetic genealogy databases, IoT wearables, and COVID-19 contact tracing apps, and provide illustrative examples of function creep, particularly the use of these technologies in criminal investigations and prosecutions despite not being intended or marketed for such use. By demonstrating the lack of clearly defined boundaries in the applications and uses of various new technologies and their associated data, and the ways they were misused, we demonstrate how human microchip implantations are headed on a similar path. The current and potential future uses of this technology raise concerns about the absence of regulation, law, and policy barring or limiting its application and use in specific sectors, and the impact of this technology on users' security, data protection, and privacy. Undeniably, the present and potential future functions, applications, uses, and extensions of human microchip implantations in various sectors warrant a proactive examination of their security, privacy, and data protection consequences and the implementation of proactive policies to regulate new and currently unregulated uses of this technology and its associated data within these sectors.
The unprecedented global public health crisis posed by the COVID-19 pandemic has caused mass upheaval of social, educational, financial, health, and justice systems around the world. Technological and other responses at the national, regional, and international level, designed to contain the spread of COVID-19, have also significantly interrupted the way that we live, work, and interact. This article explores the implications of these response efforts, and their impact on human rights, existing inequalities, and entrenched forms of discrimination. In particular, the article explores the implications of using mass surveillance and registration measures to detect, surveil, and control populations and their movements within and across borders as part of public health responses. The use of digital health credentials in automated social sorting processes and other mass surveillance and registration measures in response to the COVID-19 pandemic sets an alarming precedent for future responses to global public health crises.
Partition refinement is a method for minimizing automata and transition systems of various types. Recently we have developed a partition refinement algorithm and the tool CoPaR that is generic in the transition type of the input system and matches the theoretical run time of the best known algorithms for many concrete system types. Genericity is achieved by modelling transition types as functors on sets and systems as coalgebras. Experimentation has shown that memory consumption is a bottleneck for handling systems with a large state space, while running times are fast. We have therefore extended an algorithm due to Blom and Orzan, which is suitable for a distributed implementation to the coalgebraic level of genericity, and implemented it in CoPaR. Experiments show that this allows to handle much larger state spaces. Running times are low in most experiments, but there is a significant penalty for some.
ObjectiveThe paper compares victim group characteristics: we test routine activities theory to compare the differences in online fraud vulnerabilities of victims aged 18-54 and victims of 55 and above. Methods/sampleA representative sample of US citizens 18 and above was collected in October 2020. Victims under 55 encompassed 35.3% (n = 915), victims 55 and above 12.9% (n = 334) of the total sample (n = 2,589). We utilized non-parametric statistical methods for testing whether older and younger victims' characteristics can be derived from the same independent variables. ResultsComputer time, computer familiarity, and technical guardians determine online victimization in older individuals, similarly to younger age groups. However, older victims differ in characteristics from younger victims. Seniors were less likely to apply technical guardians such as camera cover, identity theft monitoring, and credit card freeze, even after experiencing online scams. Being a single parent was a protective factor for older individuals, but having a full-time job made older individuals more prone to experience online fraud victimization compared to being retired. In addition, older victims were less likely to report scams than younger ones. Conclusion/implicationsAlthough this research found significant differences between older and younger fraud victims' characteristics, target suitability and capable guardianship must be further investigated and conceptualized when applying routine activities theory for online fraud against older people.
The present study aims at understanding what factors contribute to the explanation of online identity theft (OIT) victimization and fear, using the Routine Activity Theory (RAT). Additionally, it tries to uncover the influence of factors such as sociodemographic variables, offline fear of crime, and computer perception skills. Data for the present study were collected from a self-reported online survey administered to a sample of university students and staff (N = 832, 66% female). Concerning the OIT victimization, binary logistic regression analysis showed that those who do not used credit card had lower odds of becoming an OIT victim, and those who reported visiting risky contents presented higher odds of becoming an OIT victim. Moreover, males were less likely than females of being an OIT victim. In turn, fear of OIT was explained by socioeconomic status (negatively associated), education (positively associated) and by fear of crime in general (positively associated). In addition, subjects who reported more online interaction with strangers were less fearful, and those reported more avoiding behaviors reported higher levels of fear of OIT. Finally, subjects with higher computer skills are less fearful. These results will be discussed in the line of routine activities approach and implications for online preventive behaviors will be outlined.
"Purpose - This chapter examines how those who study issues related to radicalization and counter-radicalization have recently drawn from the experiences of former extremists to inform our understanding of complex issues in terrorism and extremism studies. Approach - The authors synthesize the empirical research on radicalization and counter-radicalization that incorporates formers in the research designs. In doing so, the authors trace these research trends as they unfold throughout the life-course: (1) extremist precursors; (2) radicalization toward extremist violence; (3) leaving violent extremism; and (4) combating violent extremism. Findings - While formers have informed our understanding of an array of issues related to radicalization and counter-radicalization, empirical research in this space is in its infancy and requires ongoing analyses. Value - This chapter provides researchers, practitioners, and policymakers with an in-depth account of how formers have informed radicalization and counter-radicalization research in recent years as well as an overview of some of the key gaps in the empirical literature."
Digital forensic evidence is subject to a variety of challenges, and these challenges apply in the Cloud as anywhere else. This chapter is an overview of these issues specifically oriented toward the Cloud Computing environments of today.
This chapter focuses on a theoretical approach to proactive evidence collection and presents a conceptual approach for the Cloud. Forensic Readiness in the Cloud (FRC) calls upon technological and organizational strategies to address the risks that threaten organizational information. The two professions of Records Management (RM) and Digital Forensics (DF) can offer insights into how this might be achieved. In this chapter, the authors seek to explore the relationship between the two disciplines and the areas where collaboration and interdisciplinary work would be most beneficial. An initial overview of RM and its relationship to the wider field of Information Assurance (IA) precedes a more in depth comparison of the two related disciplines, using a model that integrates RM and DF. This is offered as a conceptual framework for making decisions about how to identify and manage the increasing quantities of evidence collected on networks. Organizational Network Forensic Readiness (NFR) has emerged as a method for supporting collection of digital evidence from networks using suggested checklists, procedures, and tools. This chapter elaborates upon a previously documented life cycle methodology for 'operationalizing' organizational NFR and integrates this with best practice from RM in FRC. FRC provides a conceptual approach to proactive evidence collection and identifies the phases at which RM approaches and processes might be most effectively employed in the Cloud.
For the emerging field of cloud forensics, the development of validated and repeatable scientific processes for conducting cloud forensic investigations should include requirements that establish evidence collected as legally admissible. There is currently an uncertainty in the legal requirements for cloud forensics. Forensic investigations in the cloud introduce unique issues that must be addressed, and the legal environment of the cloud must be considered. The authors will detail the process in criminal cloud forensic investigations for commanding production from cloud providers including constitutional and statutory limitations, and the civil and criminal admissibility processes. Decisions in court cases rely on the authenticity and reliability of the evidence presented. Ensuring cases involving cloud forensics follow the proper legal process and requirements will be beneficial for validating evidence when presented in court. Further, understanding of legal requirements will aid in the research and development of cloud forensics tools to aid investigations.
Cloud computing as a paradigm shift is transforming how services are being delivered. In this chapter, the authors present a Forensics as a Service (FaaS) model using cloud computing to deliver forensic services. This model leverages the flexibility, elasticity, and dynamics of cloud computing, and is affordable for business, government, or individuals in need, due to its reduced cost. It also addresses the challenge of processing a large volume of forensic data by using MapReduce and distributed computing.
Cyberattacks are a primary concern for organisations of all kinds, costing billions of dollars globally each year. As more businesses begin operating online, and as attackers develop more advanced malware and evolve their modus operandi, the demand for effective cyber security measures grows exponentially. One such measure is the threat intelligence platform (TIP): a system which gathers and presents information about current cyber threats, providing actionable insight to aid security teams in employing a more proactive approach to thwarting attacks. These platforms and their accompanying intelligence feeds can be costly when purchased from a commercial vendor, creating a financial barrier for small and medium-sized enterprises. This paper explores the use of crowdsourced open-source intelligence (OSINT) as an alternative to commercial threat intelligence. A model TIP is developed using a combination of crowdsourced OSINT, freeware, and cloud services, demonstrating the feasibility and benefits of using OSINT over commercial solutions. The developed TIP is evaluated using a dataset containing 16,713 malware samples collected via the MalwareBazaar repository.
This article is a theoretical treatment of the ways in which local worldviews on wealth acquisition give rise to contemporary manifestations of spirituality in cyberspace. It unpacks spiritual (occult) economies and wealth generation through a historical perspective. The article 'devil advocates' the 'sainthood' of claimed law-abiding citizens, by highlighting that the line dividing them and the Nigerian cybercriminals (Yahoo-Boys) is blurred with regards to the use of magical means for material ends. By doing so, the article also illustrates that the intersectionality of the spirit world and the acquisition of wealth (crime or otherwise) is connected with local epistemologies and worldviews, and its contemporaneity has social security benefits. Therefore, the view that the contemporary manifestations of spirituality in cyberspace signify a 'new-danger' and an ever-increasing outrage in Nigerian society is misplaced. I conclude that if people believe all aspects of life are reflective of the spiritual world and determined by it, the spiritual realm, by implication, is the base of society, upon which sits the superstructure comprised of all aspects of life, especially wealth. Inferentially, this conceptual position that the spirit world is the base of society is an inversion of Orthodox Marxist's theory of economic determinism.
The rise in research work focusing on detection of cyberbullying incidents on social media platforms particularly reflect how dire cyberbullying consequences are, regardless of age, gender or location. This paper examines scholarly publications (i.e., 2011-2022) on cyberbullying detection using machine learning through a systematic literature review approach. Specifically, articles were sought from six academic databases (Web of Science, ScienceDirect, IEEE Xplore, Association for Computing Machinery, Scopus, and Google Scholar), resulting in the identification of 4126 articles. A redundancy check followed by eligibility screening and quality assessment resulted in 68 articles included in this review. This review focused on three key aspects, namely, machine learning algorithms used to detect cyberbullying, features, and performance measures, and further supported with classification roles, language of study, data source and type of media. The findings are discussed, and research challenges and future directions are provided for researchers to explore.
In today's dynamic information technology system, one area of tremendous focus and recent growth has been that of the cloud-computing model in its various offerings. With this growth, however, come new challenges within the realms of e-discovery and digital forensics, as we traditionally know it. The rapid growth of cloud-computing services and the rate of acceptance and use by consumers are on the rise. Conversely, both legitimate and illegitimate activates can leverage the resources of the cloud to execute their operations. With the challenges growing to combat computer crime that utilizes the cloud ecosystem and the ease of which a criminal activity may be hidden using a cloud service, it is imperative that a cloud provider dedicate time, training, budget, and other resources to provide the facility for forensic investigators as well as law enforcement to combat this threat. The Cloud-Forensics-as-a-Service (FRaaS) model introduced later in this chapter can provide a comprehensive cloud forensics solution for creating a repeatable system. Such a system could be implemented as a standard forensics operational model for deployment within the cloud ecosystem regardless of environments and client service lines.
In recent years, the number of fake shopping sites that scam people out of their money or steal their personal information has skyrocketed. To address this problem, Japanese law enforcement agencies such as the police have been detecting fake shopping sites through information provided by a third party and by conducting manual investigations. However, this current approach is quite inefficient. Despite a number of recent studies that use machine learning to detect fake sites, there is still no system for automatically detecting fake shopping sites. Therefore, in this study, we developed an automatic detection system for fake shopping sites to solve the problem of detection inefficiency faced by law enforcement agencies in Japan. The proposed system successfully identified an average of 118,000 target URLs per day from the list of newly registered domains and collected an average of 51,000 sets of HTML data. Also, it was able to determine with 98.5% accuracy using machine learning whether the collected data were fake shopping sites or not. Since this system was able to meet the time requirements for actual operation, we developed an automatic detection system for fake Japanese shopping sites.
Cloud Computing will be a disruptive technology that will ultimately change the face of computing with a market approaching $300 billion over the next five years, according to recent study from the Market Intel Group (Mathews, 2010). The unstoppable migration of data to the Cloud is undoubtedly due to numerous financial benefits, particularly for small and medium-sized companies, which historically do not have the same capital budgets as larger enterprises. However, this boundless upside is not without risks from a legal and compliance perspective, making it all that more important for entities to look before they leap. Today, nearly every corporation is required to preserve and produce Electronically Stored Information (ESI), such as emails and other electronic documents, as part of their response to litigation, regulatory inquiries, and subpoenas. When the subject ESI happens to be stored in the Cloud, there are a handful of potential obstacles that serve to complicate the eDiscovery process. For some, this leads to sanctions and increased compliance risks. In order to navigate these potentially treacherous waters, organizations need to be proactive and follow a measure twice, cut once approach. This chapter will discuss the basics of eDiscovery and explore ways to minimize potential compliance hurdles when migrating significant data stores to/from the Cloud.
The dangers of contamination have received considerable attention in the literature regarding the investigation of physical crime scenes and physical evidence. The understanding of contamination in the context of digital evidence appears to be much less understood. Based on experiences from the field of physical evidence, we develop a generalized definition of contamination that also covers digital evidence, namely the inadvertent transfer of traits to an object of relevance at any point in the forensic process. We illustrate the definition by presenting several examples and counterexamples for contamination of digital evidence. By addressing the specifics of digital evidence in this context, we argue that our definition can be useful to understand the risks arising through contamination in this domain. (c) 2023 The Author(s). Published by Elsevier Ltd on behalf of DFRWS.
Flow monitoring has become an essential source of information for intrusion detection systems and various forms of network data analytics. However, the attention of researchers is focused primarily on the utilisation of the flow data, and the process of flow data creation is often neglected. This lack of consideration negatively affects the results of data analytics. Either the results are suboptimal due to the low quality of the flow data, or a description of the configuration of the flow monitoring system is missing, which leads to irreproducible results. The goal of this paper is to demonstrate how the configuration of the flow monitoring system affects the resulting data. The most basic flow monitoring configuration variables are the flow expiration timeouts. We analyse their effect on the number of created flow records to show their importance. Moreover, we demonstrate that the choice of the flow expiration timeouts can have a severe impact on the network data analytics. The use-case of Slowloris attack detection is used as an example to illustrate this fact.
In recent years, topic modeling has become an established method in the analysis of text corpora, with probabilistic techniques such as latent Dirichlet allocation (LDA) commonly employed for this purpose. However, it might be argued that adequate attention is often not paid to the issue of topic coherence, the semantic interpretability of the top terms usually used to describe discovered topics. Nevertheless, a number of studies have proposed measures for analyzing such coherence, where these have been largely focused on topics found by LDA, will-matrix decomposition techniques such as Non-negative Matrix Factorization (NMF) being somewhat overlooked in comparison. This motivates the current work, where we compare and analyze topics found by popular variants of both NMF and LDA in multiple corpora in terms of both their coherence and associated generality, using a combination of existing and new measures, including one based on distributional semantics. Two out of three coherence measures find NMF to regularly produce more coherent topics, with higher levels of generality and redundancy observed with the LDA topic descriptors. In all cases, we observe that the associated term weighting strategy plays a major role. The results observed with NMF suggest that this may be a more suitable topic modeling method when analyzing certain corpora, such as those associated with niche or non-mainstream domains. (C) 2015 Elsevier Ltd. All rights reserved.
This article highlights the importance of copyright industries for the developed economies and argues that criminal copyright infringement is a widespread offense, producing major economic losses for stakeholders, negatively impacting creativity, and raising significant cybersecurity and rule of law concerns. The article explains why there is a need for criminal protection of copyright protection and outlines the U.S. framework. In a comprehensive approach, based on a large corpus of data, consisting of cases brought to federal courts, in violation of Section 506 of Title 17 of the U.S. Code, and press releases and reports by law enforcement and industry groups, Section 3 describes the forms and extent of the phenomenon. Section 4 discusses essential aspects involved in the prosecution of these cases. Based on the number of cases brought to courts versus the criminal copyright infringing reports and estimates, the article concludes that this criminal phenomenon is significantly under-prosecuted and proposes a number of measures that could improve the criminal protection of copyrighted works.
"Cyber-attacks have become commonplace in the world of the Internet. The nature of cyber-attacks is gradually changing. Early cyber-attacks were usually conducted by curious personal hackers who used simple techniques to hack homepages and steal personal information. Lately, cyber attackers have started using sophisticated cyber-attack techniques that enable them to retrieve national confidential information beyond the theft of personal information or defacing websites. These sophisticated and advanced cyber-attacks can disrupt the critical infrastructures of a nation. Much research regarding cyber-attacks has been conducted; however, there has been a lack of research related to measuring cyber-attacks from the perspective of offensive cybersecurity. This motivated us to propose a methodology for quantifying cyber-attacks such that they are measurable rather than abstract. For this purpose, we identified each element of offensive cybersecurity used in cyber-attacks. We also investigated the extent to which the detailed techniques identified in the offensive cyber-security framework were used, by analyzing cyber-attacks. Based on these investigations, the complexity and intensity of cyber-attacks can be measured and quantified. We evaluated advanced persistent threats (APT) and fileless cyber-attacks that occurred between 2010 and 2020 based on the methodology we developed. Based on our research methodology, we expect that researchers will be able to measure future cyber-attacks."
Criminal procedure is increasingly becoming an important instrument of prevention. This is a globally observed tendency, and Poland is not an exception. There are several regulations in the Polish Code of Criminal Procedure that allow the preventive use of coercive measures. In 2020, a new and controversial regulation was introduced, authorising the public prosecutor or court to prohibit the publication of content interfering with the legally protected goods of the victim. The author criticises the new preventive measure as duplicating civil law injunctions and expresses the opinion that, in criminal procedure, preventive measures should be used to prevent crime, not every illegal activity. In addition, the article describes the criminal procedure for isolating persons obliged to quarantine themselves because they have tested positive for Covid-19 or had contact with infected persons. This raises the question of the limits of the preventive function of provisional arrest and possible abuse of the criminal process using it for aims unrelated to the traditional goal of the criminal process: determining the question of guilt of the accused.
This chapter aims to be a high-level introduction into the fundamental concepts of both digital forensic investigations and cloud computing for non-experts in one or both areas. Once fundamental concepts are established, this work begins to examine cloud computing security-related questions, specifically how past security challenges are inherited or solved by cloud computing models, as well as new security challenges that are unique to cloud environments. Next, an analysis is given of the challenges and opportunities cloud computing brings to digital forensic investigations. Finally, the Integrated Digital Investigation Process model is used as a guide to illustrate considerations and challenges during an investigation involving cloud environments.
The goal of this chapter is to explain the challenges that the forensic investigator faces when investigating Cloud Crime and how they can learn from the techniques used by Ethical Hackers to improve their investigation technique. The security threat posed by hackers on the Internet is constantly evolving. Cloud computing provides new avenues for hackers to exploit organizations, giving rise to new classes of vulnerability, and new security challenges. The forensic investigator must learn to think like a hacker so that they can reconstruct the path the hacker takes through the cloud environment. This chapter will explain how an Ethical Hacker works, how the Ethical Hacker views the Cloud, and in doing so illustrate the new challenges facing a forensic investigator.
Cloud computing is a major transition, and it comes at a unique historical and strategic time for applying foundational design thinking to secure the next-generation computing infrastructure and enable waves of business and technological innovation. In this chapter, the researcher summarizes six key research and development areas for designing a forensic-enabling cloud ecosystem, including architecture and matrix, standardization and strategy, evidence segregation, security and forensic integration, legal framework, and privacy.
Data acquisition and data recovery are essential to any e-discovery or digital forensic process. However, these two aspects seem to be considerably difficult in a cloud-computing environment. The very nature of the Cloud raises a number of technical and organizational challenges, which renders traditional approaches and tools inapplicable. Resource pooling, rapid elasticity, and geographical distribution of data are only a small part of the Cloud's features that hinder the forensic investigation. At the same time, there is significant absence of forensic readiness in cloud computing policy framework. In this chapter, the authors discuss the challenges pertaining to data acquisition in a cloud environment and discuss possible directions for meeting these challenges by presenting representative cases and sketching acquisition process and scenarios.
Despite a growing adoption of cloud computing, law enforcement and the judicial system are unprepared to prosecute cloud-based crimes. This chapter illuminates legal problems in the United States for electronic discovery and digital forensics arising from cloud computing and argues that cloud computing challenges the process and product of electronic discovery. The researchers investigate how to obtain forensic evidence from cloud computing using the legal process by surveying the existing statues and recent cases applicable to cloud forensics. A hypothetical case study of child pornography being hosted in the Cloud illustrates the difficulty in acquiring evidence for cloud-related crimes. For the first time, a sample search warrant is presented that could be used in this case study, and which provides sample language for agents and prosecutors who wish to obtain a warrant authorizing the search and seizure of data from cloud computing environments. The chapter concludes by taking a contrasting view and discusses how defense attorneys might be able to challenge cloud-derived evidence in court.
Improving the efficiency, effectiveness, and quality of public services has become a growing concern for many governments across the world, and more so with recent popularity of online services, widely referred as e-government services. The application of quality approaches for measuring and improving e-government services has been the subject of much research within the academic world over the last two decades. This chapter discusses the use of key quality approaches to improve services in Jordan's e-government initiatives. As more and more developing countries are adopting e-services as a means of providing quality services to their community and people through the Web, the necessary benchmarking plays an important role. Many traditional quality benchmarking performance measurements have proved futile in improving e-government services due to their quantitative focus. Though qualitative frameworks and measurement approaches such as Six Sigma and Balanced Scorecard have found their success in certain industry sectors, their relevance in the service sector has drawn attention only recently. While some studies have employed such approaches for evaluating projects in information and communication technologies, literature lacks investigations in the e-government sector. To fill this gap, this chapter investigates the application of Six Sigma and Balanced Scorecard approaches to improve quality in Jordanian e-government services.
A vehicular ad-hoc network consists of multiple local networks due to the limited communication range of Onboard Processing Unit utilised by participating vehicles. Therefore, multiple re-transmission attempts by relay vehicles are required to propagate an original information packet. However, these necessary re-transmissions cause Broadcast Storm Problem (BSP) which can in-turn create network congestions. This paper addresses the issues associated with BSP by proposing an Optimised Relay Vehicle Selection (ORVS) mechanism. To devise the strategy for an ORVS mechanism, an Evolutionary Game (EG) strategy is utilised that consists of a novel payoff function. In addition to this, we also propose a (BG) to simulate a VANET environment, which is implemented using Python, for collection of results. The numerical results obtained by the proposed EG are compared with the existing techniques, which show better performance by the ORVS mechanisms produced by the game theoretical approaches.
During this era of the Internet of Things, millions of devices such as automobiles, smoke detectors, watches, glasses and webcams are being connected to the Internet. The number of devices with the ability of monitor and collect data is continuously increasing. The Internet of Things enhances human comfort and convenience, but it raises serious questions related to security and privacy. It also creates significant challenges for digital investigators when they encounter Internet of Things devices in criminal scenes. In fact, current research focuses on security and privacy in Internet of Things environments as opposed to forensic acquisition and analysis techniques for Internet of Things devices. This chapter focuses on the major challenges with regard to Internet of Things forensics. A forensic approach for Internet of Things devices is presented using a smartwatch as a case study. Forensic artifacts retrieved from the smartwatch are analyzed and the evidence found is discussed with respect to the challenges facing Internet of Things forensics.
The unprecedented interest in cryptocurrency-related investments over the past five years has gone from a relatively unknown speculative investment to a mainstream practice and hobby. Novice investors looking for financial gains are now able to buy, sell, and trade cryptocurrency due to social media, smartphone applications, and online forums-all of which disseminate information and build trust in collaborative investment strategies. While many investors have used these platforms for financial gain, others have been the target and victim of relentless scams and forms of fraud. In this paper we argue that despite the advantages of novel financial systems that attempt create access to wealth for a broader base of individuals, the mechanisms behind cryptocurrency marketing, knowledge, and investment are predatory in nature and create a culture designed to pressure and coerce uneducated investors into financial victimhood. Specifically, we present a theoretical model whereby cryptoculture-as we define it-unintentionally produces target congruence of motivated offenders with an excess of suitable targets, further exacerbated by the lack of face-to-face engagement of online encounters. By Articulating a theoretical model should guide future studies in operationalizing cryptoculture and cryptocurrency fraud and prevent future cryptocurrency scams and fraud.
"Cloud computing is just one of many recent technologies that have highlighted shortcomings in the development of formal digital forensic processes, which up until now have been focused on a particular group of practitioners, such as law enforcement, and have been too high-level to be of significant practical use, or have been too detailed and specific to accommodate new technology as it emerges. Because the tools and procedures employed by digital forensic practitioners are generally outside the knowledge and understanding of the courts, they need to be described in such a way that they can be understood by the layperson. In addition, they should also conform to some standards of practice and be recognised by other practitioners working in the field (Armstrong, 2003; Kessler, 2010). Unfortunately, as Cohen (2011) points out, the whole field of digital forensics lacks consensus in fundamental aspects of its activities in terms of methodology and procedures. There has been a lot of activity around different aspects of cloud computing, and in Australia this has centered on the protection of personal data (Solomon, 2010). On an international scale, there have been several articles written by lawyers (Gillespie, 2012; Hutz, 2012; Kunick, 2012) discussing other legal considerations of accessing data in the cloud; however, this chapter looks at the issues surrounding digital evidence acquisition and introduces a new high-level process model that can assist digital forensic practitioners when it comes to presenting evidence in court that originated in the cloud."
Just about every technology magazine and article published today mentions virtualization or cloud computing. Technically, the two are different but very much intertwined. When environments use virtualization, there are artifacts an investigator can request that may provide valuable information. The content of this chapter explores the virtualization process, types of virtualized environments, and the part virtualization plays in cloud computing. A section will be included that presents case scenarios to demonstrate the type of evidence gathered in each environment for forensic investigations. A final section will include recommendations for additional areas of research in the area of investigating environments containing virtualization integration with cloud environments.
This article seeks to establish the connection-via shared discourse-between Incels and mainstream pornography. With an interdisciplinary approach which involves a Corpus Linguistics analysis of Reddit forum data, research into digital behaviors, and a feminist critique, this article focuses on the commonalities between the language of pornography and that of Incels. In doing so, it demonstrates how both pornography and Incels are different manifestations of the same misogyny. The findings of this study highlight the normalization of violence against women (VAW), which continues to be endemic in society, enabled and exacerbated by contemporary technologies.
International studies have shown that information security for process control systems, in particular SCADA, is weak. Many of the critical infrastructure (CI) services critically depend on process control systems. Therefore, any vulnerability in the protection of process control systems in CI may result in serious consequences for citizens and society. In order to understand their sector-wide security posture, the drinking water sector in The Netherlands benchmarked the information security of their process control environment. Large differences in the individual security postures of the ten drinking water companies were found. Good Practices for SCADA security were developed based upon the benchmark results. This paper discusses the simple but effective approach taken to perform the benchmark, the way the results were reported to the drinking water companies, and the way in which the SCADA information security good practices were developed. Apart from some high-level indications of areas requiring more security attention, no actual security posture results are presented in this paper since the study data contain company and national sensitive information. For the same reason, the figures in this paper are based on artificial data. (C) 2011 Elsevier B.V. All rights reserved.
Advanced and autonomous defence systems are needed to detect and mitigate new cyber attacks that target emerging technologies. This research defines cybersecurity preliminaries and stimulates questions to expose the complexities of automation and autonomous cybersecurity. A concept of autonomous anomaly management is introduced, where anomalies are defined not just as anomalous human readable strings, but as unknown changes and the need to better manage changes that can not be easily explained. Cybernetic principles are explored, and experiments are undertaken for quantitative analysis. Biodiversity Indices that compare relationships between species and communities in ecology are applied to cybernetic environments, to compare dynamic relationships between business applications. The proposed Cyber Diversity Index applies to all software-driven systems and environments. Changes of application's system calls are measured in real-time and monitored for unknown diversity changes, or the anomaly, when the system is placed under a zero-day attack. Experimental data is collected from continuous sampling of applications' activity states, which contributes to both the behavioral change profiles and the dynamic normal baselines for the autonomous anomaly management. The findings in this research show that it is possible to define unique system entropic behaviors from applications' activity that is triggered from internal or external stimulus.
This paper reports comparative authorship attribution results obtained on the Internet comments of the morphologically complex Lithuanian language. We have explored the impact of machine learning and similarity-based approaches on the different author set sizes (containing 10, 100, and 1,000 candidate authors), feature types (lexical, morphological, and character), and feature selection techniques (feature ranking, random selection). The authorship attribution task was complicated due to the used Lithuanian language characteristics, non-normative texts, an extreme shortness of these texts, and a large number of candidate authors. The best results were achieved with the machine learning approaches. On the larger author sets the entire feature set composed of word-level character tetra-grams demonstrated the best performance.
"Phishing victimization is prevalent and results in theft of personal identifiable information (PII) or installing malware to steal PII. Drawing upon social psychological and criminological theories, we conducted a prospective study to assess three groups of predictors to being phished or not: a) prior victimization; b) protective or vulnerable habitual strategies, and c) emotional and cognitive decision-making styles. Students (N = 236) completed a survey assessing these predictors and then about 4 weeks later received a phishing e-mail using the university's phishing testing system. The e-mail requested that they click on a link and enter their student ID to avoid having their account blocked. About half (50.8%) clicked on the link, and 81.6% of those phished entered their PII. Individuals who had low avoidant style and high generalized anxiety were four times more likely to be phished, after controlling for the significant effects of vulnerable habitual strategies and using dating apps. Machine learning analyses also found cognitive styles and generalized anxiety are the better predictors of getting phished compared to vulnerable and protective strategies and prior victimization. These findings suggest that cybersecurity training needs to be expanded to address the emotional and cognitive processing of deceptive appeals in e-mails."
Recent political events have brought renewed attention to the adaptation of immigrants in the United States, and their involvement in crime. Immigrants vary significantly in terms of when they migrate into the country. According to the Current Population Survey (CPS) (2017), of the population of approximately 13 million foreign-born immigrant children living in the United States, approximately 40% arrived during early childhood, 30% during middle childhood, and 30% during adolescence. A better understanding of the relationship between age at migration and offending can inform not only immigration policies, and policies related to the control of crime, but also policies related to immigrant-receiving institutions such as schools and social services. Using data from The National Longitudinal Survey of Youth 1997 (NLSY97), this study explores the influence of age at migration on criminal offending among foreign-born immigrants who migrated prior to adulthood.
Western law enforcement agencies have made multiple arrests targeting individuals purchasing firearms on Dark Web platforms in recent years, as these transactions may violate national laws and facilitate offline violence. Despite its market presence and growth, research exploring these online illicit markets has been scant, especially as it relates to how firearms are priced on the Dark Web, and the factors that influence their price point. Given this gap in the literature, the current study utilized a sample of 287 firearm products across 20 Dark Web vendors operating in both crypto markets and shops to identify the range and pricing model of illicit weapons. Analyses revealed that long guns offered on the Dark Web had lower average listed prices than their manufacturer's suggested retail price (MSRP), while handguns had higher advertised prices than their recommended retail value. Further, products' MSRP was a significant predictor of firearms' price point for both handguns and long guns, whereas offering a customer service line was only significant for handguns' price point. The implications of this analysis for our understanding of illicit online market operations are discussed in detail.
Prominent terrorism case studies of individuals such as Omar Mateen, Dylann Roof, and Mohammed Merah indicate the need for personality trait-based terrorism risk assessment/threat assessment (TR/TA). This chapter provides an overview of Corrado's, personality-based TR/TA instrument (see Chapter 14) by explaining the origin of each domain and the purpose of inclusion. Furthermore, this chapter displays results from a preliminary instrument validation study conducted on an open-source sample of 158 terrorists. Results of this study suggest strong statistical significance for many of the domains. This suggests the need for future inclusion of personality-based indicators in terrorism risk assessment.
The so-called a posteriori approach to optimization with multiple conflicting objective functions is to compute or approximate a Pareto front of solutions. In case of continuous objective functions a finite approximation to this set can be computed. Indicator-based multiobjective optimization algorithms compute solution sets that are optimal with respect to some quality measure on sets, such as the commonly used hypervolume indicator (HI). The HI measures the size of the space that is dominated by a given set of solutions. It has many favorable monotonicity properties but it requires a reference point the choice of which is often done ad-hoc. In this study the concept of set monotonic functions for dominated subsets is introduced. Moreover, this work presents a reference point free hypervolume indicator that uses a density that is derived from the user's preferences expressed as desirability functions. This approach will bias the distribution of the approximation set towards a set that more densely samples highly desirable solutions of the objective space. We show that the Harrington type and the Derringer-Suich type of desirability functions yield definite integrals and that the Harrington type has also the favorable property to provide a set-monotonic function over the set of dominated subspaces. It is shown that for a product type of aggregation the weighted hypervolume indicator is mathematically equivalent with an approach that computes the standard hypervolume indicator after transformation of the axes. In addition a probabilistic interpretation of desirability functions is discussed and how a correlation parameter can be introduced in order to change the aggregation type. Finally, practical guidelines for using the discussed set indicator in multiobjective search, for instance when searching for interesting subsets from a database, are provided. (C) 2014 The Authors. Published by Elsevier Ltd.
Recently, deep learning techniques have garnered substantial attention for their ability to identify vulnerable code patterns accurately. However, current state-of-the-art deep learning models, such as Convolutional Neural Networks (CNN), and Long Short-Term Memories (LSTMs) require substantial computational resources. This results in a level of overhead that makes their implementation unfeasible for deployment in real-time settings. This study presents a novel transformer-based vulnerability detection framework, referred to as VulDetect, which is achieved through the fine-tuning of a pre-trained large language model, (GPT) on various benchmark datasets of vulnerable code. Our empirical findings indicate that our framework is capable of identifying vulnerable software code with an accuracy of up to 92.65%. Our proposed technique outperforms SyseVR and VulDeBERT, two state-of-the-art vulnerability detection techniques.
It has long been lamented that firms underreport cyber attacks. In recent years, regulators have begun mandating that certain organizations must publicly report when incidents occur. Adherence to these requirements is an empirical question that has been largely unexamined to date. In this paper, we study regulatory filings by U.S. public companies to the Securities Exchange Commission and to the Department Health and Human Services that discuss cyber attacks. We also compare the findings against crowdsourced reports of cyber incidents appearing in media outlets. We find substantial gaps in coverage, both in terms of attacks that make the news but do not appear in regulatory filings and vice versa. We conclude by discussing the implications for the study of cyber attack and defense as well as for policymakers.
We explored the transmission mechanisms of corporate fraud and its punishments within social network communities. Using fraud triangle theory and trust triangle theory, we hypothesize four transmitting chan-nels of how fraud commission and detection are affected by peers' fraud and punishment. Based on Chinese listed corporations from 2008 to 2018, we first construct and detect interlocked social network communities with a community-detecting algorithm, and then examine hypotheses using a bivariate probit model with partial observability. Our findings indicate that peer-concealing and-hinting effects exist within social net-work communities. The peer-concealing effect decreases the likelihood of being detected when committing fraud, for those with more and closer fraudulent peers. The peer-hinting effect increases the likelihood of being detected when committing fraud, for those with more and closer punished peers. There is no evidence to support peer-contagion and vicarious-punishment effects. Thus, an improved understanding of the trans-mission mechanism of corporate fraud commission and detection within communities is provided to prevent and detect corporate fraud. In addition, stakeholders and regulators should be aware of the deviant subculture and social distancing in social network communities.
We identify over a quarter of a million domains used by medium and large companies within the .com registry. We find that for around 7% of these companies very similar domain names have been registered with character changes that are intended to be indistinguishable at a casual glance. These domains would be suitable for use in Business Email Compromise frauds. Using historical registration and name server data we identify the timing, rate, and movement of these look-alike domains over a ten year period. This allows us to identify clusters of registrations which are quite clearly malicious and show how the criminals have moved their activity over time in response to countermeasures. Although the malicious activity peaked in 2016, there is still sufficient ongoing activity to cause concern.
The development of the Darknet as a parallel network to the Web in the 21st century has facilitated illegal trafficking in small arms, as defined by the United Nations. The authors have used investigative research methodologies to observe six weapon sale sites on the Darknet over a six-month period to identify sellers of firearms, the type and caliber of weapons for sale, manufacturer, price in Bitcoin, and the principle national origins of the firearms. This is the first study of its type to explore the illegal sale of firearms on the Darknet. This evidence can be used by law enforcement to intercept and shut down said sites and provide insight to the nature of the illegal arms trade on the Darknet.
The Internet of Things (IoT) represents a growing aspect of how entities, including humans and organizations, are likely to connect with others in their public and private interactions. The exponential rise in the number of IoT devices, resulting from ever-growing IoT applications, also gives rise to new opportunities for exploiting potential security vulnerabilities. In contrast to conventional cryptosystems, frameworks that incorporate fine-grained access control offer better opportunities for protecting valuable assets, especially when the connectivity level is dense. Functional encryption is an exciting new paradigm of public-key encryption that supports fine-grained access control, generalizing a range of existing fine-grained access control mechanisms. This survey reviews the recent applications of functional encryption and the major cryptographic primitives that it covers, identifying areas where the adoption of these primitives has had the greatest impact. We first provide an overview of different application areas where these access control schemes have been applied. Then, an in-depth survey of how the schemes are used in a multitude of applications related to IoT is given, rendering a potential vision of security and integrity that this growing field promises. Towards the end, we identify some research trends and state the open challenges that current developments face for a secure IoT realization.
Trust is a fundamental aspect in enabling a smooth adoption of robotic technical innovations in our societies. While Artificial Intelligence (AI) is capable to uplift digital contributions to our societies while protecting environmental resources, its ethical and technical trust dimensions bring significant challenges for a sustainable evolution of robotic systems. Inspired by the safety assurance case, in this paper we introduce the concept of trust assurance case together with the implementation of its ethical and technical principles directed towards assuring a trustworthy sustainable evolution of AI-enabled robotic systems.
Highly manipulative online and telephone scams committed by strangers target everyone, but older individuals are especially susceptible to being victimized. This study aimed to (1) identify why older individuals decide not to report scams and, in parallel, (2) explore the needs of victims. Thirty-five interviews were conducted with Virginia residents who were 60 years or older in 2021. The interpretive phenomenological analysis of the semi-structured interviews revealed that victims are reluctant to report crimes or ask for help from their family or community because much-needed emotional, educational, and technical help is often inaccessible or inadequate. In particular, we found that family responses tend to intrude on privacy, community responses are not meaningful or are non-existent, police responses are inadequate, and prevention programs are inaccessible and not specified to meet the needs of older age groups. We recommend developing age-appropriate prevention and education programs, by applying the intergenerational group approach, and actively listening to victims' concerns before deciding what means of help should be applied. Research implications and recommendations are presented.
Users of computing systems and devices frequently make decisions related to information security, e.g., when choosing a password, deciding whether to log into an unfamiliar wireless network. Employers or other stakeholders may have a preference for certain outcomes, without being able to or having a desire to enforce a particular decision. In such situations, systems may build in design nudges to influence the decision making, e.g., by highlighting the employer's preferred solution. In this paper we model influencing information security to identify which approaches to influencing are most effective and how they can be optimized. To do so, we extend traditional multi-criteria decision analysis models with modifiable criteria, to represent the available approaches an influencer has for influencing the choice of the decision maker. The notion of influence power is introduced to characterize the extent to which an influencer can influence decision makers. We illustrate our approach using data from a controlled experiment on techniques to influence which public wireless network users select. This allows us to calculate influence power and identify which design nudges exercise the most influence over user decisions. (C) 2016 Elsevier B.V. All rights reserved.
Background: Many efforts have been undertaken to construct an overview of various aspects of illicit drug distribution in the United Kingdom. Yet given that national, regional, and local differences can be profound, this has proven difficult, to the extent that Scotland has been largely excluded from the conversation. In addition, the level of supply being examined, the drug type, and the actors involved only add to confusion and vast differences between some findings. Method: The current study aims to provide a holistic account, as best as possible considering variations of illegal drug supply in illicit networks, by focusing in on a particular geographical context (Scotland) and addressing drug supply at all levels. It is informed by in-depth interviews with 42 offenders involved in drug distribution from retail to wholesale/middle market to importation levels. Results: Findings indicate Scotland's importation and distribution is evolving owing to increasingly adaptive risk mitigation by importers and distributors, and market diversification of both product and demand. While a hierarchical model still dominates the market, commuting or 'county lines' and increasing demand for drugs such as cannabis, but also anabolic steroids and psychoactive substances, means that home growing, online purchasing, and street-level dealership is common. Conclusion: The findings have the capacity to further inform police and practitioners about the diverse and evolving nature of drug distribution in Scotland (with a particular focus on the west of the country), so that they may become more effective in improving the safety and wellbeing of people, places and communities.
Industrial control systems (ICSs) used to be operated in closed networks, that is, separated physically from the Internet and corporate networks, and independent protocols were used for each manufacturer. Thus, their operation was relatively safe from cyberattacks. However, with advances in recent technologies, such as big data and internet of things, companies have been trying to use data generated from the ICS environment to improve production yield and minimize process downtime. Thus, ICSs are being connected to the internet or corporate networks. These changes have increased the frequency of attacks on ICSs. Despite this increased cybersecurity risk, research on ICS security remains insufficient. In this paper, we analyze threats in detail using STRIDE threat analysis modeling and DREAD evaluation for distributed control systems, a type of ICSs, based on our work experience as cybersecurity specialists at a refinery. Furthermore, we verify the validity of threats identified using STRIDE through case studies of major ICS cybersecurity incidents: Stuxnet, BlackEnergy 3, and Triton. Finally, we present countermeasures and strategies to improve risk assessment of identified threats.
In this paper we are conducting a series of experiments with several state-of-the-art models, based on Transformers architecture, to perform Named Entity Recognition and Classification (NERC) on text of different styles (social networks vs. news) and languages, and with different levels of noise. We are using different publicly-available datasets such as WNUT17, CoNLL2002 and CoNLL2003. Furthermore, we synthetically add extra levels of noise (random capitalization, random character additions/replacements/removals, etc.), to study the impact and the robustness of the models. The Transformer models we compare (mBERT, CANINE, mDeBERTa) use different tokenisation strategies (token-based vs. character-based) which may exhibit different levels of robustness towards certain types of noise. The experiments show that the subword-based models (mBERT and mDeBERTa) tend to achieve higher scores, especially in the presence of clean text. However, when the amount of noise increases, the character-based tokenisation exhibits a smaller performance drop, suggesting that models such as CANINE might be a better candidate to deal with noisy text.
Libel sites publish anonymously submitted unproven libelous claims about individuals that often include personal information about the subject. The stated goal of the sites is to warn the public about an individual but the impact is harassment and ruining the subject's reputation. These individual libelous posts are surfaced when searching for a person's name using an online search engine and can cause a range of harms from emotional to economic. For example, the libelous posts might surface if a potential employer performs a Google search as part of a background check. There have been prior news reports of this troubling phenomena but no systematic analysis of the ecosystem. In this paper, we conduct a rigorous analysis of these libel sites, supporting services, and intervention by Google. We discovered and analyzed 9 libel sites, 7 websites for reputation management services, and 12 related websites. We found that all of the libel websites included at least one method of generating revenue. The most common revenue generation method was including advertisements for reputation management services which require payment for the removal of a post. We found that all of these removal services were dubious in nature and that the removal policies were akin to extortion. Our analysis of Google's intervention to reduce the visibility of these websites indicated that it appeared to only reduce the visibility of the specific libel post URL but that other URLs containing links to the post or the headline text of the post were still highly ranked. Based on our findings, we make recommendations to many of the stakeholders about potential approaches for mitigating this abusive ecosystem.
In the a-posteriori approach to multicriteria decision making the idea is to first find a set of interesting (usually non-dominated) decision alternatives and then let the decision maker select among these. Often an additional demand is to limit the size of alternatives to a small number of solutions. In this case, it is important to state preferences on sets. In previous work it has been shown that independent normalization of objective functions (using for instance desirability functions) combined with the hypervolume indicator can be used to formulate such set-preferences. A procedure to compute and to maximize the probability that a set of solutions contains at least one satisfactory solution is established. Moreover, we extend the model to the scenario of multiple decision makers. For this we compute the probability that at least one solution in a given set satisfies all decision makers. First, the information required a-priori from the decision makers is considered. Then, a computational procedure to compute the probability for a single set to contain a solution, which is acceptable to all decision makers, is introduced. Thereafter, we discuss how the computational effort can be reduced and how the measure can be maximized. Practical examples for using this in database queries will be discussed, in order to show how this approach relates to applications. (C) 2015 The Authors. Published by Elsevier B.V.
Introduction Despite the increasing use of domestic social robots by older adults, there remains a significant knowledge gap regarding attitudes, concerns, and potential adoption behavior in this population. This study aims to categorize older adults into distinct technology adoption groups based on their attitudes toward domestic social robots and their behavior in using the existing technology.Methods An exploratory qualitative research design was used, involving semi-structured interviews with 24 retired Slovenian older adults aged 65 years or older, conducted between 26 June and 14 September 2023.Results Four distinct groups of older adults were identified: (1) Cautious Optimists, (2) Skeptical Traditionalists, (3) Positive Optimists, and (4) Technophiles based on eight characteristics.Discussion These groups can be aligned with the categories of the Diffusion of Innovation Theory. Privacy and security concerns, influenced by varying levels of familiarity with the technology, pose barriers to adoption. Perceived utility and ease of use vary considerably between groups, highlighting the importance of taking into account the different older adults. The role of social influence in the adoption process is complex, with some groups being more receptive to external opinions, while others exhibit more autonomous decision-making.
Captchas are a standard defense on commercial websites against undesirable or malicious Internet bot programs, but widely deployed schemes can be broken with simple but novel attacks. Applying security engineering expertise to the design of Captchas can significantly improve their robustness.
This paper analyzes the correlates of traditional (burglary, vehicle theft, theft from vehicles, and pickpocketing) and online property crime (data theft and online fraud), with particular attention given to the relation between them and the risk of multiple victimizations. Data are gathered from a large and representative victimization survey conducted in a Swiss city (N = 7,885). The findings suggest that both traditional and online property victimization are related positively to the participants' lifestyle routines, physical and online protection measures, and educational level. Men, young persons, those employed actively, and university graduates are overrepresented among the multiple victims of online-offline property victimization.
Electromyogram (EMG) signals provide valuable insights into the muscles' activities supporting the different hand movements, but their analysis can be challenging due to their stochastic nature, noise, and non-stationary variations in the signal. We are pioneering the use of a unique combination of wavelet scattering transform (WST) and attention mechanisms adopted from recent sequence modelling developments of deep neural networks for the classification of EMG patterns. Our approach utilizes WST, which decomposes the signal into different frequency components, and then applies a non-linear operation to the wavelet coefficients to create a more robust representation of the extracted features. This is coupled with different variations of attention mechanisms, typically employed to focus on the most important parts of the input data by considering weighted combinations of all input vectors. By applying this technique to EMG signals, we hypothesized that improvement in the classification accuracy could be achieved by focusing on the correlation between the different muscles' activation states associated with the different hand movements. To validate the proposed hypothesis, the study was conducted using three commonly used EMG datasets collected from various environments based on laboratory and wearable devices. This approach shows significant improvement in myoelectric pattern recognition (PR) compared to other methods, with average accuracies of up to 98%.
Since the advent of the Internet, right-wing extremists and those who subscribe to extreme right views have exploited online platforms to build a collective identity among the like-minded. Research in this area has largely focused on extremists' use of websites, forums, and mainstream social media sites, but overlooked in this research has been an exploration of the popular social news aggregation site Reddit. The current study explores the role of Reddit's unique voting algorithm in facilitating othering discourse and, by extension, collective identity formation among members of a notoriously hateful subreddit community, r/The_Donald. The results of the thematic analysis indicate that those who post extreme-right content on r/The_Donald use Reddit's voting algorithm as a tool to mobilize like-minded members by promoting extreme discourses against two prominent out-groups: Muslims and the Left. Overall, r/The_Donald's sense of community facilitates identity work among its members by creating an environment wherein extreme right views are continuously validated.
Previous studies on fighting computer-assisted frauds have attempted to assist law enforcement agencies (LEAs) to better understand important aspects of motivation, opportunity and deterrence. However, there have been few empirical studies on the profiles of convicted offenders, post detection. This paper examines characteristics of frauds and their associated respective law enforcement response with particular emphasis on frauds facilitated by information technology. The findings show how the prosecution and conviction of the offenders differ among commonly-seen types of computer assisted frauds, and bring new evidence to the common association of gender and crime, severity and punishment, etc. The findings may help LEAs and legislative bodies to evaluate their current practices from the point of restorative social justice.
Cyberbullying has grown into a global issue with the Internet becoming a readily accessible commodity around the world. During the outbreak and spread of novel coronavirus disease (COVID-19), most personal and professional activities shifted to the virtual environment, increasing the time children spent online and the opportunities to engage in deviant behavior such as cyberbullying. In this paper, we analyzed cyberbullying data drawn from a nationally representative survey of South Korean adolescents during the COVID-19 pandemic. Correlates of cyberbullying perpetration were examined in addition to the moderating impact of parental support. Policy implications and future research directions were discussed based on the findings.
We investigate the problem of creating ambiguous file system partitions, i.e., the possibility to have two fully functional file systems within a single file system partition. The problem is different from steganographic data hiding since there is no real distinction between content and cover data, and no translation process may be applied to the content data. Since typical file systems that occur in forensic analysis are usually unambiguous, ambiguous file system partitions may be useful corner cases in forensic tools and processes. We show that it is possible to create ambiguous file system partitions by integrating a guest file system into the structures of a host file system in two cases: We integrate a fully functional FAT32 into Ext3 and HFS+. In a third example we even integrate two guest file systems (HFS+ and FAT32) into a single Btrfs file system partition. We test common forensic tools on these examples and exhibit some deficiencies. Moreover, we develop a taxonomy of ambiguous file system partitions and argue that the existence of essential data at fixed positions still is a way to distinguish host from guest and so to heuristically reduce the ambiguity, without removing it completely. (C) 2022 The Author(s). Published by Elsevier Ltd on behalf of DFRWS This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).
Internet can be misused by cyber criminals as a platform to conduct illegitimate activities (such as harassment, cyber bullying, and incitement of hate or violence) anonymously. As a result, authorship analysis of anonymous texts in Internet (such as emails, forum comments) has attracted significant attention in the digital forensic and text mining communities. The main problem is a large number of possible of authors, which hinders the effective identification of a true author. We interpret open class author attribution as a process of expert recommendation where the decision support system returns a list of suspected authors for further analysis by forensics experts rather than a single prediction result, thus reducing the scale of the problem. We describe the task formally and present algorithms for constructing the suspected author list. For evaluation we propose using a simple Winner-Takes-All (WTA) metric as well as a set of gain-discount model based metrics from the information retrieval domain (mean reciprocal rank, discounted cumulative gain and rank-biased precision). We also propose the List Precision (LP) metric as an extension of WTA for evaluating the usability of the suspected author list. For experiments, we use our own dataset of Internet comments in Lithuanian language and consider the use of language-specific (Lithuanian) lexical features together with general lexical features derived from English language. For classification we use one-class Support Vector Machine (SVM) classifier. The results of experiments show that the usability of open class author attribution can be improved considerably by using a set of language-specific lexical features together with general lexical features, while the proposed method can be used to reduce the number of suspected authors thus alleviating the work of forensic linguists.
In addition to hosting user-generated video content, YouTube provides recommendation services, where sets of related and recommended videos are presented to users, based on factors such as co-visitation count and prior viewing history. This article is specifically concerned with extreme right (ER) video content, portions of which contravene hate laws and are thus illegal in certain countries, which are recommended by YouTube to some users. We develop a categorization of this content based on various schema found in a selection of academic literature on the ER, which is then used to demonstrate the political articulations of YouTube's recommender system, particularly the narrowing of the range of content to which users are exposed and the potential impacts of this. For this purpose, we use two data sets of English and German language ER YouTube channels, along with channels suggested by YouTube's related video service. A process is observable whereby users accessing an ER YouTube video are likely to be recommended further ER content, leading to immersion in an ideological bubble in just a few short clicks. The evidence presented in this article supports a shift of the almost exclusive focus on users as content creators and protagonists in extremist cyberspaces to also consider online platform providers as important actors in these same spaces.
An insider threat can take on many forms and fall under different categories. This includes malicious insider, careless/unaware/uneducated/naive employee, and the third-party contractor. Machine learning techniques have been studied in published literature as a promising solution for such threats. However, they can be biased and/or inaccurate when the associated dataset is hugely imbalanced. Therefore, this article addresses the insider threat detection on an extremely imbalanced dataset which includes employing a popular balancing technique known as spread subsample. The results show that although balancing the dataset using this technique did not improve performance metrics, it did improve the time taken to build the model and the time taken to test the model. Additionally, the authors realised that running the chosen classifiers with parameters other than the default ones has an impact on both balanced and imbalanced scenarios, but the impact is significantly stronger when using the imbalanced dataset.
This study examines darknet markets through the lens of a business theory on knowledge management. Taking epistemological and ontological dimensions into consideration, this study uses Nonaka's (1991) SECI model as a theoretical framework to identify and describe how tacit and explicit knowledge is created and shared on Silk Road, Pandora and Agora darknet markets, and how people affect this process. By studying this process, insights can be obtained into darknet market criminal organizations and communities of practice and their impact on the continuity and resilience of illicit darknet markets. This project used data from the Internet Archive collection of publicly available darknet market scrapes between 2011 and 2015 from Branwen et al. (2015). We observed instances of the SECI model (socialization, externalization, combination, and internalization) on darknet markets in both criminal organizations and communities of practice. Darknet market leaders and groups facilitated both knowledge creation and sharing. This study is the first to test the SECI model on darknet markets. The study provides an understanding of the complexity and resilience of darknet markets, as well as valuable information to help guide law enforcement agencies efforts to stop the illicit trade of goods and services.
"The current study explored the effect of a school-based intervention on online risk awareness and behavior in order to shed light on a relatively unexplored field with high practical relevance. More than 800 Belgium primary school children (grade 4 and 6) were assessed at two measurements (n T1 = 812, 51.2 % female; n T2 = 819, 51.3 % female) before and after the intervention. Half of them received a 10 min classroom intervention indicating online risks. Children in the control group received a 10 min presentation concerning online applications without any emphasis on risks. Children in the intervention group were more likely to be aware of online risks directly after the intervention; this effect was still noticeable 4 months after. Reporting of online risk behavior in the intervention group was also higher compared to the control group who did not receive the intervention. Overall online risk awareness and online risk behavior were negatively associated and the awareness did not modulate the association between the intervention and online risk behavior. Furthermore, individual differences were assessed. Girls were more likely to be aware of online risks and asserted less online risk behavior than boys were. In line with the imperative in adolescence to become more risk taking, children in a higher grade were more likely to behave in a risky manner when online. The current study provides a valuable starting point for further research on how to decrease online risk behavior in early adolescence."
Due to recent advances in pose-estimation methods, human motion can be extracted from a common video in the form of 3D skeleton sequences. Despite wonderful application opportunities, effective and efficient content-based access to large volumes of such spatio-temporal skeleton data still remains a challenging problem. In this paper, we propose a novel content-based text-to-motion retrieval task, which aims at retrieving relevant motions based on a specified natural-language textual description. To define baselines for this uncharted task, we employ the BERT and CLIP language representations to encode the text modality and successful spatio-temporal models to encode the motion modality. We additionally introduce our transformer-based approach, called Motion Transformer (MoT), which employs divided space-time attention to effectively aggregate the different skeleton joints in space and time. Inspired by the recent progress in text-to-image/video matching, we experiment with two widely-adopted metric-learning loss functions. Finally, we set up a common evaluation protocol by defining qualitative metrics for assessing the quality of the retrieved motions, targeting the two recently-introduced KIT Motion-Language and HumanML3D datasets. The code for reproducing our results is available here: https://github.com/mesnico/text-to-motion-retrieval.
The quality of the extracted traditional hand-crafted Electromyogram (EMG) features has been recently identified in the literature as a limiting factor prohibiting the translation from laboratory to clinical settings. To address this limitation, a shift of focus from traditional feature extraction methods to deep learning models was witnessed, as the latter can learn the best feature representation for the task at hand. However, while deep learning models achieve promising results based on raw EMG data, their clinical implementation is often challenged due to their significantly high computational costs (significantly large number of generated models' parameters and a huge amount of data needed for training). This paper is focused on combining the simplicity and low computational characteristics of traditional feature extraction with the memory concepts from Long Short-Term Memory (LSTM) models to efficiently extract the spatial-temporal dynamics of the EMG signals. The novelty of the proposed method can be summarized in a) the memory concept leveraged from deep learning structures, capturing short-term temporal dependencies of the EMG signals, b) the use of cardinality to generate logical combinations of spatially distinct EMG signals and as a feature extraction method and 3) low computational costs and the enhanced classification performance. The performance of the proposed method is validated using three EMG databases collected with 1) laboratory hardware (9 transradial amputees and 17 intact-limbed), and 2) wearables (22 intact-limed using two wearable consumer armbands). In comparison to several other well-known methods from the literature, the proposed method shows significantly enhanced myoelectric pattern recognition performance, with accuracies reaching up to 99%.
Phishing emails are becoming more and more sophisticated, making current detection techniques ineffective. The reporting of phishing emails from users is, thus, crucial for organizations to detect phishing attacks and mitigate their effect. Despite extensive research on how the believability of a phishing email affects detection rates, there is little to no research about the relationship between the believability of a phishing email and the associated reporting rate. In this work, we present a controlled experiment with 446 subjects to evaluate how the reporting rate of a phishing email is linked to its believability and detection rate. Our results show that the reporting rate decreases as the believability of the email increases and that around half of the subjects who detect the mail as phishing, have an intention to report the email. However, the group intending to report an email is not a subset of the group detecting the mail as phishing, suggesting that reporting is still a concept misunderstood by many.
Social networking sites are increasingly subject to malicious activities such as self-propagating worms, confidence scams and drive-by-download malwares. The high number of users associated with the presence of sensitive data, such as personal or professional information, is certainly an unprecedented opportunity for attackers. These attackers are moving away from previous platforms of attack, such as emails, towards social networking websites. In this paper, we present a full stack methodology for the identification of campaigns of malicious profiles on social networking sites, composed of maliciousness classification, campaign discovery and attack profiling. The methodology named REPLOT, for REtrieving Profile Links On Twitter, contains three major phases. First, profiles are analysed to determine whether they are more likely to be malicious or benign. Second, connections between suspected malicious profiles are retrieved using a late data fusion approach consisting of temporal and authorship analysis based models to discover campaigns. Third, the analysis of the discovered campaigns is performed to investigate the attacks. In this paper, we apply this methodology to a real world dataset, with a view to understanding the links between malicious profiles, their attack methods and their connections. Our analysis identifies a cluster of linked profiles focusing on propagating malicious links, as well as profiling two other major clusters of attacking campaigns.
Objective The purpose of the current research was to examine the predictors of cyberbullying victimization among South Korean students during a period in which the coronavirus disease was spreading worldwide. We assessed whether parental guardianship protected against victimization when most people worked from home and school instructions were shifted to online learning. Methods We analyzed nationally representative data collected between October 6 and November 13, 2020. Binary logistic regression models were developed based on the Routine Activities Theory theoretical model to investigate the correlates of cyberbullying victimization among participants. Results The results showed that respondents' routine online activities were closely related to victimization, and parental guardianship provided partial protection by reducing non-violent victimization. Conclusion Parents could play a critical role in protecting children from cyberbullying victimization. Future research should continue to investigate the impact of parenting on reducing cyberbullying victimization, specifically the effects of different parenting styles and protections.
Finitary monads on Pos are characterized as precisely the free-algebra monads of varieties of algebras. These are classes of ordered algebras specified by inequations in context. Analogously, finitary enriched monads on Pos are characterized: here we work with varieties of coherent algebras which means that their operations are monotone.
Using representative survey data from Canada, the United States, Australia, the United Kingdom, and five European Union member countries (n = 9,011), this paper predicts the use of privacy-enhancing technologies and techniques as a form of individual target hardening behavior. It presents a novel extension of routine activity theory by revisiting the theory from the perspective of Internet users in their efforts to minimize their likelihood of victimization. The results show that perceived concern and exposure to certain motivated offenders leads to target hardening behaviors, such as using two-factor authentication and antivirus software. Encryption communication services, however, were not associated with concern for any combination of motivated offenders. Moreover, perceived deficiencies in governmental but not company guardianship predicted new protective behaviors.
"By reframing the theoretical relationship between the notions of privacy/data protection and security, the paper examines how and to what extent enforcing the right to the protection of personal data can improve the security of the cyberspace. It pays special attention to the implementation of the principle/approach of data protection by-design, which concerns the adoption of relevant and ad hoc privacy-enforcing technical, technological or organizational solutions in the design specifications and architecture of systems and processes. This principle is nowadays formally endorsed by the European Union data protection legal framework. It is binding and directly applicable in all the EU Member States. Its implementation by relevant cyberspace's stakeholders is both a challenge and an opportunity; the latter is particularly true as per the implications that the by-design approach generates with respect to the enhancement of cybersecurity. However, to be effective and have a significant positive impact on the European societies and their members - both in terms of improved safeguards for privacy and better promotion of cybersecurity, the implementation of the data protection by-design approach should become a cultural element. It should permeate individuals and organisations' beliefs, attitudes and behaviour."
Cybersecurity professionals need hands-on training to prepare for managing the current advanced cyber threats. To practice cybersecurity skills, training participants use numerous software tools in computer-supported interactive learning environments to perform offensive or defensive actions. The interaction involves typing commands, communicating over the network, and engaging with the training environment. The training artifacts (data resulting from this interaction) can be highly beneficial in educational research. For example, in cybersecurity education, they provide insights into the trainees' learning processes and support effective learning interventions. However, this research area is not yet well-understood. Therefore, this paper surveys publications that enhance cybersecurity education by leveraging trainee-generated data from interactive learning environments. We identified and examined 3021 papers, ultimately selecting 35 articles for a detailed review. First, we investigated which data are employed in which areas of cybersecurity training, how, and why. Second, we examined the applications and impact of research in this area, and third, we explored the community of researchers. Our contribution is a systematic literature review of relevant papers and their categorization according to the collected data, analysis methods, and application contexts. These results provide researchers, developers, and educators with an original perspective on this emerging topic. To motivate further research, we identify trends and gaps, propose ideas for future work, and present practical recommendations. Overall, this paper provides in-depth insight into the recently growing research on collecting and analyzing data from hands-on training in security contexts.
Cybersecurity education and training are essential prerequisites of achieving a secure and privacy-friendly digital environment. Both professionals and the general public widely acknowledge the need for high-quality university education programs and professional training courses. However, guides, recommendations, practical tools, and good examples that could help institutions design appropriate cybersecurity programs are still missing. In particular, a comprehensive method to identify skills needed by cybersecurity work roles offered on the job market is missing. This paper aims to provide practical tools and strategies to help higher education providers design good cybersecurity curricula. First, we analyze the content of 89 existing study programs worldwide, collect recommendations of renowned institutions within and outside the EU, and provide a comprehensive survey accompanied by a dynamic web application called Education Map. Based on the knowledge about the current state in cybersecurity education, we design the SPARTA Cybersecurity Skills Framework that provides the currently missing link between work roles and required expertise and shows how to develop a curriculum that reflects job market requirements. Finally, we provide a practical tool that implements the framework and helps education and training providers design new study programs and analyze existing ones by considering the requirements of cybersecurity work roles.
The opioid epidemic, impacted from the proliferation of fentanyl, has added impetus to the need to detect fentanyl, sources of fentanyl, and places where fentanyl and drugs adulterated with fentanyl are available. Many darknet marketplaces (DNMs) have rules that ban fentanyl. However, it is unclear how these affect the fentanyl market. Using the AlphaBay DNM as a case study, we conducted mixed methods qualitative research. We scraped and analyzed data from the AlphaBay I2P website using, among other methods, content and social network analysis, to uncover hidden fentanyl networks. Our research highlights the next evolution of darknet marketplaces - the migration of DNMs from Tor to I2P and the methods that can be used identify fentanyl networks, irrespective of where sites are: I2P, Tor, or multihomed on I2P and Tor. Despite its ban in the Global AlphaBay Rules, our research revealed the sale of fentanyl on the AlphaBay DNM. Unlike previous studies, our findings predominantly revealed the covert sale of fentanyl on AlphaBay and predatory vendors selling illicit drugs, which unbeknownst to buyers, contained fentanyl. To a lesser extent, our findings identified the overt sale of fentanyl patches on AlphaBay. Although we examined only one DNM, the prevalence of the covert sale of fentanyl and the presence of predatory vendors underscores the importance of research that decodes the language of vendors who surreptitiously sell fentanyl or drugs adulterated with fentanyl or other illicit substances. The results of our research can inform strategies aimed at disrupting and dismantling DNM fentanyl networks.
The foci of the present study are to estimate the extent of cyberstalking victimization, and develop, specify, and test a theoretically based model of cyberstalking victimization among college women. A pursuit- and fear-based measure of victimization was utilized, and three leading theories-self-control, opportunity, and control balance-were tested as explanations of cyberstalking victimization. Key concepts from each theoretical perspective were operationalized using primary survey data from a probability sample of college women (N = 1,987) from two large universities. Results show that approximately 3.4% of female students were victims of cyberstalking during the academic year. Findings from a path model revealed significant direct effects for opportunity and having a control deficit on cyberstalking. The effects of self-control were indirect through these two measures, whereas having a control surplus was not related to victimization risk. Overall, findings support the application of these three theoretical approaches to predicting and explaining cyberstalking victimization.
One of the main objectives of this study is to help prioritize targets for law enforcement by analyzing online websites hosting child exploitation material and finding key players within. Key players are defined as websites that display a combination of high connectivity and a lot of hardcore material and would provide the most disruption in a network if they were to be removed. In this study, various strategies based on Principal Component Analysis are presented to identify those nodes that act as the key players in an online child exploitation network. For evaluating the results of these strategies, we consider the results of various attack strategies. The measures for evaluation are the density, clustering coefficient, average path length, diameter, and the number of connected components in the resulting network. The results show that the strategies proposed are more successful at reducing all of the outcome measures than existing strategies.
In Hong Kong, nearly 1,300 women participated by telephone in the International Violence Against Women Survey in 2006. One in five respondents had experienced violence since age 16. Sexual violence (13.4%) was more frequent than physical violence (11.7%). Women were more likely to be abused by men they knew (13.5%) than by strangers (8%). Compared with other surveyed countries, Hong Kong recorded among the lowest rates of violence by both intimate partners and non-partners. These results suggest that cultural influences linked to the interaction of modernization and some protective factors found in the adherence to traditional Chinese values are relevant.
In this paper, we propose a deep learning framework for malware classification. There has been a huge increase in the volume of malware in recent years which poses a serious security threat to financial institutions, businesses and individuals. In order to combat the proliferation of malware, new strategies are essential to quickly identify and classify malware samples so that their behavior can he analyzed. Machine learning approaches are becoming popular for classifying malware, however, most of the existing machine learning methods for malware classification use shallow learning algorithms (e.g. SVM). Recently, Convolutional Neural Networks (CNN), a deep learning approach, have shown superior performance compared to traditional learning algorithms, especially in tasks such as image classification. Motivated by this success, we propose a CNN-based architecture to classify malware samples. We convert malware binaries to grayscale images and subsequently train a CNN for classification. Experiments on two challenging malware classification datasets, Malimg and Microsoft malware, demonstrate that our method achieves better than the state-of-the-art performance. The proposed method achieves 98.52% and 99.97% accuracy on the Malimg and Microsoft datasets respectively.
Purpose The purpose of this study is to review the literature on money laundering and its related areas. The main objective is to identify any gaps in the literature and direct attention towards addressing them. Design/methodology/approach A systematic review of the money laundering literature was conducted with an emphasis on the Pro-Quest, Scopus and Science-Direct databases. Broad research themes were identified after investigating the literature. The theme about the detection of money laundering was then further investigated. The major approaches of such detection are identified, as well as research gaps that could be addressed in future studies. Findings The literature on money laundering can be classified into the following six broad areas: anti-money laundering framework and its effectiveness, the effect of money laundering on other fields and the economy, the role of actors and their relative importance, the magnitude of money laundering, new opportunities available for money laundering and detection of money laundering. Most studies about the detection of money laundering have focused on the use of innovative technologies, banking transactions or real estate- and trade-based money laundering. However, the literature on the detection of shell companies being explicitly used to launder funds is relatively scarce. Originality/value This paper provides insights into an area related to money laundering where research is relatively scant. Shell companies incorporated in the UK alone were identified to be associated with laundering 80bn pound of stolen money between 2010 and 2014. The use of these entities to launder billions of dollars as witnessed through the laundromat schemes and several data leaks clearly indicate the need to focus on illicit financial flows through such entities.
Several system management technologies have been introduced that leverage additional devices on the main board to asynchronously access and control the host's computing resources. One such prominent technology for server systems is the Baseboard Management Controller (BMC), a co-processors with some firmware that allows an administrator to monitor and administer a server remotely. This paper introduces BMCLeech, the first software that brings forensic memory acquisition onto the BMC which makes it very useful for incident response teams. BMCLeech is based on the open source BMC implementation OpenBMC and internally leverages the power of PCILeech, a well-known framework for memory acquisition via DMA. (C) 2020 The Author(s). Published by Elsevier Ltd.
With the arrival of connected and autonomous vehicles, Vehicle-to-Pedestrian (V2P) communications are promising to facilitate efficient future of mobility on the road by ensuring maximum protection and safety for both drivers and pedestrians. However, this new technology poses new security and privacy challenges that should be taken into account. For instance, a probable malicious node claiming to be a legitimate pedestrian or vehicle within the network can impact the traffic flow, or even cause serious congestion and traffic accidents by broadcasting fake observations or phenomena on the roads. Therefore, it is crucial to identify legitimate vehicles and road users against adversaries pretending to be one. The aim of this paper is to address these issues, by proposing a distributed trust management scheme that relies on blockchain technology and a trust computation approach for efficient and secure management of trust relationships between pedestrians and vehicles in Vehicle-to-Pedestrian (V2P) networks.
Applying a random-effect meta-analysis, the current study examines previous empirical findings about three main purposes of Emotional Literacy (EL) correctional programs: improving empathy, mindfulness, and self-regulation among inmate populations. A total of 22 programs in 15 studies in Germany, Portugal, the Netherlands, and the US are analyzed. Results suggest that EL programs significantly improve levels of empathy, mindfulness, and self-regulation, although a possibility of publication bias is observed. Based on these findings, this study concludes that EL programs retain a potential as an effective rehabilitation strategy and should be sought and implemented in U.S. correctional settings.
Breaches of security, a.k.a. security and data breaches, are on the rise, one of the reasons being the well-known lack of incentives to secure services and their underlying technologies, such as cloud computing. In this article, I question whether the patchwork of six EU instruments addressing breaches is helping to prevent or mitigate breaches as intended. At a lower level of abstraction, the question concerns appraising the success of each instrument separately. At a higher level of abstraction, since all laws converge on the objective of network and information security - one of the three pillars of the EU cyber security policy - the question is whether the legal 'patchwork' is helping to 'patch' the underlying insecurity of network and information systems thus contributing to cyber security. To answer the research question, I look at the regulatory framework as a whole, from the perspective of network and information security and consequently I use the expression cyber security breaches. I appraise the regulatory patchwork by using the three goals of notification identified by the European Commission as a benchmark, enriched by policy documents, legal analysis, and academic literature on breaches legislation, and I elaborate my analysis by reasoning on the case of cloud computing. The analysis, which is frustrated by the lack of adequate data, shows that the regulatory framework on cyber security breaches may be failing to provide the necessary level of mutual learning on the functioning of security measures, awareness of both regulatory authorities and the public on how entities fare in protecting data (and the related network and information systems), and enforcing self-improvement of entities dealing with information and services. I conclude with some recommendations addressing the causes, rather than the symptoms, of network and information systems insecurity. (C) 2018 Maria Grazia Porcedda. Published by Elsevier Ltd. All rights reserved.
Democracy requires transparency. Consequently, courts of law must publish their decisions. At the same time, the interests of the persons involved in these court decisions must be protected. For this reason, court decisions in Europe are anonymized using a variety of techniques. To understand how well these techniques protect the persons involved, we conducted an empirical experiment with 54 law students, whom we asked to de-anonymize 50 German court decisions. We found that all anonymization techniques used in these court decisions were vulnerable, most notably the use of initials. Since even supposedly secure anonymization techniques proved vulnerable, our work empirically reveals the complexity involved in the anonymization of court decisions, and thus calls for further research to increase anonymity while preserving comprehensibility. Toward that end, we provide recommendations for improving anonymization quality. Finally, we provide an empirical notion of reasonable effort, to flesh out the definition of anonymity in the legal context. In doing so, we bridge the gap between the technical and the legal understandings of anonymity.
Research summaryDarknet marketplaces (DNMs) are global digital marketplaces used primarily to buy and sell illicit drugs online. High rates of adulterated substances have contributed to the creation of harm reduction policies by DNM administrators to address growing rates of overdose worldwide. This paper explores the extent to which harm reduction occurs in buyer feedback of Adderall and Oxycodone purchased on AlphaBay and how these comments are impacted by AlphaBay's administrator-led harm reduction policy. This study finds that harm reduction strategies are present in buyer feedback of Oxycodone and Adderall pills, but AlphaBay's policy has very little impact on the preexisting harm reduction communication within buyer feedback.Policy implicationsInternational policy proposals have placed emphasis on addressing the overdose crisis through harm reduction programs that provide people who use drugs with the necessary services and resources to buy and use drugs safely. There have been very few proposals that have considered how these programs can address the unique setting of buying and using drugs purchased on DNMs. Communication occurring among DNM buyers reveals how harm reduction strategies are being employed by users purchasing drugs from DNMs. In particular, these findings offer insight into the shared experiences of drug buyers in anonymous settings and the strategies they are using to protect one another from overdose and other unwanted side effects often caused by adulterated substances. Understanding these strategies highlights the ways in which street-based harm reduction programs can extend their services to online environments to assist buyers with making safe and informed decisions when using substances purchased online.
The article is devoted to the study of the problems of polygraphic research to obtain forensically significant infounation. An analysis of the legal basis for the use of the polygraph in Ukraine. Problematic issues concerning the appropriateness of using a polygraph in the investigation and detection of crimes have been studied. The domestic legal norms that regulate this issue, as well as foreign experience are analyzed. The article reveals the essence of the polygraph, the legal basis and requirements for its use. Attention is drawn to the main difficulties of using a polygraph and ways to solve them.
Phishing attacks are increasingly more sophisticated, with attackers exploiting publicly available information on their targets to personalize their attacks. Although an increasing body of research has investigated the effectiveness of tailored phishing campaigns, researchers have primarily focused on large enterprises. Company size, composition, and resource availability (e.g., of security experts or a phishing response team handling incidents) play an important role in the studied dynamics. However, whether the same also applies to small and medium-sized enterprises (SMEs), which typically do not have those resources, is unclear. On the other hand, studying SME security is hard as they generally have no expertise in-house to run the required experiments. This work provides a first study filling this gap by investigating the effectiveness of tailored phishing campaigns against an SME IT company in Europe. To this end, we conducted a field experiment targeting 30 employees at an SME and, subsequently, interviewed nine employees to understand the cognitive processes underlying the detection and response of our phishing campaign as well as the group defense mechanisms at the SME. Our findings show that expectation mismatch was the primary method for detecting our phishing email and that the collective defense mechanism enabled a surprisingly prompt response and containment of the attack, possibly, due to the network dynamics of a small company.
End users present a key challenge for the protection of contemporary information security systems. The manipulation of people through deceit to gain access to sensitive information and otherwise secure systems is known to hackers, information security practitioners, and other technologists as social engineering. To date, little research has investigated the attributes that people who engage in such deception - so-called social engineers - associate with vulnerable targets. To address this gap, this study engages in a grounded theory-based analysis of interviews with nonprofessional and professional social engineers. The results describe six attributes of a model victim for social engineers, a hypothetical person considered particularly susceptible to social engineering deceptions: (1)prized, (2)uninformed, (3)unconcerned, (4)outgoing, (5)connected, and (6)controlled. Additionally, this study describes heuristic categories described by participants to help make decisions about target vulnerability which include targetsocio-demographic characteristics, social roles, andorganizational positions. Implications for theory, future research, and policy are considered.
In this work we study, design, and evaluate computational methods to support interpretation of statutory terms. We propose a novel task of discovering sentences for argumentation about the meaning of statutory terms. The task models the analysis of past treatment of statutory terms, an exercise lawyers routinely perform using a combination of manual and computational approaches. We treat the discovery of sentences as a special case of ad hoc document retrieval. The specifics include retrieval of short texts (sentences), specialized document types (legal case texts), and, above all, the unique definition of document relevance provided in detailed annotation guidelines. To support our experiments we assembled a data set comprising 42 queries (26,959 sentences) which we plan to release to the public in the near future in order to support further research. Most importantly, we investigate the feasibility of developing a system that responds to a query with a list of sentences that mention the term in a way that is useful for understanding and elaborating its meaning. This is accomplished by a systematic assessment of different features that model the sentences' usefulness for interpretation. We combine features into a compound measure that accounts for multiple aspects. The definition of the task, the assembly of the data set, and the detailed task analysis provide a solid foundation for employing a learning-to-rank approach.
Capture the Flag games represent a popular method of cybersecurity training. Providing meaningful insight into the training progress is essential for increasing learning impact and supporting participants' motivation, especially in advanced hands-on courses. In this paper, we investigate how to provide valuable post-game feedback to players of serious cybersecurity games through interactive visualizations. In collaboration with domain experts, we formulated user requirements that cover three cognitive perspectives: gameplay overview, person-centric view, and comparative feedback. Based on these requirements, we designed two interactive visualizations that provide complementary views on game results. They combine a known clustering and time-based visual approaches to show game results in a way that is easy to decode for players. The purposefulness of our visual feedback was evaluated in a usability field study with attendees of the Summer School in Cyber Security. The evaluation confirmed the adequacy of the two visualizations for instant post-game feedback. Despite our initial expectations, there was no strong preference for neither of the visualizations in solving different tasks.
Intrusion detection in computer networks is of great importance because of its effects on the different communication and security domains. The detection of network intrusion is a challenge. Moreover, network intrusion detection remains a challenging task as a massive amount of data is required to train the state-of-the-art machine learning models to detect network intrusion threats. Many approaches have already been proposed recently on network intrusion detection. However, they face critical challenges owing to the continuous increase in new threats that current systems do not understand. This paper compares multiple techniques to develop a network intrusion detection system. Optimum features are selected from the dataset based on the correlation between the features. Furthermore, we propose an AdaBoost-based approach for network intrusion detection based on these selected features and present its detailed functionality and performance. Unlike most previous studies, which employ the KDD99 dataset, we used a recent and comprehensive UNSW-NB 15 dataset for network anomaly detection. This dataset is a collection of network packets exchanged between hosts. It comprises 49 attributes, including nine types of threats such as DoS, Fuzzers, Exploit, Worm, shellcode, reconnaissance, generic, and analysis Backdoor. In this study, we employ SVM and MLP for comparison. Finally, we propose AdaBoost based on the decision tree classifier to classify normal activity and possible threats. We monitored the network traffic and classified it into either threats or non-threats. The experimental findings showed that our proposed method effectively detects different forms of network intrusions on computer networks and achieves an accuracy of 99.3% on the UNSW-NB15 dataset. The proposed system will be helpful in network security applications and research domains.
PurposeThis study aims to develop a unified theoretical framework that presents a cohesive picture of workplace cyberbullying to better understand the interplay between cyberbullying, its effects on organizations and organizational controls enacted to contain these effects.Design/methodology/approachThe study conducts a theoretical review of the workplace cyberbullying literature between 2005 and 2021 drawing upon existing literature and two important theories, the routine activities theory and control theory. The final sample of 54 empirical papers represents a comprehensive body of literature on cyberbullying published across various disciplines.FindingsA theoretical model of workplace cyberbullying is developed, which highlights major antecedents to workplace cyberbullying and its impact on individual employees as well as organizations.Originality/valueAs firms increasingly rely on information and communication technologies (ICTs), the misuse of ICTs in the form of cyberbullying is also increasing. Workplace cyberbullying severely hurts an organization's employees and compromises the efficacy of its information systems. Fortunately, various controls can be utilized by firms to minimize workplace cyberbullying and its attendant costs. In all, eleven propositions are offered, providing a robust agenda for future research. The authors also offer insights for practitioners on how to minimize cyberbullying in the workplace and its damaging effects.
Currently, the Dark Web is one key platform for the online trading of illegal products and services. Analysing the . onion sites hosting marketplaces is of interest for law enforcement and security researchers. This paper presents a study on 123k listings obtained from 6 different Dark Web markets. While most of current works leverage existing datasets, these are outdated and might not contain new products, e.g., those related to the 2020 COVID pandemic. Thus, we build a custom focused crawler to collect the data. Being able to conduct analyses on current data is of considerable importance as these marketplaces continue to change and grow, both in terms of products offered and users. Also, there are several anti-crawling mechanisms being improved, making this task more difficult and, consequently, reducing the amount of data obtained in recent years on these marketplaces. We conduct a data analysis evaluating multiple characteristics regarding the products, sellers, and markets. These characteristics include, among others, the number of sales, existing categories in the markets, the origin of the products and the sellers. Our study sheds light on the products and services being offered in these markets nowadays. Moreover, we have conducted a case study on one particular productive and dynamic drug market, i.e., Cannazon. Our initial goal was to understand its evolution over time, analyzing the variation of products in stock and their price longitudinally. We realized, though, that during the period of study the market suffered a DDoS attack which damaged its reputation and affected users' trust on it, which was a potential reason which lead to the subsequent closure of the market by its operators. Consequently, our study provides insights regarding the last days of operation of such a productive market, and showcases the effectiveness of a potential intervention approach by means of disrupting the service and fostering mistrust.
Anonymous systems (e.g. anonymous cryptocurrencies and updatable anonymous credentials) often follow a construction template where an account can only perform a single anonymous action, which in turn potentially spawns new (and still single-use) accounts (e.g. UTXO with a balance to spend or session with a score to claim). Due to the anonymous nature of the action, no party can be sure which account has taken part in an action and, therefore, must maintain an ever-growing list of potentially unused accounts to ensure that the system keeps running correctly. Consequently, anonymous systems constructed based on this common template are seemingly not sustainable. In this work, we study the sustainability of ring-based anonymous systems, where a user performing an anonymous action is hidden within a set of decoy users, traditionally called a ring. On the positive side, we propose a general technique for ring-based anonymous systems to achieve sustainability. Along the way, we define a general model of decentralised anonymous systems (DAS) for arbitrary anonymous actions, and provide a generic construction which provably achieves sustainability. As a special case, we obtain the first construction of anonymous cryptocurrencies achieving sustainability without compromising availability. We also demonstrate the generality of our model by constructing sustainable decentralised anonymous social networks. On the negative side, we show empirically that Monero, one of the most popular anonymous cryptocurrencies, is unlikely to be sustainable without altering its current ring sampling strategy. The main subroutine is a sub-quadratic-time algorithm for detecting used accounts in a ring-based anonymous system.
Taxonomies and ontologies are handy tools in many application domains such as knowledge systematization and automatic reasoning. In the cyber security field, many researchers have proposed such taxonomies and ontologies, most of which were built based on manual work. Some researchers proposed the use of computing tools to automate the building process, but mainly on very narrow sub -areas of cyber security. Thus, there is a lack of general cyber security taxonomies and ontologies, possibly due to the difficulties of manually curating keywords and concepts for such a diverse, inter-disciplinary and dynamically evolving field. This paper presents a new human-machine teaming based process to build taxonomies, which allows human experts to work with automated natural language processing (NLP) and information retrieval (IR) tools to co-develop a taxonomy from a set of relevant textual documents. The proposed process could be generalized to support non-textual documents and to build (more complicated) ontologies as well. Using the cyber security as an example, we demonstrate how the proposed taxonomy building process has allowed us to build a general cyber security taxonomy covering a wide range of data-driven keywords (topics) with a reasonable amount of human effort.
We consider the structures of the plane-line and point-line inci-dence matrices of the projective space PG(3 , q) connected with orbits of planes, points, and lines under the stabilizer group of the twisted cubic. In the literature, lines are partitioned into classes, each of which is a union of line orbits. In this paper, for all q , even and odd, we determine the incidence matrices connected with a family of orbits of the class named O6. This class contains lines external to the twisted cubic. The consid-ered family includes an essential part of all O6 orbits, whose complete classification is an open problem.
Machine learning is becoming increasingly popular in modern technology and has been adopted in various application areas. However, researchers have demonstrated that machine learning models are vulnerable to adversarial examples in their inputs, which has given rise to a field of research known as adversarial machine learning. Potential adversarial attacks include methods of poisoning datasets by perturbing input samples to mislead machine learning models into producing undesirable results. While such perturbations are often subtle and imperceptible from the perspective of a human, they can greatly affect the performance of machine learning models. This paper presents two methods of verifying the visual fidelity of image-based datasets by using QR codes to detect perturbations in the data. In the first method, a verification string is stored for each image in a dataset. These verification strings can be used to determine whether or not an image in the dataset has been perturbed. In the second method, only a single verification string is stored and can be used to verify whether an entire dataset is intact.y
Recently, smart cities have increasingly been experiencing an evolution to improve the lifestyle of citizens and society. These emerge from the innovation of information and communication technologies (ICT) which are able to create a new economic and social opportunities. However, there are several challenges regarding our security and expectation of privacy. People are already involved and interconnected by using smart phones and other appliances. In many cities, smart energy meters, smart devices, and security appliances have recently been standardized. Full connectivity between public venues, homes, cares, and some other social systems are on their way to be applied, which are known as Internet of Things. In this paper, we aim to enhance the performance of security in smart city communication networks by using a new framework and scheme that provide an authentication and high confidentiality of data. The smart city system can achieve mutual authentication and establish the shared session key schemes between smart meters and the control center in order to secure a two-way communication channel. In our extensive simulation, we investigated and evaluated the security performance of the smart city communication network with and without our proposed scheme in terms of throughput, latency, load, and traffic received packet per seconds. Furthermore, we implemented and applied a man-in-the-middle (MITM) attack and network intrusion detection system (NIDS) in our proposed technique to validate and measure the security requirements maintaining the constrained resources.
Cyber strategies play a role in combating child sexual abuse material (CSAM). These strategies aim to detect offenders and prevent them from accessing and producing CSAM, or to identify victims. This paper explores five cyber strategies: peer-to-peer network monitoring, automated multi-modal CSAM detection tools, using web crawlers to identify CSAM sites, pop-up warning messages, and facial recognition. This research synthesis captures the background of each strategy, how it works and the evaluative research, along with the benefits, limitations and implementation considerations, offering a practical overview for a broad audience.
Online consumer fraud is one of the most fast-growing crimes. It corresponds to situations where, for instance, the products that are bought, but are not delivered. The present study (N = 1710) aimed at exploring, on the one hand, the predictors of online fraud victimization and, on the other hand, to examine the reporting to the police. Even though 223 were victims of online consumer fraud, only 20 reported to the police. Moreover, while individual variables were not predictors of victimization, online routines were relevant to its explanation. Finally, monetary loss was the most important predictor of reporting to the police.
Due to the nature of mobile systems in which devices change their locations continuously, there is a need for efficient techniques to integrate responses from different agents. Such techniques implement cross-layering service migration models to handle integration issues. This paper proposes a method that aims to model the migration process in an integrated architecture to resolve cross-layering issues, develop a dynamic policy, and optimize the cost of migration in terms of power consumption and communication. The proposed technique extends the Markovian model to adopt the spatiotemporal aspects of this problem, allowing modern 5G vertical appli-cations to communicate seamlessly in highly connected IoT networks. Because of its dynamicity, we propose to project the environmental aspects into the observation part of HMM. We performed experiments to test the performance of the proposed method in terms of efficiency, reliability, and complexity. The results showed a significant performance in the mobile edge computing environment.
With continuously escalating threats and attacks, accurate and timely intrusion detection in communication networks is challenging. Many approaches have already been proposed recently on network intrusion detection. However, they face critical challenges due to the continuous increase of new threats that current systems do not understand. Motivated by the outstanding performance of deep learning (DL) in many detection and recognition tasks, we introduce an intelligent and efficient network intrusion detection system (NIDS) based on DL. This study proposes a non-symmetric deep auto-encoder for network intrusion detection problems and presents its detailed functionality and performance. We validate the robustness and effectiveness of the proposed NIDS using a benchmark dataset, i.e., KDD CUP'99. Our DL-based method is implemented in the TensorFlow library and GPU framework, and it achieves an accuracy of 99.65%. The proposed system can be used in network security research domains and DL-based detection and classification systems.
Street gangs, by definition, enjoy a special relationship with the street. Prior research shows that some communities are synonymous with gangs and that turf holds a combination of expressive and instrumental value for gang members. As gangs evolve over time and through different levels of organization, however, gangs relationship with the street changes. This shifting street dynamic is underexplored in prior research, thus, drawing on qualitative data from Scotland and Bourdieus theory of social field, the current study presents three cases of gangs at different stages of evolution and examines how levels of gang organization affect spatial relationships. As gangs accumulate sufficient street capital to evolve, we find territory is defined less physically and more relationally, with implications for gang research and practice.
In this paper, we present a combined experimental and analytical investigation of the impact of security compliance on a three-tier web application hosted on a virtualized platform. We used two-group experimental design for our experiments, and analyzed the impact of security using the ANCOVA model. The results of experiments suggest that security measures have significant impact on system performance of web application, particularly on the web and the database tiers. We modeled the three-tier web application using Queueing Network and incorporated security delays in the existing MVA based solution. We compared the results of the experiments with the MVA model results.
This work presents a conceptual proposal to address the problem of intensive human specialized resources that are nowadays required for the maintenance and optimized operation of digital contents filtering in general and anti-spam filtering in particular. The huge amount of spam, malware, virus, and other illegitimate digital contents distributed through network services, represents a considerable waste of physical and technical resources, experts and end users time, in continuous maintenance of anti-spam filters and deletion of spam messages, respectively. The problem of cumbersome and continuous maintenance required to keep anti-spam filtering systems updated and running in an efficient way, is addressed in this work by the means of genetic programming grammatical evolution techniques, for automatic rules generation, having SpamAssassin anti-spam system and SpamAssassin public corpus as the references for the automatic filtering customization. (C) 2014 The Authors. Published by Elsevier Ltd.
Accessing online support services can be dangerous for some users, such as domestic abuse survivors. Many support service websites contain quick exit buttons that provide an easy way for users to escape the site. We investigate where exit buttons and other escape mechanisms are currently in use (country and type of site) and how they are implemented. We analyse both the security and usability of exit mechanisms on 323 mobile and 404 desktop sites. We find exit buttons typically replace the current page with another site, occasionally opening additional tabs. Some exit buttons also remove the page from the browser history. When analysing the design choices and shortcomings of exit button implementations, common problems include cookie notices covering the buttons, and buttons not remaining on the screen when scrolling. We provide recommendations for designers of support websites who want to add or improve this feature on their website.
"The smallest possible length of a q-ary linear code of covering radius R and codimension (redundancy) r is called the length function and is denoted by l(q)(r, R). In this work, for q an arbitrary prime power, we obtain the following new constructive upper bounds on l(q)(3t + 1, 3): l(q)(r, 3) (sic) 3 root k . q((r-3)/3) . 3 root ln q, r = 3t + 1, t >= 1, q >= [W(k)], 18 < k <= 20.339, W(k) is a decreasing function of k; l(q)(r, 3) (sic) 3 root 18 . q((r-3)/3) . 3 root ln q, r = 3t + 1, t >= 1, q large enough. For t = 1, we use a one-to-one correspondence between codes of covering radius 3 and codimension 4, and 2-saturating sets in the projective space PG(3, q). A new construction providing sets of small size is proposed. The codes, obtained by geometrical methods, are taken as the starting ones in the lift-constructions (so-called q(m)-concatenating constructions) to obtain infinite families of codes with radius 3 and growing codimension r = 3t+ 1, t >= 1. The new bounds are essentially better than the known ones."
Tampered digital evidence may jeopardize its correct interpretation. To assess the risks in a court of law, it is helpful to quantify the necessary effort to perform a convincing manipulation of digital evidence. Based on a sequence of controlled experiments with graduate students and digital forensics professionals, we study the effort to manipulate copies of main memory taken during a digital investigation. Confirming previous results on hard disc image tampering, manipulating main memory dumps can be considered hard in the sense that most forgeries were successfully detected. However, while the effort to detect a manipulation is generally bounded by the tampering effort, some forgeries fooled the analysts and caused analysis effort that was higher than the manipulation effort. The detection effort by graduate students, however, was generally higher than that of professionals. We study different manipulation and detection approaches and their success. Overall, tampering with main memory dumps appears to be harder than tampering with hard disc images but the probability to fool an analyst is higher too. (C) 2020 The Authors. Published by Elsevier Ltd.
Digital forensic investigations has become an important field in this era due to the raise of cybercrimes. Therefore, most governments and companies found the urgent need to invest more in research related to digital forensic investigations. To perform digital forensic investigations covering extraction, analysis, and reporting of digital evidences, new methods and techniques are required. One of these methods used when applying digital forensics on a Windows operating system, is PowerShell. While PowerShell is mainly used to configure, manage and administrate the Windows operating system and other installed programs, this paper will also show that it could be used to collect forensic evidences from a Windows operating system. This paper will discuss Windows PowerShell functions and how they can be beneficiary to a digital forensic investigator. Moreover, the paper will focus on the tools and modules made specifically for forensic investigations. Subsequently, different digital forensic experiments will be conducted using PowerForensics tool in order to extract and identify different Windows forensic artifacts. The results are presented the capabilities of PowerForensics tool to extract forensic evidences from Windows operating system and provide an insight into its limitations.
Industrial control systems (ICSs) are, at present, extremely vulnerable to cyber attack because they are homogenous and interconnected. Mitigating solutions are urgently required because systems breaches can feasibly lead to fatalities. In this paper we propose the deployment of permuted code onto Physically Unclonable Unique Processors in order to resist common cyber attacks. We present our proposal and explain how it would resist attacks from hostile agents.
Internet of Things (IoT) consists of a large number of connected objects that are communicating with each other. In order to support trusted communication between IoT objects, effective authentication procedures should be applied between the communicating entities. In this paper a survey of IoT authentication techniques, which are proposed in the literature, is presented. The survey aims to help other researchers in delving into the details of such techniques by going through their classification and comparison. The classification has been done based on the inherent features of these authentication technique such as being distributed vs. centralized, flat vs. hierarchical, and more others. A comparison between these techniques according to the used evaluation models and their security analysis is illustrated.
Cyber criminals increasingly target Small and Medium Sized Businesses (SMEs) since they are perceived to have the weakest defences. Some will not survive a cyber attack, and others will have their ability to continue trading seriously impaired. There is compelling evidence that, at present, SMEs do not seem to be implementing all the advisable security measures which could help them to resist such attacks. Many in the security industry believe that this is because SMEs do not take the threat seriously. This paper reports on a study to find out whether this is the case, or not. The primary finding is that most SMEs do care about the threat but that very few implement even a small subset of the available security precautions. One contributory factor seemed to be the uncertainty caused by the wealth of conflicting and confusing online advice offered by industry and official bodies. This seemed to be hindering rather than helping SMEs so that they did not know what actions to take to improve their resilience. The conclusion is a recommendation for actions to be taken to better inform SMEs and help them to secure their systems more effectively.
Focusing on two of the most common deceptive strategies employed by fraudsters, we assess which type of cue-politeness or urgency-is more likely to result in an email fraud attempt. We also examine whether these cues are mutually exclusive and consistent throughout the progression of a fraud attempt. To answer our research questions, we posted for-sale advertisements on classified-advertisement websites and interacted with fraudsters who responded to our advertisements. Findings reveal that fraud attempts are more likely to follow probe emails that include cues of urgency than cues of politeness. Moreover, although the majority of fraudsters' probe emails include deceptive cues of either politeness or urgency, the majority of subsequent emails include deceptive cues of both politeness and urgency.
In this position paper, we argue for a holistic perspective on threat analysis and other studies of state-sponsored or state-aligned eCrime groups. Specifically, we argue that understanding eCrime requires approaching it as a sociotechnical system and that studying such a system requires combining linguistic, regional, professional, and technical expertise. To illustrate it, we focus on the discourse of the Conti ransomware group in the context of the Russian invasion of Ukraine. We discuss the background of this group and their actions and argue that the technical approach alone can lose the important aspects specific to the cultural and linguistic context, such as language, slang and humor. We provide examples of how the discourse and threats from such groups can be easily misunderstood without appropriate linguistic and domain expertise.
Websites typically include many forms or web elements that allow users to enter and submit data. This data will be eventually executed in the back-end databases. Users can, intentionally or unintentionally enter improper input data that, if reach those back-end databases, may cause some serious security or damaging problems. For proper user interface design as well as for security reasons, it is important for web-designers to consider input-validation techniques at the user interface level or as early as possible. The goal is to stop further actions for any invalid input data. In this paper, we conducted an evaluation study of how much input validation is used by web-designers. We used some of the web attacks that target improper input validations as indicators to show the quality of the input validation process for the evaluated websites. Results showed that those types of attacks continue to be effective and serious methods. Results showed also that there is a need for systematic and frequent evaluation for those websites to ensure that basic input validation guidelines are observed. This paper includes the following contributions: An SQL-injection-vulnerability assessment of a dataset of several selected websites, and the proposal of a mutation, fault-based model to test websites for possible SQL-injection vulnerabilities.
"The incorporation of ICT in public sector organisations is progressing rapidly in Oman where the government sees this as a means to enhance the delivery of online services. In this context, preserving the security of information, and making Information Security a core organisational aspect in public sector organisations, requires attention from management. Our research is the first known attempt to gauge management attitudes toward Information Security in Oman. We also consider how such attitudes influence Information Security governance. In addressing these issues, we review current compliance with Information Security procedures in Omani public sector organisations; review management attitudes toward Information Security governance practices; and explore how management attitudes toward Information Security impact upon these aspects."
The nature of today's online communication and the emergence of online social networks have introduced a great challenge related to the identification and the verification of users in web environments. The assumption that every user interact uniquely with a web site provides a baseline for studying user identification based on historical records of user's interactions with specific web site. In this paper, we proposed an approach for using clickstream data to identify users based on their navigational behavior. The study investigates using server-side clickstream data collected from previous users' interaction to create a behavioral profile per user. User's profile can be used to identify possible future interactions, which can be associated with distinguishing authenticated user from malicious user. To accomplish this identification, the ability to gather a historical record for user's navigation in a specific web site, is investigated. The goal of the collected data is to use it for training a model that is able to identify if a future interaction can be associated with a certain user.
Data theft and espionage attacks, which is evolved rapidly, impose the need to develop new and stealthy communication techniques to protect sensitive data that is transferred over the Internet. These new techniques should be built in different way than traditional and known communication techniques, in order to eschew detection and monitoring tools. This paper proposes a new covert channel for stealthy communication. The communication channel uses Least Significant Bit steganography and Tariq port knocking to hide data, besides that GnuPG encryption is applied before hiding the data to add another layer of protection. The communication efficiency has been evaluated using Peak Signal to Noise Ratio, and the result indicates better image quality compared with other techniques. Also, the maximum capacity and transfer rate of the channel have been evaluated, the channel is capable to achieve 152 bps as a maximum transmission rate in its current implementation.
Cyber-attacks are on the rise due to the increased usage of social networking application's built-in Android devices via Wi-Fi connections, which has resulted in privacy issues. Several studies have been conducted to investigate Android phones, however, none of these have proposed a comprehensive Android investigation method, which begins with a Man-in-the-Middle attack and ending in a criminal investigation. The purpose of this research is to propose an Android forensics framework against such Wi-Fi attacks, using advanced forensic tools, such as the Cellebrite Universal Forensic Extraction Device and the Oxygen. This will assist the researcher to prove the suggested arguments in the following: 1. To implement guidelines for the forensic examiners, especially for those new in the field of forensics, and 2. To guide Android and social networking application developers to enhance the level of security. Furthermore, this study recommends the best data extraction methods designed for Android devices.
"In this study we investigate malicious spam emails in the context of educational institutes. The goal of the study is two folds; first, is to explore spam types of attacks and what their malicious contents may include and secondly, to analyze if these attacks exhibit discriminative characteristics. This study offers an empirical analysis of spam emails dataset and provides a rich set of features that exist in the dataset. The features could then be used by researchers to build new intelligent systems that are capable of classifying and blocking spam emails."
With the advent of Advanced Persistent Threats (APTs) and exploits such as Eurograbber, we can no longer trust the users PC or mobile phone to be honest in their transactions with banks. This paper reviews the current state of the art in protecting PCs from malware and APTs that can modify banking transactions, and identifies their strengths and weaknesses. It then proposes an enhanced USB device based on speech and vision. User trials with a software prototype show that such a device is both user friendly and that users are less susceptible to accepting subtly modified transaction with this device than with other vision only USB devices. Since human factors are usually the weakest point in the security chain, and are often the way that APT actors perform their attacks, the focus of the proposed solution is on improving the usability of existing USB devices. However the device is still not failsafe, and therefore may not be as preferable as Sm@rt TAN-plus that is currently used by many German banks.
In order to identify attacks and malicious behaviors, network packets must be intercepted and analyzed. However, the amount of data logged will be enormous. Trying to process manually this volume of traffic would be massively expensive. This research presents an integrated framework IWNetFAF that captures and analyzes the 802.11 wireless network traffic. IWNetFAF meets the common requirements of a wireless network forensics, namely: capturing wireless network traffic, analyzing the captured traffic according to the investigation's needs, and extracting and documenting digital evidence from the analyzed traffic.
"This paper presents a review of recent academic scholarship and debates on cyber terrorism, and more broadly of what is known about terrorist's direct use of the Internet as weapon and, less directly, as a communication device. It presents an overview of a field of discourse that has, since its inception, provided a number of foreboding and even doomsday warnings about the future of cyber terrorism, which in the main have failed to come to realization. First, it surveys why these gloomy warnings regarding future proliferation of cyber terrorism have not been born out in practice, and explains that rather than looking for instances of the Internet being used directly as a weapon by terrorists, current debates in academic and policy circles have shifted to trying to measure and ascertain the role that the Internet plays in spreading and supporting extremist discourse to ever wider audiences. It continues by posing a series of questions regarding online audiences that are in need of future research if we are to better understand the role of the Internet in spreading and supporting violent extremist discourse and cultivating terrorism; most importantly the role of audiences as autonomous agents in navigating, reacting and responding to online violent extremist materials."
Every day, hundreds of people fly on airline tickets that have been obtained fraudulently. This crime script analysis provides an overview of the trade in these tickets, drawing on interviews with industry and law enforcement, and an analysis of an online blackmarket. Tickets are purchased by complicit travellers or resellers from the online blackmarket. Victim travellers obtain tickets from fake travel agencies or malicious insiders. Compromised credit cards used to be the main method to purchase tickets illegitimately. However, as fraud detection systems improved, offenders displaced to other methods, including compromised loyalty point accounts, phishing, and compromised business accounts. In addition to complicit and victim travellers, fraudulently obtained tickets are used for transporting mules, and for trafficking and smuggling. This research details current prevention approaches, and identifies additional interventions, aimed at the act, the actor, and the marketplace.
"PurposeThis study aims to examine social determinants and social strains of cyberbullying victimization among expatriate populations in high-income countries such as Qatar. The authors argue that expatriate students will be exposed to stains and pressures due to several factors, such as feeling alienated, lonely, homesick, insecure and helpless. This study examines a partial assumption of general strain theory (GST), which posits that expats' cyberbullying victimization can create negative stimuli that lead to negative feelings and, as a result, to delinquent behavior. The delinquent behavior can be seen as a coping method in encountering strains. Logistic regression analysis is applied (using SPSS v. 21) to test the hypothesis that the victimization of expats' cyberbullying could lead to strain and delinquency.Design/methodology/approachA sample of 2,428 expatriate students (46% males and 54% females) was randomly selected from seven public schools in Qatar. The average number of siblings in the household was 2.7. Around 10.8% of the sample reported victimization by cyberbullying. Most came from intact families (80.6%) compared to 19.4 living in broken homes. Concerning the birth order of the student in the family, 25% of the sample were firstborn, and 20% were middle-born. Students' mother relationships were higher than students' father relationships (69% vs 51%, respectively). Fathers were higher in undergraduate education than mothers (60% vs 49%, respectively). Considering that 58% of women did not have a job, most fathers work in administrative positions (47% in administrative work and 39% in professional positions) as opposed to mothers' positions (15% and 21%, respectively). The survey was distributed among expatriate students by trained researchers, teachers and social workers in public schools. The researchers explained the purpose of the study, confidentiality and potential risks and provided directions for completing the survey. Parents and their children provided signed informed consent before participation, following the institutional review boards of Qatar University's Human Subject Research Committee and the Ministry of Social Development. The victimization of cyberbullying is measured at the dichotomy level by asking a general cyberbullying question (In the last 12 months, had you been bullied by other students using mobile messages, email, voice or video messages? Responses were 1 - yes and 0 - no). A definition of cyberbullying was included in the questionnaire. Independent measures are as follows: (1) demographic variables: gender, belief, health, number of friends and exposure to violence; (2) family variables: delinquent family, coercive parenting, family relations, family disputes, father absence and family ties; (3) school variables: school satisfaction, school violence, student fighting, teachers' violence, school truancy and going late to school; (4) imprudent behavior: smoking, alcohol, not using a seat belt, gambling, stealing less than 100 QR, chewing tobacco, stimuli, volatile drugs, sexual harassment, throwing garbage out of the car, cheating, vandalism and victimization.FindingsFindings showed that n = 255, 10.8% of the expats' sample N = 2,428 reported victimization of cyberbullying, of whom 46% were males and 54% were females. A total of 24% of the victims of cyberbullying were males, and 5.8% were females. About a third of the sample were cyberbullying perpetrators (n = 716, 29.5%). Victims of cyberbullying came from intact families (62.4%) compared to broken families (37.6%). A round third of the victims reported the absence of a father during their childhood (n = 78, 31.7%). Most of the victims came from a typical community compared to the delinquent community (71.2% vs 28.2%). About a third of the victims reported it was easy to talk with the father when needed (32% compared to 42.7%), who said it was easy. More than half of the victims do not use seat belts. A low percentage of them were gambling (n = 68, 27% or 2.9% of the total sample). Regarding escaping imprudent and delinquent behaviors, findings showed that a third and more than a tenth of the cyberbullying were smokers and alcoholics (n = 69, 27.4% and n = 42, 16.7% respectively). Moreover, among the most serious, widely spread student problems, around a quarter of the victims reported using chewing tobacco (Sweeka is the local name) (n = 54, 21.4%). Finally, drug use among victims was more than a tenth of them (n = 41, 16.3%). About a quarter of the victims reported stealing less than 100 QR (n = 67, 26.8% or 2.9% of the total sample). Concerning the crimes committed by victims, findings showed that more than forth of them committed assault (n = 71, 28.3%), student fights (n = 202, 80.8%), school violence (n = 117, 46.6%) and half of the victims (50.2%) were also victims of face-to-face bullying. Little below half of the sample was exposed to violence (n = 120, 48.6%) and was the victim of violence in the past 12 months (n = 100, 40.8%). A significant gender differences was found in the victimization of cyberbullying (1 = yes, 0 = no) (a = 000). Moreover, significant mean differences were found between expatriates student victimization of cyberbullying at (a = 000) in all strain variables. Using binary regression analysis to examine the equality of probability of being a victim of cyberbullying as accounted for by the independent variables, the model's predictability was 89.4%. The Hosmer and Lemeshow test and omnibus tests support the model's fit (a = 000). Nagelkerke R2 shows that the stain variables accounted for 24% of the variance in the expat's student victimization of cyberbullying.Research limitations/implicationsConceptualization and determination of what constitutes cyberbullying from the criminal law perspective are needed. Cyberbullying is defined as a form of violence and crime involving new technologies (Sun et al., 2016, p. 62). Policymakers should provide accessible and equitable access to the criminal justice system, the integration of expatriates and the provision of support services to avoid delinquency.Practical implicationsA social cohesion, inclusion and well-being policy is needed for expatriate students and their families in Qatar. In particular, cultural diversity policy and programs, a source of strength and enrichment should be promoted in educational settings. Social inclusion and cultural diversity programs could ease the alienation and marginalization that expatriate students may encounter in their host societies to prevent stains and negative emotions that lead to delinquency and criminal behaviors. Access to support services without discrimination to avoid health and psychological strains and risk factors. This includes but is not limited to avoidance of stigmatization, embarrassment, sense of helplessness, humiliation and other negative feelings toward expatriates. Awareness and promotion of cultural diversity values are needed to enhance cultural acceptance to reduce victimization among the expatriate population. Criminal law and security implications conceptualization and determination of what constitutes cyberbullying from the criminal law perspective are needed. Cyberbullying is a form of violence and crime involving new technologies (Sun et al., 2016). Policymakers should provide accessible and equitable access to the criminal justice system, integrating expatriates and providing support services to avoid delinquency. There is a pressing need for further research within the realm of crime and law to establish the precise legal boundaries surrounding cyberbullying and to delineate the potential scope for legislative measures aimed at safeguarding against victimization in Qatar. Although the state of Qatar has taken steps to address cyberbullying by incorporating it into relevant legal frameworks, there remains a gap in terms of specifically targeting cyberbullying involving children and adolescents. Despite the introduction of the new Cybercrime Prevention Law, the issue of cyberbullying among these vulnerable groups is not adequately addressed.Social implicationsSocial implication includes but is not limited to avoidance of stigmatization, embarrassment, sense of helplessness, humiliation and other negative feelings towards expatriate. Awareness and promotion of cultural diversity values is needed to enhance cultural acceptance to reduce victimization among expatriate population.Originality/valueThe present study examines some strain theory assumptions and the victimization of the cyberbullying expatriate population. This study tests a partial assumption of GST, which states that the expatriate population's exposure to the victimization of cyberbullying can lead to negative stimuli that, in return, create negative feelings and, as a result, imprudent and delinquent behavior. The cyberbullying behavior can be seen as an escaping and coping method in encountering strains. This study underscores the presence of cyberbullying within high schools among expatriate students, exerting significant effects on their personal, social and emotional behaviors. The novel insights gleaned from this investigation contribute substantively to the comprehension of both the pervasiveness and repercussions of cyberbullying on the well-being of expatriate students. This contribution is particularly vital, given the dearth of research in the field, largely attributable to the growing dependence of students on the internet for various cyber activities. This study examines a partial assumption of GST (in non-Western country). Moreover, it uses an advanced statistical analysis and large sample."
Extant research on the antecedents of workplace cyberbullying pays little attention to the role of perpetrator traits in influencing workplace cyberbullying, as well as the unique occurrence context that distinguishes workplace cyberbullying with juvenile cyberbullying, workplace bullying, and adult cyberbullying in general. To fill these gaps, we consider the antecedents of workplace cyberbullying under the theoretical lens of the general theory of crime and routine activities theory. We build a model incorporating low self-control, a widely discussed perpetrator trait in criminology theories, with three types of routine activities representing the unique occurrence context for workplace cyberbullying-mWork, boundary-spanning in enterprise social media, and proactive email checking. We tested our model with 2025 employees in the United States. Our findings demonstrate that low self-control and the three routine activities are strong motivators for workplace cyberbullying. Our findings further show that the effect of low self-control on workplace cyberbullying is amplified by the three routine activities. The study contributes to our understanding of why workplace cyberbullying occurs and offers potential implications for managers interested in reducing incidences of workplace cyberbullying in their organization.
Cloud computing has become the norm in the provisioning of computing resources due to its flexible and proven reliability. Businesses perceive cloud services as a trend that presents enormous possibilities both in economic and technical terms. The growth in cloud services have also increased bottlenecks and security risks to business assets. Cloud security monitoring has remained relatively unexplored in security terms, a factor that has led businesses to be oblivious on the metrics to capture and the appropriate techniques to use. In this paper, we explore security monitoring in terms of tracking specific user requirements based on a case study. We identify various security tools that are practically relevant for addressing the requirements, and devise selection criteria for choosing the best tools. We present an evaluation of the tools and present a ranking for the tools that meet the particular requirements of the case study. The effort in this paper broadens the notion of cloud security monitoring and provides a methodical practical approach to solving a security related issue.
A building automation system (BAS) is an instance of a cyber-physical-system (CPS) in control of building functionalities like lighting, ventilation, CCTVs, and access control. The amount of smart buildings has been growing over the years, introducing new technologies which are now being targeted by attackers. In this work, we present the first collection of publicly disclosed security incidents involving Building Automation Systems (BAS). We then provide a qualitative study of attackers targeting BAS and unveil their main characteristics and differences to traditional CPS attackers. We learn that, generally speaking, BAS attackers show a lower sophistication level and that most BAS attacks target the smart IoT components present in modern buildings. Further, access to the BAS is often not the attacker's final goal but just a mean to achieve their actual goal. Lastly, we do not observe any advanced, state-sponsored BAS attacks hinting that these play less of a role in BAS (compared to CPS).
To protect a system from potential cyber security breaches and attacks, one needs to select efficient security controls, taking into account technical and institutional goals and constraints, such as available budget, enterprise activity, internal and external environment. Here we model the security controls selection problem as a two-stage decision making: First, managers and information security officers defme the size of security budget. Second, the budget is distributed between various types of security controls. By viewing loss prevention with security controls measured as gains relative to a baseline (losses without applying security controls), we formulate the decision making process as a classical portfolio selection problem. The model assumes security budget allocation as a two objective problem, balancing risk and return, given a budget constraint. The Sharpe ratio is used to identify an optimal point on the Pareto front to spend the budget. At the management level the budget size is chosen by computing the tradeoffs between Sharpe ratios and budget sizes. It is shown that the proposed two-stage decision making model can be solved by quadratic programming techniques, which is shown for a test case scenario with realistic data. (C) 2016 The Authors. Published by Elsevier B.V.
There are many organizations using databases to store and hide confidential data. Some of these data are published through World Wide Web (WWW) and the remaining data are hidden. Unfortunately, the attackers usually try to access and steal these hidden data by attacking the structure and the content of the database using an attacking technique called Structural Query Language Injection Attack (SQLIA). This technique gives the attackers illegal authorization to execute queries on database through the vulnerabilities in input boxes and page URL's. These queries may reveal or change the confidential data. Many techniques are available in the literature to prevent and detect SQLIA. However, these techniques do not consider languages other than the English language such as Arabic, Greek, and Japanese. Therefore, these techniques may not be able to discover attacks using such languages. In this paper, we present a formal approach to detect and prevent common types of SQLIA considering multi-languages. We formalize tautology and alternative encoding attacks using regular expressions and finite automata. We consider cases not discussed in the literature. Furthermore, we provide regular expressions and code in ASP.net which can be used by developers to detect and prevent attacks on websites that use Microsoft SQL server 2014 (MS SQL). We validate our work manually and by using tools. Results show that our model can detect and prevent SQL injection attacks including languages other than the English language.
in this paper user verification and identification system on touch screen mobile devices is proposed. The system examines the keystroke dynamics and uses it as a second authentication factor. The study proposes a prototype for a keyboard application developed for collecting timing and non-timing information from keystroke dynamics. In addition to other mentioned in literature studies, we propose complex password combination, which consists of text, numbers, and special characters. Strengthening access control using artificial neural networking model is suggested. Neural network model based on multilayer perceptron classifier which uses back propagation algorithm is proposed. This paper presents a unique approach for combining timing and non-timing features together, as it includes several non-timing features such pressure, size, and position in addition to the duration time features. Several experiments have been done based on specific machine learning for data mining and classification toolkit named WEKA. The obtained results show that keystroke dynamics provides acceptable level of performance measures as a second authentication factor. The distinguishable role for non-timing features beside the timing features is demonstrated. These features have a significant role for improving the performance measures of keystroke dynamic behavioral authentication. The proposed model achieves lower error rate of false acceptance of 2.2%, false rejection of 8.67%, and equal error rate of 5.43% which are better than most of references provided in the literature.
"The cheapest form of communication in the world today is email, and its simplicity makes it vulnerable to many threats. One of the most important threats to email is spam; unsolicited email, normally with an advertising content sent out as a mass mailing. Malicious spam is spam with malicious content in forms of harmful attachments or links to phishing websites. In the case of educational institutes, malicious spam threatens the privacy and security of large amount of sensitive data relating to staff and students. Hence, a system that can automatically learn how to classify malicious spam in educational institutes is highly desirable. In this paper, we aim to improve detection of malicious spam through feature selection, with focus on the educational field. We propose a model that employs a novel dataset for the process of feature selection, a step for improving classification in later stage. This dataset is unprecedented as no research in the literature was intended to serve malicious spam detection in a specific domain or field such as the educational field. Feature selection is expected to improve training time and accuracy of malicious spam detection."
Digital forensic is the process of collecting, preserving and analyzing the evidence obtained from the digital media, which start from the moment of collecting these evidence. On the other hand, if the file system is corrupted or deleted, then file carving techniques should be established to restore the maximum amount of deleted data, especially, the fragments. File carving is a methodology that helps the investigators to retrieve and acquire the data from unallocated space. There are many carving techniques are used to find different types of file such as (PDF, XML, JPEG. etc.). This paper will focus on the JPEG file carving techniques since the JPEG is the most widespread loss compression formats used by digital cameras. The main contribution of this paper is to review the existing techniques for JPEG file carving and evaluate them to identifying their characteristics. Also are classified according to the types of carving techniques used, fragmentation handling issues, and the existence of file system. Additionally, a hybrid method proposed to perform special tasks depending on fragmentation handling issues. To the best of our knowledge, this paper becomes a step forward for researchers interested in carving techniques by helping them finding the best algorithm for carving and obtaining any deleted data in cyberspace.
"Traditional access control systems lack the ability to cope with dynamic environments where several factors can affect the decision-making process. On the contrary, Risk-based access control systems offer a preeminent alternative where multiple risk factors sway the access control decision. Nonetheless, risk is often measured qualitatively, and is subject to uncertainty; thus, making it susceptible to underestimating or overestimating its value. Conversely, Fuzzy Inference System has been proven effective in solving problems where uncertainty has a dominant influence over the outcomes, making it an excellent candidate to solve the aforementioned issue. This study improves on the authors' previous work in which risk adaptive hybrid RFID access control system is proposed. In this study, a multilevel fuzzy inference system is designed as a supplementary risk assessment model where risk is estimated using fuzzy logic controller. The results showed that the proposed design has significantly improved the overall risk calculation process."
"Attacks of Ransomware are increasing; this form of malware bypasses many technical solutions by leveraging social engineering methods. This means established methods of perimeter defence need to be supplemented with additional systems. Honeypots are bogus computer resources deployed by network administrators to act as decoy computers and detect any illicit access. This study investigated whether a honeypot folder could be created and monitored for changes. The investigations determined a suitable method to detect changes to this area. This research investigated methods to implement a honeypot to detect ransomware activity, and selected two options, the File Screening service of the Microsoft File Server Resource Manager feature and EventSentry to manipulate the Windows Security logs. The research developed a staged response to attacks to the system along with thresholds when there were triggered. The research ascertained that witness tripwire files offer limited value as there is no way to influence the malware to access the area containing the monitored files."
With an ever-increasing trend of cybercrimes and incidents due to software vulnerabilities and exposures, effective and proactive vulnerability management becomes imperative in modern organisations regardless large or small. Forecasting models leveraging rich historical vulnerability disclosure data undoubtedly provide important insights to inform the cyber community with the anticipated risks. In this paper, we proposed a novel framework for statistically analysing long-term vulnerability time series between January 1999 and January 2016. By utilising this sound framework, we initiated an important study on not only testing but also modelling persistent volatilities in the data. In sharp contrast to the existing models, we consider capturing both mean and conditional variance latent in the disclosure series. Through extensive empirical studies, a composite model is shown to effectively capture the sporadic nature of vulnerability time series. In addition, this paper paves the way for further study on the stochastic perspective of cyber vulnerability proliferation towards more accurate prediction models and better risk management.
This chapter focuses on the risks in online medicine purchasing and utilises a theoretical framework - respectable deviance to underpin how the risks are challenged. Incorporating three stages involving the construction of deviance, the justification of deviance and the management of respectability, respectable deviance provides insight into how online medicine consumers account for their activity, which sometimes transgresses regulation and disputes authority.
This article introduces and applies an integrative model of cyberharassment victimization. The model combines routine activity theory (RAT), the general theory of crime (GTC), and the personal resources approach to analyze risk factors for victimization while acknowledging the protective role of a sense of mastery. Survey respondents were aged 15 to 25 years (N = 4816) from the U.S., Finland, Spain, and South Korea. Logistic regression models were used to analyze cyberharassment victimization. RAT-related factors were positively associated with cyberharassment victimization. Low self-control was positively associated with cyberharassment victimization in the U.S., Finland, and Spain but not in South Korea. The sense of mastery was negatively associated with cyberharassment victimization in the U.S., Finland, and South Korea but not in Spain. Protective factors against cyberharassment victimization should be utilized in future studies as adequate knowledge of protective factors could assist policymakers in generating preventative measures against cyberharassment. Our study demonstrates the benefits of integrating criminological theories and protective factors in studies using cross-national data to gain a better understanding of the dynamics of cyberharassment.
In the cyber security landscape, the human ability to comprehend and adapt to existing and emerging threats is crucial. Not only technical solutions, but also the operator's ability to grasp the complexities of the threats affect the level of success or failure that is achieved in cyber defence. In this paper we discuss the general concept of situation awareness and associated measurement techniques. Further, we describe the cyber domain and how it differs from other domains, and show how predictive knowledge can help improve cyber defence. We discuss how selected existing models and measurement techniques for situation awareness can be adapted and applied in the cyber domain to measure actual levels of cyber situation awareness. We identify generic relevant criteria and other factors to consider, and propose a methodology to set up cyber situation awareness measurement experiments within the context of simulated cyber defence exercises. Such experiments can be used to test the viability of different cyber solutions. A number of concrete possible experiments are also suggested.
This chapter provides concluding remarks, drawing together the research on the opportunities and risks for online medicine consumers. It highlights the significance of the respectable deviance framework in understanding how consumers utilise the opportunities and respond to the risks involved in online medicine purchasing. It is argued that an analysis of online medicine purchasing must be located in a critical discussion of construction and hegemony.
The major goal of this chapter is to overview and present selected emerging technologies for cybersecurity. In the first part we show the practical realisations of the bio-inspired concepts for cybersecurity. We do not focus on discussing the bio-inspired techniques on a high and abstract level, but we focus on our own practical developments. We want to present concrete solutions with the magazine-like language understandable to all readers. Our goal is to prove that the bio-inspired techniques can be really implemented to protect networks and that the readiness level of such technology is constantly increasing. In this chapter, we present and focus on our own results and give references to our past and on-going cyber security projects where we successfully implemented different nature-inspired solutions.
This chapter considers the new opportunities for purchasing medicine provided by the Web. These opportunities are incorporated within the challenges to the pharmaceutical market place, governance and regulation, and healthcare expertise fused with the discourses on risk. In the first instance an investigation into the routes to online medicine purchasing is undertaken utilising the empirical research data, before the chapter turns to how regulation is navigated by distinguishing between legitimate and illegitimate online medicine purchasing. Qualitative accounts involving challenges to healthcare expertise are also presented.
Decentralized cryptocurrencies still suffer from three interrelated weaknesses: Low transaction rates, high transaction fees, and long confirmation times. Payment Channels promise to be a solution to these issues, and many constructions for cryptocurrencies, such as Bitcoin and Ethereuem, are known. Somewhat surprisingly, no solution is known for Monero, the largest privacy-preserving cryptocurrency, without requiring system-wide changes like a hard-fork of its blockchain like prior solutions. In this work, we close this gap for Monero by presenting the first provably secure payment channel protocol that is fully compatible with Monero's transaction scheme. Notably, the payment channel related transactions are identical to standard transactions in Monero, therefore not hampering the coins' fungibility. With standard techniques, our payment channels can be extended to support atomic swap of tokens in Monero with tokens of several other major currencies like Bitcoin, Ethereum, Ripple, etc., in a fungible and privacy-preserving manner. Our main technical contribution is a new cryptographic tool called verifiable timed linkable ring signatures (VTLRS), where linkable ring signatures can be hidden for a pre-determined amount of time in a verifiable way. We present a practically efficient construction of VTLRS which is fully compatible with the transaction scheme of Monero, and allows for users to make timed payments to the future which might be of independent interest to develop other applications on Monero. Our implementation results show that even with high network latency and with a single CPU core, two regular users can perform up to 93500 payments over 2min (the block production rate of Monero). This is approximately five orders of magnitude improvement over the current payment rate of Monero.
This chapter presents the issue of online medicine purchasing and outlines the rationale and aim of the book. The reader is introduced to a novel theoretical framework - respectable deviance, to understand how online medicine purchasing has been constructed and in responding to this, how consumers justify and manage their behaviours. The chapter also describes the methodology, including ethical considerations of undertaking online research. It concludes by summarising the key areas and definitions involved in online medicine purchasing.
In this paper, we develop a language-agnostic methodology to extract features of interest to an analyst from forum posts and visualize them in a way which facilitates identification and stratification of areas of interest in the forums, as well as further manual analysis of the text. We then apply this methodology to a specific Russian underground forum. The visualization acts as a 'thumbnail' for individual posts, conveying semantic metadata of post contents. By viewing the thumbnail, an analyst is provided with an immediate 'sense' of post length and key features present within a post, as well as their frequency and spatial arrangement. Using the generated visualizations of posts from the underground forum we speed up analyst identification of post subject matter by up to 72%. As a key novelty, we propose that the image output of our method has fractal properties that can be exploited when sorting threats and extracting highly technical posts. Thus, we use a method based on the Minkowski-Bouligand fractal dimension to prioritize analysis of posts which represent more sophisticated threats.
Use of IP addresses by courts in their decisions is one of the issues with growing importance. This applies especially at the time of the increased use of the internet as a mean to violate legal provisions of both civil and criminal law. This paper focuses predominantly on two issues: (1) the use of IP addresses as digital evidence in criminal and civil proceedings and possible mistakes in courts' approach to this specific evidence, and (2) the anonymisation of IP addresses in cases when IP addresses are to be considered as personal data. This paper analyses the relevant judicial decisions of the Slovak Republic spanning the time period from 2008 to 2019, in which the relevant courts used the IP address as evidence. On this basis, the authors formulate their conclusions on the current state and developing trends in the use of digital evidence in judicial proceedings. The authors demonstrate the common errors that occur in the courts' decisions as regards the use of IP addresses as evidence in the cases of the IP addresses anonymisation, usage of the in dubio pro reo principle in criminal proceedings, and the relationship between IP addresses and devices and persons. (C) 2020 The Author(s). Published by Elsevier Ltd.
"Current network security systems are progressively showing their limitations. One credible estimate suggests that only about 45% of new threats are detected. Therefore it is vital to find a new direction that cybersecurity development should follow. We argue that the next generation of cybersecurity systems should seek inspiration in nature. This approach has been used before in the first generation of cybersecurity systems; however, since then cyber threats and environment have evolved significantly, and accordingly the first-generation systems have lost their effectiveness. A next generation of bio-inspired cybersecurity research is emerging, but progress is hindered by the lack of a framework for mapping biological security systems to their cyber analogies. In this paper, using terminology and concepts from biology, we describe a cybersecurity ecology and a framework that may be used to systematically research and develop bio-inspired cybersecurity."
The purpose of this paper is to present the importance of terminology related to child sexual exploitation, and is based on a research project that included five European countries (Slovenia, Norway, Finland, the United Kingdom and Poland) and Europol. A qualitative approach was used to analyse the data, which was collected by means of unstructured interviews, which focused on the term child pornography, which is commonly used all over the globe. Nevertheless, the term child pornography, while appropriate from the linguistic aspect and widely known, accepted, and used in many official documents and by experts in different fields, might be misleading for non-experts who might not be aware of the seriousness of the crimes related thereto. Many different expressions can be found in the area of child sexual exploitation and much consideration must be devoted to this topic to avoid any misunderstandings. The main goal of this paper is to suggest terms that are applicable, relevant, understandable, and inoffensive to the victims as these crimes have a major impact on their lives. The term child pornography is therefore slowly being abandoned in professional circles, as it minimises the seriousness of the criminal offence and contributes to the stigmatisation and re-victimisation of victims. On the basis of the research findings, the authors agree that term child sexual exploitation material [CSEM] should be used instead of child pornography. In addition, there is also another relevant term that can be used as a CSEM subtype, i.e. child sexual abuse material [CSAM].
In multimedia forensics, learning-based methods provide state-of-the-art performance in determining origin and authenticity of images and videos. However, most existing methods are challenged by out-of-distribution-data, i.e., with characteristics that are not covered in the training set. This makes it difficult to know when to trust a model, particularly for practitioners with limited technical background. In this work, we make a first step toward redesigning forensic algorithms with a strong focus on reliability. To this end, we propose to use Bayesian neural networks (BNN), which combine the power of deep neural networks with the rigorous probabilistic formulation of a Bayesian framework. Instead of providing a point estimate like standard neural networks, BNNs provide distributions that express both the estimate and also an uncertainty range. We demonstrate the usefulness of this framework on a classical forensic task: resampling detection. The BNN yields state-of-the-art detection performance, plus excellent capabilities for detecting out-of-distribution samples. This is demonstrated for three pathologic issues in resampling detection, namely unseen resampling factors, unseen JPEG compression, and unseen resampling algorithms. We hope that this proposal spurs further research toward reliability in multimedia forensics.
While website domain typosquatting is highly annoying for legitimate domain operators, research has found that it relatively rarely presents a great risk to individual users. However, any application (e.g., email, ftp, ...) relying on the domain name system for name resolution is equally vulnerable to domain typosquatting, and consequences may be more dire than with website typosquatting. This paper presents the first in-depth measurement study of email typosquatting. Working in concert with our IRB, we registered 76 typosquatting domain names to study a wide variety of user mistakes, while minimizing the amount of personal information exposed to us. In the span of over seven months, we received millions of emails at our registered domains. While most of these emails are spam, we infer, from our measurements, that every year, three of our domains should receive approximately 3,585 legitimate emails meant for somebody else. Worse, we find, by examining a small sample of all emails, that these emails may contain sensitive information (e.g., visa documents or medical records). We then project from our measurements that 1,211 typosquatting domains registered by unknown entities receive in the vicinity of 800,000 emails a year. Furthermore, we find that millions of registered typosquatting domains have MX records pointing to only a handful of mail servers. However, a second experiment in which we send honey emails to typosquatting domains only shows very limited evidence of attempts at credential theft (despite some emails being read), meaning that the threat, for now, appears to remain theoretical.
Breast cancer (BC) is the second most prevalent type of cancer among women leading to death, and its rate of mortality is very high. Its effects will be reduced if diagnosed early. BC's early detection will greatly boost the prognosis and likelihood of recovery, as it may encourage prompt surgical care for patients. It is therefore vital to have a system enabling the healthcare industry to detect breast cancer quickly and accurately. Machine learning (ML) is widely used in breast cancer (BC) pattern classification due to its advantages in modelling a critical feature detection from complex BC datasets. In this paper, we propose a system for automatic detection of BC diagnosis and prognosis using ensemble of classifiers. First, we review various machine learning (ML) algorithms and ensemble of different ML algorithms. We present an overview of ML algorithms including ANN, and ensemble of different classifiers for automatic BC diagnosis and prognosis detection. We also present and compare various ensemble models and other variants of tested ML based models with and without up-sampling technique on two benchmark datasets. We also studied the effects of using balanced class weight on prognosis dataset and compared its performance with others. The results showed that the ensemble method outperformed other state-of-the-art methods and achieved 98.83% accuracy. Because of high performance, the proposed system is of great importance to the medical industry and relevant research community. The comparison shows that the proposed method outperformed other state-of-the-art methods.
Digital sovereignty has become a buzzword in digital policies. Contrary to the imaginary of digital transformation as preceding an era of limitless global networking in the 1990s, approaches to state regulation and delimitation of data flows as well as programmes for national digital infrastructures are justified with calls for digital sovereignty across very different contexts. This forum brings together contributions from political geography, law, computer science, and ethics that compare and analyse discourses and practices of digital sovereignty. The case studies on Russia and the EU reveal parallels as well as fundamental differences in the conception and implementation of digital sovereignty. Essays on the challenges posed by new forms of cross-border interaction (such as cloud computing) and new actors (such as digital platforms) illustrate that the traditional coupling of concepts of sovereignty, territoriality and the state, of jurisdiction and borders, must be rethought. The essays in this forum thus make it clear that the digital transformation is not simply a socio-technical modernisation process. It is rather shaped in specific ways and should be understood and analysed as (geo)-political discourses and practices. The forum contributes to the development of a political digital geography that analyses how the digital transformation is contested and produced in specific ways and unearths the politics and spatialities conceived and produced in these discourses and practices.
