Purpose This paper aims to study the developed countries' experience on the cyberbullying legal regulation among adolescents, to identify existing shortcomings in the developing countries' laws and to develop recommendations for regulatory framework improvement. Design/methodology/approach The authors have studied the state regulatory practice of the UK, the USA, Canada, Malaysia, South Africa, Turkey, UAE and analyzed the statistics of 2018 on the cyberbullying manifestation among adolescents in these countries. Findings The study results can encourage countries to create separate cyberbullying legislation and periodically review and modify already existing legislation. Originality/value The study provides a list of the recommendations to regulate cybercrime in developing countries and prevent it as well. The results may contribute to creating laws related to the regulation of cyberbullying in countries where such legislation does not exist yet or existing regulatory legal acts do not bring the expected results, namely, in Post-Soviet countries and other developing countries of the world.
Detecting malicious software or malware is one of the major concerns in information security governance as malware authors pose a major challenge to digital forensics by using a variety of highly sophisticated stealth techniques to hide malicious code in computing systems, including smartphones. The current detection techniques are futile, as forensic analysis of infected devices is unable to identify all the hidden malware, thereby resulting in zero day attacks. This chapter takes a key step forward to address this issue and lays foundation for deeper investigations in digital forensics. The goal of this chapter is, firstly, to unearth the recent obfuscation strategies employed to hide malware. Secondly, this chapter proposes innovative techniques that are implemented as a fully-automated tool, and experimentally tested to exhaustively detect hidden malware that leverage on system vulnerabilities. Based on these research investigations, the chapter also arrives at an information security governance plan that would aid in addressing the current and future cybercrime situations.
There is comparatively little information about the roles and the separation of these roles within financially-motivated cybercrime online. As Darknet Markets (DNMs) are online fora, roles can often be conflated with membership or user types within such fora, e.g., administrator, new user, etc. The insights presented in this paper are grounded in a Conversation Analysis of underground forum threads in combination with Social Network Analysis of the relationships between actors in these fora and an automated analysis of the thematic scope of their communications using NLP techniques. This results in a more nuanced understanding of roles, and the power relationships between roles, as they emerge through and are defined by linguistic interactions. Based on this mixed methods approach, we developed a dynamic typology of three key roles within DNMs that goes beyond a basic supply-demand logic: entrepreneurs, influencers and gatekeepers. A closer analysis of these roles can contribute to a better understanding of emerging trends in a forum and allow for the identification and prioritisation of high-risk targets.
New and nontraditional approaches are required to effectively tackle the global problem of cybercrime. Online warning messages offer the unique potential to influence information behavior at the exact point of user decision-making. This research assessed the prevention effect of differing components of warning messages. Thirty-five male participants, aged 18-43, participated in a behavioral-compliance task comprising messages received when visiting websites likely to contain malware. Participants also rated messages on believability, severity, and effects on intention to comply. The components of messages tested were as follows: three signal words (warning, hazard, and stop), two levels of message explicitness (high, low), and two imagery conditions (eyes, no eyes). Contrary to expectations, explicitness was the only message component to yield a significant preventative effect on self-rated and behavioral responses. Participants not only perceived the explicit messages as more believable, severe, and likely to increase intention to comply but also demonstrated, through their behavioral-compliance data, a preventative effect from more explicit messages. The implications of these findings for designing messages to prevent cybercrimes are explored.
New innovations in computer systems were created aiming to provide convenience for its users, but unfortunately they are often followed by threats that surround them. The Company's computer security system can not control the security of every company data. Because information technology attacks have impact on consumer information management and corporate marketing activities, it is necessary to have an institution or business supervisory body for mining company to deal with corporate cybercrime. Data collection uses some methods including observation and document analysis. The data sources used are primary, secondary and tertiary. There are four stages of data analysis, namely data collection, processing, verification, and conclusions. The results indicated that data security of mining companies is critically required to protect company assets such as natural and human resources from the threats of internet criminals. The role of Indonesian National Cyber Bureau is to prevent the occurrence of crimes on the internet through the cycle of supervision formining business, and mining companies should have a robust computer software system.
Today we live in a time where everything is digitalized and technology moves rapidly forward. This big technology progress is closely linked to increase of cybercrime. In an effort to fight e-crime and to collect relevant digital evidence, law enforcement agencies are incorporating the collection and analysis of digital evidence, also known as computer forensics, into their infrastructure. They are challenged by the need to train officers and specialists to collect digital evidence and keep up with rapidly evolving technologies such as computer operating systems and mobile devices. With a lack of resources to send new people to the trainings, very often recently trained officers and experts become trainers to their colleagues. This role demands specific competencies and skills that can be learned on specialized courses. Based on an empirical analysis done on specimen of 20 experienced digital forensic specialists, this paper shows results of this research, offers explanation of competencies and skills needed for digital forensic trainer and helps law enforcement agencies to recognize potential trainers in their ranks, and helps them in their fight with the e-crime.
With the global diffusion of cybercrime, the ever-growing market penetration of high-performance and low-cost personal digital devices, and the commercial success of cloud computing, the area of digital forensics is faced with various new challenges that must be taken seriously. In this chapter, the authors describe a novel approach to digital investigations based on the emerging Forensics as a Service (FaaS) model. This model attempts to optimize Law Enforcement Agency's (LEA) forensic procedures, reduce complexity, and save operational costs. Inspired by previous work on distributed computing for forensic analysis, this chapter provides the reader with design guidelines of a FaaS platform for secure service delivery. The proposed FaaS platform should be able to support investigators and practitioners in their daily tasks (e.g. digital evidence examination, analysis, and reporting) once implemented by a cloud forensic provider or internally by a LEA. In this chapter, the authors also present the architecture components, interfaces, communication protocols, functional and non-functional requirements, as well as security specifications of the proposed framework in detail.
According to a study by Cybersecurity Ventures, cybercrime is expected to cost $6 trillion annually by 2021. Most cybersecurity threats access internal networks through infected endpoints. Recently, various endpoint environments such as smartphones, tablets, and Internet of things (IoT) devices have been configured, and security issues caused by malware targeting them are intensifying. Event logs-based detection technology for endpoint security is detected using rules or patterns. Therefore, known attacks can respond, but unknown attacks can be difficult to respond to immediately. To solve this problem, in this paper, local outlier factor (LOF) and Autoencoder detect suspicious behavior that deviates from normal behavior. It also detects threats and shows the corresponding threats when suspicious events corresponding to the rules created through the attack profile are constantly occurring. Experimental results detected eight new suspicious processes that were not previously detected, and four malicious processes and one suspicious process were judged using Hybrid Analysis and VirusTotal. Based on the experiment results, it is expected that the use of operational policies such as allowlists in the proposed model will significantly improve performance by minimizing false positives.
With technological advancements and the increasing use of computers and internet in our day to day lives, the issue of security has become paramount. The rate of cybercrime has increased tremendously in the internet era. Out of the numerous crimes. identity theft is perhaps the one that poses the most dangers to an individual. More and more voices strongly declare that the password is no longer a reliable IT security measure and must be replaced by more efficient systems for protecting the computer contents. Behavioural biometrics is an emerging technology that resoles some of the major flaws of the previous scheme. We first present an overview of behavioural biometrics, followed by a comprehensive literature survey of techniques. Then, we propose a novel system for authentication which uses the concept of a logical DNA integrating multiple factors to build a user profile. This model is known as 'BehavioAttlh'. The new model demonstrates an accuracy of 96% with FAR and FRR being negligible (3% and 3.3% respectively). Finally, we compare the new model with the currently existing mechanisms.
Robocalling, voice phishing, and caller ID spoofing are common cybercrime techniques used to launch scam campaigns through the telephony channel, which unsuspecting users have long trusted. More reliable than online complaints, a telephony honeypot provides complete, accurate and timely information about unwanted phone calls across the United States. Our first goal is to provide a large-scale data-driven analysis of the telephony spam and fraud ecosystem. Our second goal is to uniquely identify bad actors potentially operating several phone numbers. We collected about 40,000 unsolicited calls. Our results show that only a few bad actors, robocallers or telemarketers, are responsible for the majority of the spam and scam calls, and that they can be uniquely identified based on audio features from their calls. This discovery has major implications for law enforcement and businesses that are presently engaged in combatting the rise of telephony fraud. In particular, since our system allows end-users to detect fraudulent behavior and tie it back to existing fraud and spam campaigns, it can be used as the first step towards designing and deploying intelligent defense strategies.
Digital evidences hold great significance for governing cybercrime. Unfortunately, previous acquisition tools were troubled by either the shortage of suspending the target system's running or the security of the acquisition tools themselves, thus the correctness and accuracy of their obtained evidences cannot be guaranteed. In this paper, we propose VAIL, a novel virtualization based monitoring system for mini-intrusive live forensics, which employs hardware assisted virtualization technique to gather integrated information from the native computer system. Meanwhile, the execution of the target system will not be interrupted and VAIL keeps immune to attacks from the target system. We have implemented a proof-of-concept prototype that has been validated with a Windows guest system. The experimental results show that VAIL can obtain comprehensive digital evidences from the target system as designed, including the CPU state, the physical memory content, and the I/O activities. And on average, VAIL only introduces 4.21 % performance overhead to the target system, which proves that VAIL is practical in real commercial environments.
The development of the Internet and computer-mediated communications (CMC), such as email and instant messaging, has transformed the lives of young people. The ability to communicate in near real time with others provides beneficial social impact, though it has also created unique opportunities for victimization. Research has focused on certain forms of sexual activity online, such as sexting, but little study has been devoted to coercion to engage in sexual conversations as a form of sexual harassment. This study examined this phenomenon in a sample of middle and high school youth in Kentucky, applying an integrated low self-control and routine activity framework. The findings demonstrated that youth who were victimized were more likely to have low levels of self-control, view pornography, have peers who engaged others in sexual conversation online, and be female. The relationship between gender and victimization was examined in depth, along with the implications of this study for our understanding of the utility of routine activity theory and the general theory of crime.
Information technology is becoming increasingly important for entrepreneurs. Protecting their technical infrastructure and stored data is, therefore, also becoming more vital. Nevertheless, research into the safety and security of entrepreneurs in general, and online threats targeted at entrepreneurs in particular, is still limited. This study investigates to what extent self-employed entrepreneurs protect themselves against online threats, and what motivates them to do so. Based on secondary analyses on data collected from 1622 Dutch entrepreneurs, we observe that the majority implement technical and personal coping measures. By adopting protection motivation theory as a theoretical basis for our study, we found that coping appraisal explains the adoption of protective measures. Entrepreneurs are likely to implement protective measures when they believe a measure is effective, when they are capable of using internet technology, when their attitude towards online protection is positive and when they believe they are responsible for their own online security. Although the secondary analysis provides some limitations, this study offers new insights into the usage of protective measures and the motivation for taking them. These insights can help to improve information security campaigns.
IntroductionFew studies have examined the sales of stolen account credentials on darkweb markets. In this study, we tested how advertisement characteristics affect the popularity of illicit online advertisements offering account credentials. Unlike previous criminological research, we take a novel approach by assessing the applicability of knowledge on regular consumer behaviours instead of theories explaining offender behaviour.MethodsWe scraped 1,565 unique advertisements offering credentials on a darkweb market. We used this panel data set to predict the simultaneous effects of the asking price, endorsement cues and title elements on advertisement popularity by estimating several hybrid panel data models.ResultsMost of our findings disconfirm our hypotheses. Asking price did not affect advertisement popularity. Endorsement cues, including vendor reputation and cumulative sales and views, had mixed and negative relationships, respectively, with advertisement popularity.DiscussionOur results might suggest that account credentials are not simply regular products, but high-risk commodities that, paradoxically, become less attractive as they gain popularity. This study highlights the necessity of a deeper understanding of illicit online market dynamics to improve theories on illicit consumer behaviours and assist cybersecurity experts in disrupting criminal business models more effectively. We propose several avenues for future experimental research to gain further insights into these illicit processes.
Although geographic information systems (GIS) are popular tools for investigations and resource deployment/management within law enforcement agencies, the utility of software programs like Esri's ArcGIS mapping software to identify vulnerable areas or populations to prevent or mitigate the impact of crime is rarely explored within the field. What is even less explored is how online information can be leveraged and mapped to inform practitioners of areas where harm reduction strategies could be deployed. Recognizing this gap in the literature, this investigation explored whether publicly available online chatter by established and potential customers of sex workers could point to areas of targeted outreach before harm occurs. Ultimately, we found that online chatter about accessing sex workers can be mapped to identify vulnerable areas and people. While this study was exploratory and others should build on this idea, this study demonstrated GIS could aid in the prevention of crime by leveraging online information to highlight areas and people at risk for harm.
Statistical methods are essential for qualitative, quantitative, and mixed-method research methodology. Mostly, statistical methods involve mathematical formulas, models, and techniques used in analyzing data. Each method and statistical analysis used brings different results to a study. Therefore, this study aims to explore the use of statistical methods in the ICT ethics area. This study reviews the research done on ICT ethics for the years 2015 until 2020. The research questions addressed in this study are: (i) What statistical methods were used in the ICT ethics research? and (ii) What application area in ICT ethics employ the statistical methods? The findings indicated that the inferential statistical method was most employed, followed by the descriptive statistical method and other methods. Also, non-parametric was the most technique used compared to parametric. Meanwhile, the application area in ICT ethics that employ statistical methods is privacy, piracy, access right, cybercrime, harmful action, social impact, and other related ethics issues such as hacking, internet addiction, free speech, and cyberloafing. The implication of the study is described.
Phishing is a kind of worldwide spread cybercrime that uses disguised websites to trick users into downloading malware or providing personally sensitive information to attackers. With the rapid development of artificial intelligence, more and more researchers in the cybersecurity field utilize machine learning and deep learning algorithms to classify phishing websites. In order to compare the performances of various machine learning and deep learning methods, several experiments are conducted in this study. According to the experimental results, ensemble machine learning algorithms stand out among other candidates in both detection accuracy and computational consumption. Furthermore, the ensemble architectures still provide impressive capability when the amount of features decreases sharply in the dataset. Subsequently, the paper discusses the factors why ensemble machine learning methods are more suitable for the binary phishing classification challenge in up-date training and real-time detecting environment, which reflects the sufficiency of ensemble machine learning methods in anti-phishing techniques.
This paper presents a systematic approach to designing a series of digital forensics instructional materials to address the severe shortage of active learning materials in the digital forensics community. The materials include real-world scenariobased case studies, a set of hands-on problem-driven labs for each case study, and an integrated forensic investigation environment. In this paper, we first clarify some fundamental concepts related to digital forensics, such as digital forensic artifacts, artifact generators, and evidence. We then re-categorize knowledge units of digital forensics based on the artifact generators for measuring the coverage of learning outcomes and topics. Finally, we utilize a real-world cybercrime scenario to demonstrate how knowledge units, digital forensics topics, concepts, artifacts, and investigation tools can be infused into each lab through active learning. The repository of the instructional materials is publicly available on GitHub. It has gained nearly 600 stars and 22k views within several months.
Prior studies have x-rayed the potentials and impetus for development that can be resultant from a full scale adoption of ICT in Africa particularly in Nigeria. Current challenges relating to infrastructures, cybercrime, government policies and so on that mitigate the benefits accruable from a virile ICT growth have also been highlighted. However, research also needs to really address possible areas of developmental benefit of ICT to rural target groups such as farmers, extension officers, health and social workers This paper explores possible alignment of ICT4D2.0 goals to ICT development in Nigeria. It posited that the digital divide that ICTD1.0 sets to bridge are still very evident at grassroots level. Adding computing and Internet functionality to technologies that already penetrate the grassroots such as mobiles phones, radios and televisions will enhance ICT4D2.0 goals in Nigeria. This will in turn increase productivity among this target group with attendant contribution to national development.
As most software used by government agencies and companies is proprietary, malicious computer activity targeting breaches in that software can be likened to a pandemic of an infectious disease in the cyber world. When a breach occurs, the consequences can be widespread and damaging because the damage can spread rapidly. Therefore, cybercrime prevention needs to involve all users in a cooperative effort, with warnings and information on countermeasures distributed to users in order to prevent the disease from spreading when unprotected computers encounter an attack. This cooperative effort relies heavily on all institutions reporting information security incidents. Based on institutional theory, together with regulatory pluralism and responsive regulation theory, this paper examines the pluralized regulatory approach adopted to promote a system for sharing reports of information security incidents in Taiwan and China. An expanded model of regulatory enforcement and a strengths-based pyramid are proposed and used as a framework for discussing existing systems for encouraging the reporting of information security incidents.
"The Article looks into recovering assets for victims of frauds using the financial system to launder proceeds, against the backdrop of two case-examples and the lessons learned by the authors from their practice as lawyers in Portugal. It sets out relevant EU instruments (Section I) and then turns into the main difficulties and obstacles in practice (Section II). Finally, it addresses the use of preventive suspension of banking operations (SOB) under AML laws at the service of the interests of the victims of cross-border fraud. It argues albeit these have not been built or designed bearing victims' interests in mind they are essential to the recovery of the proceeds and their return to fraud victims. The authors propose to reflect on improving such mechanisms, namely by guaranteeing that it is possible to make consecutive blockings that follow the trail of the proceeds of the fraud through the various jurisdictions involved; and also on the pertinence of giving an explicit place to the victim in the legal framework that regulates the mechanisms for the prevention of money laundering, namely by facilitating the access of the victims to reporting channels that allow for a quick activation of such mechanisms (Section III). The piece concludes stating that although multiple mechanisms are available to ensure the protection of victims' financial interests in this context, the articulation and definition of the strategy is highly complex, making the intervention of a lawyer essential. In any event, if there is no swift intervention and freezing of the proceeds at the outset of the detection of fraud, it is as a rule impossible to recover victims' assets. It is ironic that in practice such freezing is at best made possible by AML laws that were not designed having victims' interests at the forefront."
"The paper emphasizes that the digitalization of the modern world, the development of information technology, the spread of the Internet, computer networks, and the use of cyberspace have facilitated the daily life of society. However, these factors have entailed a threat to the security and confidentiality of information, personal data, and the financial system. The authors state that financial fraud is becoming an increasingly severe global problem since criminals use the financial ecosystem for money laundering and illegal financial transactions. The study's primary purpose is to identify the latest trends in the financial crime of the world. The methodological tools of the research consisted of theoretical research methods (grouping, abstraction), empirical research methods (observation, description), and the resource base of the information platform, bibliometric analysis, and modeling. The following scientific categories were chosen as the objects of study: regulatory and legal aspects of financial crimes, online crimes and cybercrimes, methods and systems of regulation, control, prevention, counteraction, combating financial crime, and modeling of financial crime processes. The paper analyzes the literature sources regarding the shift of the interest of modern financial market scientists to the study of the features of financial crime. The relevance of identifying the latest trends in financial crime lies in the fact that the study of financial crime trends will allow the improving of the awareness of financial fraud, creating common databases, building coalitions, and identifying effective and efficient ways to improve the ability to combat financial crime at a more effective national and global level. At the initial stage of the work, a bibliometric analysis of scientific publications dedicated to the study of the latest trends in financial crime was carried out. As a result, literary works for the study of the specified issue were systematized; a map of relationships between key terms and other scientific concepts was created; a content-contextual and intercluster analysis of the obtained blocks of bibliometric analysis was carried out; a map of the relationships of the studied key concepts with other scientific categories in dynamics was constructed and the contextual-time block was analyzed. The research in this paper is carried out in three parts, which provide for the definition of several research vectors. As a result of research - the authors discuss potential sources and tools of financial fraud with their negative, harmful aspects for identification, in-depth consideration, and study; they identify tools for countering financial crimes; practical models for evaluating, analyzing, identifying, comparing, and visualizing the features of financial crime are described. The conclusion of the study assumes that the results of the study can be practically applied by financial organizations, institutions, and business entities for the future safe functioning of the financial sector but taking into account the need for constant development of IT support for financial transactions as a response to the rapidly changing needs of our time."
6G networks can be truly intelligent wireless systems that have the flexibility for continuous technological improvements. However, security events affecting information systems have grown increasingly complicated and sophisticated. Intruders may be able to operate due to a lack of evidence authenticity. Using network forensics methods can solve many cybercrime instances. These methods can be used to locate the intruder and to determine the intrusion's origin and how it can be avoided in the future. Existing network forensics techniques confront security threats and vulnerabilities in several networks, such as personal area networks, local area networks, wide area networks, and wireless wide area networks under telecom networks, in practice, but cannot operate in 6G and beyond telecom networks, which are critical at this moment. Thus, a comprehensive literature analysis of telecom networks is conducted to identify existing concerns and challenges in conducting digital communication forensics. Hence, a digital forensics system for 6G and beyond networks is proposed as an initial idea on the basis of the highlighted issues, thereby assessing security incidents and providing justified evidence that explains the footprints of the attackers.
A plethora of evidence demonstrates the effects of the digitisation of society on everyday life. Developments in web science (the world wide web), online technologies (internet of things, social media, e-governance), artificial intelligence and robotics present major challenges for contemporary societies. These technological advances create risks (loss of autonomy, cybercrime, online abuse, threats to children's safety and national security) and opportunities (climate change mitigation, responses to global health scourges, medical therapies, intergenerational connectivity, smart cities). This article focusses on the contribution of the social sciences to the digital revolution, whether it be in the public or private sectors, civil society or households. The authors explore how technological innovations can result from international cooperation between researchers in different disciplines. They consider how evidence from the social sciences is used to measure the societal impacts of technological change in different cultural, economic and political contexts. They review the ethical issues raised by the datafication of society and autonomous learning machines, while assessing the contribution of social sciences to the policymaking process in the digital age.
There is an increasing demand for expert analysis of cybercriminal communities. Cybercrime is continually becoming more complex due to the rapid development of digital technologies, on the one hand, in new types of criminal activity, such as hacking, distributing malware and DDoS attacks, and on the other hand, in digitised forms of more traditional crimes, such as email scams, phishing, identity theft, and cryptographically secured black markets. Tackling this broad array of behaviour requires tool support for multi-disciplinary investigations, and a connecting framework that can adjust flexibly to changes in the populations being studied. In this work, we present AMoC, a multi-faceted machine learning toolkit that combines structured queries, anomaly detection, social network analysis, topic modelling and accounts recognition to enable comprehensive analysis of cybercriminal communities and users. The toolkit enables the extraction of findings regarding the motivations, behaviour and characteristics of offenders, and how cybercriminal communities react to interventions such as arrests and take-downs. In our demonstration, the toolkit is deployed to analyse over 150,000 accounts from 35 underground marketplaces.
Cybergrooming is a serious cybercrime that primarily targets youths through online platforms. Although reactive predator detection methods have been studied, proactive victim protection and crime prevention can also be achieved through vulnerability analysis of potential youth victims. Despite its significance, vulnerability analysis has not been thoroughly studied in the data science literature, while several social science studies used survey-based methods. To address this gap, we investigate humans' social-psychological traits and quantify key vulnerability factors to cybergrooming by analyzing text features in the Linguistic Inquiry and Word Count (LIWC). Through pairwise correlation studies, we demonstrate the degrees of key vulnerability dimensions to cybergrooming from youths' conversational features. Our findings reveal that victims have negative correlations with family and community traits, contrasting with previous social survey studies that indicated family relationships or social support as key vulnerability factors. We discuss the current limitations of text mining analysis and suggest cross-validation methods to increase the validity of research findings. Overall, this study provides valuable insights into understanding the vulnerability factors to cybergrooming and highlights the importance of adopting multidisciplinary approaches.
The world is facing a new era of criminal activities in cyber space, which are being committed across the world, irrespective of geographical boundaries. These cybercrime acts may be financially driven acts, related to computer content, or against the confidentiality, integrity, and accessibility of computer systems. The relative risk and threat differs between governments and businesses. The level of criminal organization represents a defining feature of the human association element behind criminal conduct. India accounts for close to $8bn of the total $110bn cost of global cyber crime. The Information Technology (IT) Act, 2000, specifies the acts that are punishable. Cyber crime has also affected the social media. A crime prevention plan with clear priorities and targets needs to be established, and government should include permanent guidelines in its programmes and structure for controlling crime and ensuring that clear responsibilities and goals exist within government for the organization of crime prevention. This chapter seeks to find out the motives and suspects of cyber crime perpetration and suggests measures for crime prevention.
Remote work due to the COVID-19 pandemic is expected to be the new normal, suggesting a situation where people use their personal computers at home for several activities like reading emails, surfing the web, chatting with friends. While doing this, users are not focused on securing their systems and they often do not have the skills and knowledge to defend against cybercrime. In this paper, we present the design and the evaluation of a novel interface that warns users against phishing attacks. This interface looks like the ones shown by browsers like Chrome and Firefox when opening a suspicious phishing website, but it includes information that explains the reasons why the website might be a scam. Such explanations are based on website features commonly used by AI-based systems to classify a website as phishing or not and aim to help users detecting phishing websites. To ensure a high understandability and effectiveness of the explanations, the C-HIP model was adopted to design such messages, which have been iteratively refined performing a static analysis of their comprehension, sentiment, and readability.
"In the current technological climate, there is an increase in the number of instances of cybercrime. In the real world, it is never simple to track down and apprehend a cybercriminal. Even if the perpetrator is captured, it will be challenging to secure his conviction since there are not enough standardized digital investigation models. As a consequence of a badly handled forensic investigation, a person who committed a cyber crime may go free, while an innocent person may be subject to adverse ramifications as a result of the inquiry. Forensic investigators are required to adhere to consistent and well-defined forensic methods in order to successfully capture and prosecute cyber criminals. In recent years, a number of new investigating models for digital forensics have been developed; nonetheless, the percentage of cases that result in a conviction remains low since the same investigation model is not relevant in all nations. This chapter introduces a one-of-a-kind Digital Forensic Investigation Model that is built on the architecture and cyber regulations of the Indian subcontinent."
Internet Forensics has become an indispensable part of Cyber Forensics. This is due to the rapid growth in the number of cybercrimes which are related to Internet usage. These crimes vary from malw are crimes to crimes related to use of Social Media, banking transactions and other financial services. In these type of crimes, the browser files which are generated by different web browsers, should be analyzed. Among the different artifacts left by web browsers, the most relevant file in forensic investigation is the cache file as it stores important cyber forensics information of frequently visited websites. Investigators can obtain a clear picture of visited websites, loaded pictures and other objects using the information stored in the cache files. The paper describes the structure of cache files created by Google Chrome in detail. The results obtained in this way can provide forensically sound information in cybercrime investigation. Advanced analysis of java scripts and other objects obtained in this way provide crucial evidence in proving different types of cybercrimes including malw are crimes.
"This paper examines the current state of cryptocurrency forensics focusing on digital forensics investigation supporting law enforcement activities. We begin with a brief overview of cryptocurrency and blockchain technologies, and with that understanding, we explore criminal and malicious activities enabled by these technologies which drive the need for cryptocurrency forensics. Followed by this, the paper will explore current cryptocurrency forensics techniques and challenges pertaining to mobile forensics, computer & memory forensics, and blockchain & network forensics. We will analyze the current body of research and identify gaps in the area. Also, current researchers and practicing investigators indicate that cryptocurrency forensics seems to be lagging and needs improved inter-agency coordination along with forensic techniques to counter the existing and emerging threats.& COPY; 2023 Elsevier Ltd. All rights reserved."
"Purpose Whilst anyone can be scammed, individuals with acquired brain injury (ABI) may have unique risk factors to cyberscams for which tailored interventions are required. To address this, a co-design approach was utilised to develop cybersafety resources with people with living experience of ABI and scams. This study aimed to evaluate the co-design experience to inform future utilisation of co-design methods. Method Semi-structured qualitative interviews explored perceived benefits and challenges, level of support and the co-design process for people with ABI (n= 7) and an attendant care worker (ACW) (n= 1). Transcripts were analysed using a six-stage reflexive thematic analysis. Results Five themes were identified: An Intervention Addressing Shame; Feeling Validated and Valued; Experiencing a 'Profound Change Amongst a Group of Peers'; 'Gaining Stronger Scam Awareness'; and 'Taking Ownership'. Adjustments to support communication, memory impairments and fatigue in the co-design process were recommended. Conclusions Participant reflections on the co-design process extended beyond resource design and highlighted therapeutic benefits of increased insight and emotional recovery from shame. Likely mechanisms underpinning these benefits were the peer group format and opportunities to make meaningful contributions. Despite identified challenges in facilitating co-design projects, the practical and emotional benefits reported by participants underscore the value of co-design with people with ABI."
In the cyberspace environment, access control is one of the foremost fundamental safeguards used to prevent unauthorized access and to minimize the impact from security breaches. Fog computing preserves many benefits for the integration of both internet of things (IoT) and cloud computing platforms. Security in Fog computing environment remains a significant concern among practitioners from academia and industry. The current existing access control models, like the traditional Context-Aware Access Control (CAAC), are limited to access data from centralized sources, and not robust due to lack of semantics and cloud-based service. This major concern has not been addressed in the literature, also literature still lacks a practical solution to control fog data view from multiple sources. This paper critically reviews and investigates the limitations of current fog-based access control. It considers the trade-off between latency and processing overheads which has not been thoroughly studied before. In this paper, a new generation of Fog-Based Context-Aware Access Control (FB-CAAC) framework is proposed to enable flexible access control data from multiple sources. To fill the gap in the literature this paper introduces (i) a general data model and its associated mapping model to collate data from multiple sources. (ii) a data view model to provide an integrated result to the users, dealing with the privacy requirements of the associated stakeholders, (iii) a unified set of CAAC policies with an access controller to reduce both administrative and processing overheads, and (iv) a data ontology to represent the common classes in the relevant data sets. The applicability of FB-CAAC proposal is demonstrated via a walkthrough of the entire mechanism along with several case studies and a prototype testing. The results show the efficiency, flexibility, effectiveness, and practicality of FB-CAAC for data access control in fog computing environment. (C) 2020 Elsevier B.V. All rights reserved.
"During the last decades, Dark Web content has risen in necessity in an increasingly connected world, where international anonymous networks provide access to data marketplaces and illicit multimedia material through the TOR or I2P networks. The motivation behind this paper is to gauge the current state and growth of the Dark Web in relation to the role it plays with special focus on Small and Medium-sized Enterprises (SMEs and MEs). More specifically, we devise Machine Learning and specialised Information Retrieval techniques to extract insights and investigate how the Dark Web enables cybercrime, maintains marketplaces with breached enterprise data collections and pawned email accounts. The research questions that we address concern: a) the role that the Dark Web plays for SMEs, MEs, and society in general; b) the criticality of cybercriminal activities and operations in the Dark Web exploiting threat taxonomies and scoring schemes; and c) the maturity and efficiency of technical tools and methods to curb illegal activities on the Dark Web through raising awareness via efficient text analytics, visual reporting and alerting mechanisms."
IT security becomes increasingly important due to the rise of cybercrime incidents but also obligatory security and privacy laws that include confidentiality regulations. To prevent cybercriminal attacks, the business level has to identify critical business data and introduce organization-wide security standards. A close cooperation with the IT level is crucial to avoid mistakes and misunderstandings of security requirements, both may cause severe security breaches. An important building block are access control requirements (ACRs). In a costly, complex and manual role engineering process, experts have to elicit appropriate role-based access control (RBAC) policies according to business security and confidentiality models. This paper makes a first step to close this gap with an approach that automatically extracts business level ACRs from BPMN business processes to build an initial RBAC role model and establish traceability from RBAC policies to business processes. Case study results indicate that the accuracy of extracted policies is appropriate, adaptations in evolution scenarios become faster and human errors are reduced during the engineering of RBAC policies.
The underground economy is a key component in cyber carding crime ecosystems because it provides a black marketplace for cyber criminals to exchange malicious tools and services that facilitate all stages of cyber carding crime. Consequently, black market sellers are of particular interest to cybersecurity researchers and practitioners. Malware/carding sellers are critical to cyber carding crime since using malwares to skim credit/debit card information and selling stolen information are two major steps of conducting such crime. In the underground economy, the malicious product/service quality is reflected by customers' feedback. In this paper, we present a deep learning-based framework for identifying top malware/carding sellers. The framework uses snowball sampling, thread classification, and deep learning-based sentiment analysis to evaluate sellers' product/service quality based on customer feedback. The framework was evaluated on a Russian carding forum and top malware/carding sellers from it were identified. Our framework contributes to underground economy research as it provides a scalable and generalizable framework for identifying key cybercrime facilitators.
Nowadays, botnets are the most advanced cybercrime as being powerful threaten to the internet infrastructure by risking the Internet stability and security. Millions of computers have been hijacking and infecting by botnets especially during peak activity. The P2P botnets exploit users and dominating the P2P technology which make botnets are harder to detect and terminated. As P2P botnets issues been highlighted as it's dramatically evolvement, this paper addresses on current problems relate to P2P botnets faced by users and recommending the improvement. Also, this paper concentrated on proposing P2P botnets detection framework. Also, an in-depth analysis of P2P botnets has been conducted to understand and cope with their behaviors and characteristics. The new improvement has been introduced at the propose botnets framework architecture to improve the effectiveness of P2P detection analysis. The framework architecture has been structuralized with hybrid analyzer through the marriage of host-based and network based. Prior to this matter, this research has proposed a new enhancement on framework architecture that has been reinforced by hybrid detection technique to improve the effectiveness and efficiency of P2P botnets detection.
"There are several issues associated with Dark Web Structural Patterns mining (including many redundant and irrelevant information), which increases the numerous types of cybercrime like illegal trade, forums, terrorist activity, and illegal online shopping. Understanding online criminal behavior is challenging because the data is available in a vast amount. To require an approach for learning the criminal behavior to check the recent request for improving the labeled data as a user profiling, Dark Web Structural Patterns mining in the case of multidimensional data sets gives uncertain results. Uncertain classification results cause a problem of not being able to predict user behavior. Since data of multidimensional nature has feature mixes, it has an adverse influence on classification. The data associated with Dark Web inundation has restricted us from giving the appropriate solution according to the need. In the research design, a Fusion NN (Neural network)-(SVM)-V-3 for Criminal Network activity prediction model is proposed based on the neural network; NN- (SVM)-V-3 can improve the prediction."
Cybercrime is estimated to have cost the global economy just under USD 1 trillion in 2020, indicating an increase of more than 50% since 2018. With the average cyber insurance claim rising from USD 145,000 in 2019 to USD 359,000 in 2020, there is a growing necessity for better cyber information sources, standardised databases, mandatory reporting and public awareness. This research analyses the extant academic and industry literature on cybersecurity and cyber risk management with a particular focus on data availability. From a preliminary search resulting in 5219 cyber peer-reviewed studies, the application of the systematic methodology resulted in 79 unique datasets. We posit that the lack of available data on cyber risk poses a serious problem for stakeholders seeking to tackle this issue. In particular, we identify a lacuna in open databases that undermine collective endeavours to better manage this set of risks. The resulting data evaluation and categorisation will support cybersecurity researchers and the insurance industry in their efforts to comprehend, metricise and manage cyber risks.
Mobile systems abled a way to shorten the distance between Men. With this panorama also came cybercrime, terrorism and other phenomena of a moving society fully globalized, where the land borders are of little importance in the task of limiting the active agents, harmful or not, to these systems. Recently the world discovered by media and scientific projects that the leading powers watch closely their citizens, aimed directly the mobile systems that they use in conversations, e-mail or Web traffic. Recurring to powerful monitoring and surveillance tools, peaceful or in turmoil nations persecute and deny uncontrolled web access without armful repercussions to their citizens. The present work is the result of the application of the Paranoid Operative Systems methodology in a mobile operative system, something that so far is only been showed in a desktop operative system. To support this study, an analysis of platforms that allow for anonymous web browsing, technologies and programs with potential computer intrusion and violation of privacy was conducted. This study analysed the computer monitoring and surveillance technologies identifying the available countermeasure.
The Internet and connected technology platforms have enabled an increase of cyber influence activity. These actions target a range of personal to national level security and privacy attributes related to cybercrime, behavior, and identities. These emerging threats call for new indicators for improved awareness, decisions, and action. This research proposes a cyber-physical-human spectrum of identification with a prototyped classification method. Classifier goals are to aid in awareness of activity and potential harmful intent such as detection of identity feature acquisition, fraudulent identities and entities, and targeting or influential behavior. Emerging malicious influence actors prey on human social demographic groups and trends using the Internet infrastructure with social network platform access to large target populations as their attack surface. The methodology discusses how this problem could benefit from a combined human-technical approach to understand indicators of influencing human perception that persuade someone perform a desired action. This method is designed to aid in rapid influence awareness and introduce a counter-influence concept. A prototyped experiment trial demonstrates how awareness may be beneficial to balancing national security with personal privacy.
To ensure the security of web applications and to reduce the constant risk of increasing cybercrime, basic security principles like integrity, confidentiality and availability should not be omitted. Even though Transport Layer Security/Secure Socket Layer (TLS/SSL) authentication protocols are developed to shield websites from intruders, these protocols also have their fair share of problems. Incorrect authentication process of websites can give birth to notorious attack like Man in The Middle attack, which is widespread in HTTPS websites. In MITM attack, the violator basically positions himself in a communication channel between user and website either to eavesdrop or impersonate the communicating party to achieve malicious goals. Initially, the MITM attack is defined as a binary machine learning problem. Deep Q learning is utilized to build the MITM attack classification model. Thereafter, training process is applied on 60% of the obtained dataset. Remaining 40% dataset is used for testing purpose. The experimental results indicate that the proposed technique performs significantly better than the existing machine learning technique-based MITM prediction techniques for SSL/TLS-based websites.
A few highly skilled cybercriminals run the Crime as a Service business model. These expert hackers provide entry-level criminals with tools that allow them to enhance their cybercrime operations significantly. Thus, effectively and efficiently disrupting highly proficient cybercriminals is of a high priority to law enforcement. Such individuals can be found in vast underground forums, though it is particularly challenging to identify and profile individual users. We tackle this problem by combining two analysis methods: text analysis with Latent Dirichlet Allocation (LDA) and Social Network Analysis with centrality measures. In this paper, we use LDA to eliminate around 79% of hacker forum users with very low to no technical skills, while also inferring the forum roles held by the remaining users. Furthermore, we use centrality measures to identify users with hugely popular public posts, including users with very few public posts who receive much attention from their peers. We study various preprocessing methods, wherein we achieve our results by following a series of rigorous preprocessing steps. Our proposed method works towards overcoming current challenges in identifying and interrupting highly proficient cybercriminals.
Cybercrime is on the rise, especially for the insurance industry, which collects massive amounts of sensitive data. In response, the National Association of Insurance Commissioners adopted the Model Insurance Data Security Act. This model law provides that state-licensed insurers must conduct a risk assessment as well as implement appropriate security measures, and it lays out when insurers must report data breaches to state insurance commissioners or consumers. As states have implemented their own versions of an Insurance Data Security Act, they have often modified it to make compliance easier for insurers, but in doing so have weakened its safeguards. Iowa's Insurance Data Security Act broadened exemptions for small insurers significantly, creating a gap in privacy protection that leaves many consumers vulnerable to data breaches. This Note argues that Iowa should close this gap by amending the law to narrow the exemptions back to the model law's original scope and help small insurers bear the significant costs of compliance by providing data privacy consultations, education, and/or lobbing the National Association of Insurance Commissioners to provide these necessary resources.
The COVID-19 pandemic outbreak and the subsequent lockdowns and restrictions by many countries worldwide to control the spread of the virus forced several organizations to shift work to homes, aided by digital technologies. However, digital infrastructure and homeworking space, which facilitate home-based e-working, are constrained in the developing world. It is, therefore, crucial to understand how developing country organizations maintain business operations under pandemic situations using home-based e-working and the challenges it poses. This study employs interpretive qualitative research methodology with a socio-technical perspective to explore home-based e-working under COVID-19 conditions using five reputable corporate organizations in Ghana. Findings from the study show that home-based e-working under pandemic conditions can be seen as a business continuity strategy that requires business continuity policy and plan, digital infrastructure and platforms, and cybersecurity. The findings also identified the challenges as partial work virtualization, cybercrime, transition cost, digital divide, and home environment constraints. The findings have implications for research and organizational management in developing countries.
The popularity of Ethereum decentralized applications (Dapps) also brings in new security risks: it has been reported that these Dapps have been under various kinds of attacks from cybercriminals to gain profit. To the best of our knowledge, little has been done so far to understand this new cybercrime, in terms of its scope, criminal footprints and attack operational intents, not to mention any efforts to investigate these attack incidents automatically on a large scale. In this paper, we performed the first measurement study on real-world Dapp attack instances to recover critical threat intelligence (e.g., kill chain and attack patterns). Utilizing such threat intelligence, we proposed the first technique DE-FIER to automatically investigate attack incidents on a large scale. Running DEFIER on 2.3 million transactions from 104 Ethereum on-chain Dapps, we were able to identify 476,342 exploit transactions on 85 target Dapps, which related to 75 0-day victim Dapps and 17K previously-unknown attacker EOAs. To the best of our knowledge, it is the largest Ethereum on-chain Dapp attack incidents dataset ever reported.
There is weak integration of digital forensics and forensic science, despite over a decade of effort to break down the borders between them. As more criminal investigations involve digital traces in increasing amounts and complexity, the quality of digital forensic results is decreasing and comprehension of cybercrime is diminishing. The consequences of errors and omissions in digital forensics include imprisoning innocent people, leaving dangerous criminals free to perpetrate additional crimes, and continuing victimization of the organizations and people targeted by offences. To mitigate these risks it is necessary to harmonize digital forensics and forensic science, and to strengthen knowledge management throughout decentralized forensic ecosystems. To drive the needed harmonization and knowledge management, there are two essential roles: the investigative advisor with operational experience, and the forensic advisor with scientific background. These advisors can negotiate the borders between police, digital forensic specialists, criminal intelligence analysts and attorneys to cultivate a criminal justice system that treats digital traces effectively, has visibility across criminal activities, and addresses crime and security more strategically.
The rapid increase in cybercrime, causing a reported annual economic loss of $600 billion (Lewis 2018), has prompted a critical need for effective cyber defense. Strategic criminals conduct network reconnaissance prior to executing attacks to avoid detection and establish situational awareness via scanning and fingerprinting tools. Cyber deception attempts to foil these reconnaissance efforts by camouflaging network and system attributes to disguise valuable information. Game-theoretic models can identify decisions about strategically deceiving attackers, subject to domain constraints. For effectively deploying an optimal deceptive strategy, modeling the objectives and the abilities of the attackers, is a key challenge. To address this challenge, we present Cyber Camouflage Games (CCG), a general-sum game model that captures attackers which can be diversely equipped and motivated. We show that computing the optimal defender strategy is NP-hard even in the special case of unconstrained CCGs, and present an efficient approximate solution for it. We further provide an MILP formulation accelerated with cut-augmentation for the general constrained problem. Finally, we provide experimental evidence that our solution methods are efficient and effective.
Images of main memory are an increasingly important piece of evidence in cybercrime investigations, especially against advanced malware threats, and software tools that dump memory during normal system operation are the most common way to acquire memory images today. Of all proposed methods, Stuttgen and Cohen's robust memory acquistion (as implemented in the pmem tool) can be considered the most advanced technique today. This paper presents Styx, of a proof-of-concept system that perfectly covers its traces against pmem and other tools that perform software-based forensic memory acquisition. Styx is implemented as a loadable kernel module and is able to subvert running 64-bit Linux systems using Intel's VT-x hardware virtualization extension, without requiring the system to reboot. It further uses the second address translation via Intel's EPT to hide behind hidden memory. While exhibiting the limitations of robust memory acquisition, it also shows the potential of undetectable forensic analysis software. (c) 2018 The Author(s). Published by Elsevier Ltd on behalf of DFRWS.
The rapid development of the cyberspace brought about a relatively new type of crime, namely cybercrime, that transcends both time and space convergence conditions of traditional crime definitions. The darknet and anonymity it assures have been very inviting for cybercriminals in the recent years and it has been plagued with criminal activity, such as distribution of child pornography, illegal arms trafficking or drug trafficking. The purpose of the paper is to show legislative limits of investigating crimes committed using the darknet in Slovenia on examples of a few basic investigative acts: identification of the IP holder, search and seizure of electronic devices, undercover investigative measures and use of malware. Our main finding is that the current legal limitations do not allow a deep darknet investigation of potential offenders. If an IP address of a potential offender is leaked, law enforcement authorities can identify the IP address holder and seize his related electronic devices. The current legal limitations effectively end the investigation at this point as they do not allow the use of any kind of malware by the law enforcement authorities.
Recently published reports on cybercrime indicate an ever-increasing number of security incidents related to IT systems. Many attacks causing the incidents abuse (in) directly one or more security defects. Fixing the security defect once fielded is costly. To avoid the defects and the subsequent need to fix them, security has to be considered thoroughly when developing software. The earliest phase to do so is the requirements engineering, in which security threats should be identified early on and treated by defining sufficient security requirements. In a previous paper [1], we introduced a methodology for Problem-based Security Requirements Elicitation (PresSuRE). PresSuRE provides a computeraided security threat identification. The identification is based on the functional requirements for a system-to-be. Still, there is a need for guidance on how to derive security requirements once the threats are identified. In this work, we provide such guidance extending PresSuRE and its tool support. We illustrate and validate our approach using a smart grid scenario provided by the industrial partners of the EU project NESSoS.
"Social media platforms are crucial public forums connecting users around the world through a decentralised cyberspace. These platforms host high volumes of content and, as such, employ content moderators (CMs) to safeguard users against harmful content like child sexual abuse material (CSAM). These roles are critical in the social media landscape however, CMs' work as digital first responders is complicated by legal and systemic debates over whether the policing of cyberspace should be left to the self-regulation of technology companies, or if greater state-regulation is required. In this empirical policy and literature review, major debates in the area of content moderation and, in particular, the online policing of CSAM are identified and evaluated. This includes the issue of territorial jurisdiction, and how it obstructs traditional policing; concerns over free speech and privacy if CMs are given greater powers, and debates over whether technology companies should be legally liable for user-generated content (UGC). In outlining these issues, a more comprehensive foundation for evaluating current practices for monitoring and combatting online CSAM is established which illustrates both the practical and philosophical challenges of the existing status quo, wherein the state and private companies share these important responsibilities."
The exploitation of Internet technology represents for terrorists and criminals a convenient means for establishing and advertising illegal activities. Especially, social networks facilitate new collaborations as well as the spreading of information with a lower risk of being exposed and fetched. Indeed, due to the increasing number of social media and the huge amount of data continuously generated from them, the discovering process of cyber-criminals is a hard task to be performed by the Law Enforcement Agencies and Police Forces if only based on traditional approaches. It becomes even harder if the heterogeneous nature of data, due to multi-cultural aspects, such as the variety of languages, is considered during the searching process. As a consequence, the adoption of a computer-based approach represents a viable solution. In particular, this paper aims at supporting the automatic identification process of potential online suspicious users, who act on social media. A methodological process, centered on the combination of well-known text analysis techniques by considering multi-language aspects, is proposed. In addition, an evaluation approach, based on the exploitation of different qualitative evaluation criteria, is employed to assess the level of suspiciousness of the identified users. Finally, a software tool that supports the execution of the proposed process is developed and its experimentation is shown through a case study on Twitter.
The global mobile networks are built with a set of core technologies developed during the 1980s, combined with subsequent generations of networking technologies for improved performance. Due to political pressure by European governments during the 1980 the initial GSM network, commonly called 2G, was purposely designed and built with weak security to allow easy interception of phone traffic by law enforcement agencies. Despite strengthened security in the more recent networking technologies of 3G and 4G, the weak security of 2G represents the 'weakest link' which thereby limits the security level of mobile networks in general. While this cybersecurity vulnerability is currently exploited by domestic law enforcement agencies for legal interception and surveillance, as well as by criminal and foreign powers for cybercrime and espionage, it is interesting to notice that it was created on purpose. This paper describes the background and the evolution of mobile network security, analyses the nature and consequences of security vulnerabilities in mobile networks, and proposes political and technical solutions to mitigate the threats posed by these vulnerabilities.
Key Points In South Africa, a similar regulation strategy to the European Union General Data Protection Regulation, called the Protection of Personal Information Act (No 4 of 2013) (POPIA), will be implemented, with a view to mitigate cybercrime and information security vulnerabilities. A qualitative exploratory analysis of information security management at universities in South Africa, using a Technology, Organisation, and Environment model, highlights the need for maintaining the security infrastructure to facilitate management of security within the university network, while placing emphasis on information security management processes, such as risk analysis, architecture review, code inspection, and security testing. Organizational factors were the most critical factors when compared to the technological and environmental factors which appear to influence the effectiveness of information security measures and, subsequently, data regulation readiness. Universities will have to balance the implementation of tangible solutions to mitigate risks within the scope of their budget while promoting user compliance, despite perceived 'restrictions.' For biomedical researchers, questions remain on the impact of POPIA legislation on data sharing, open science, and collaborations.
The security of personal data is crucial for a company or any individual. Phishing is one of the most common and dangerous cybercrime attacks. These attacks aim to steal information used by individuals and organizations using social engineering, which is a key point for the success of the phishing attack. Even though there are several systems and solutions, the amount of personal information stolen continues to increase as cyberattacks become more difficult to detect. This paper consists of a broad review to study the work carried out in the fight against phishing and the identification of vulnerabilities in existing systems to achieve better efficiency. The authors focused on the social medium Twitter to study the phishing attacks passing through this medium, and they present their new design, which is based on new features. The classification of the approach includes 23 features and uses the MLP artificial neural network (ANN MLP) algorithm. Experiments show that the system is effective at detecting phishing sites, with a 96% success rate using recent data.
The growth of the Internet has seen the emergence of elaborate examples of cybercrime in the form of 'scams'. Alongside this, a resistance has also developed, with 'scambaiters' engaging in complex and deceptive scenarios to waste scammers' time and educate others about online scams. This has been facilitated by the evolution of new media platforms inclusive of the live stream video-sharing site Twitch, where scambaiters take part in interactions with scammers in real time in front of large audiences. Such platforms present significant potential for diverse interaction and participation roles that move beyond those of other sites and audiovisual media. Scambaiting texts thus present a valuable opportunity to explore the complex interactive affordances of live stream video-sharing platforms. In this article, we aim to map an interactional framework for live-streamed video-sharing platforms by analysing a scambait call by the scambaiter 'Kitboga' on Twitch. The results show that the platform of Twitch and the novel context of a live-streamed scambait call offer new insights into online interaction within the context of emerging digital genres.
The present study explores the relationship between cybersecurity knowledge, online behavior, and risk perception. To simulate an online shopping experience, we invited participants to access our newly designed website and answer a set of questions to evaluate their level of cybersecurity knowledge. The experiment identifies multiple subsequent stages that enhance different features of the social online shopping literature by offering different tasks. The online activities provided on our website play the role of essential vital enablers to interact with the user such as giving the buyer the chance to earn more credits in exchange for private information, saving the virtual credit card information and selecting a specific shopping scenario. Our outcomes highlight the significance of engaging individuals with cybersecurity and provide an analysis of consumer profiling and practice in an online shopping context. Moreover, our findings lead to two sets of actions: reducing the perceived risk of cybercrime in online activities by increasing the level of knowledge in cybersecurity and evolving user behavior when spotting a high level of privacy concern.
IoT is an emerging technology nowadays, which integrates personal computers (PC), Laptops, smart phones and other electronic gadgets used in our routine life communication. This communication can be for leisure purposes or for any official task. Twitter is the most popular Instant Message (IM) toolkit in today's world. The digital remains left after using twitter can be of great source of evidence when an IoT is involved in a court of law. These evidences can be helpful in finding the culprit. For this purpose, digital forensics of IM can be practiced in order to find the evidences and from those evidences the law and order maintaining officer and the officials responsible for security of information can find some heinous cybercrime syndicates. This research has targeted on the usage of Twitter on the desktop PC under windows 10 operating system for the purpose of presenting supportive digital evidences for information security concerns by collecting and analyzing volatile digital bread crumbs. This analysis is done by using forensic tools such as belkasoft Ramcapturer, Wireshark and ProDiscover basic.
This study discusses whether the concept of societal security is embedded in the Rus- sian formal and informal discourses as well as in the Russian strategic documents on national security and the Baltic Sea region. Particularly, the paper describes four paradigms of international relations (neorealism, neoliberalism, globalism and post -positivism) and theoretical approaches to the concept of societal security formulated in them. On a practical plane, Russia has managed to develop - together with other regional players - a common regional approach to understanding societal securi- ty threats and challenges in the Baltic Sea region. These challenges include uneven regional development, social and gender inequalities, unemployment, poverty, manifestations of intolerance, religious and political extremism, separatism, large-scale migration, climate change, natural and man-made catastrophes, transnational organized crime and cybercrime, international terrorism, so-called hybrid threats, disharmony between education systems, etc. In 2017, Russia and other Baltic countries agreed that the Council of the Baltic Sea States would be the regional institution to implement a common societal security strategy exemplified by the Baltic 2030 Agenda Action Plan.
"Purpose Global evidence suggests that youth offending has reduced; however, this study aims to suggest a more complex picture, with youth crime potentially being displaced to the digital space. Historically, young people and crime have been synonymous with public spaces and being visible. A shift or expansion to online offending requires revision of how the justice and educational systems respond to youth offending. Design/methodology/approach A systematic literature review explored keywords related to age, digital offence or harm and criminal or harmful nature, using a search, appraisal, synthesis and analysis framework. Findings Three emergent areas of digital youth crime are discussed: digitally assisted crime, digitally dependent crime and digital harm. Practical implications The shift in youth offending requires response adjustment from prevention to detection. Opportunities may exist to disrupt or redirect youth before they offend. Further data specific to digital offending is needed. These findings seek to provide a possible direction for future research. Originality/value The concept of digital displacement of youth offending is progressively emerging. This paper examines types of offending categorised into three areas of interest."
Multimedia content's development and technological evolution have enhanced and even facilitated the application of steganography as a means to introduce hidden messages for cybercrime-related purposes. Artificial intelligence models have been widely implemented as a way to detect the presence of these messages in image content. However, the possibility of applying explainability techniques in order to provide visual representations of the signatures of different steganography algorithms has not been studied yet. This work presents a novel steganalysys methodology, STEG-XAI, not only for detecting steganography in images but also for explaining the machine learning model's findings, and extracting the steganography algorithm's signature. A convolutional neural network with EfficientNet architecture is implemented, along with the explainability algorithms LIME and Grad-CAM. The model is trained with a dataset of images modified by UERD, a steganography method designed for JPEG images, and achieves a weighted AUC of 0.944, displaying a high level of discrimination between original and tampered images. Furthermore, the explanation methods enable visualizing both the image modifications identified by the neural network, and a signature of the UERD algorithm.
Phishing is an increasingly serious cybercrime. Phishers create phishing websites by mimicking legitimate websites to confuse users and steal their personal information. The proliferation of phishing websites and more advanced camouflage techniques are problems faced by most existing methods. In this paper, we propose a features fusion networks (MultiPhish) which is the first study on fusing multi-modal features with neural networks for the phishing detection task. In this end-to-end network, the domain and favicon of the website are represented via deep neural networks, and the representation of the website identity is obtained through multi-modal features fusion. In addition, the variation autoencoder (VAE) is introduced to optimize the representation. In the phishing detection module, we incorporate URL features to improve situations where phishing websites cannot be detected only by estimating whether the website identity is disguised. Based on the latest collected dataset, we have carried out extensive experiments and proved that our model is superior to the relevant methods. In addition, MultiPhish is a completely language-independent strategy, so it can perform phishing detection regardless of the text language.
OVER TWO BILLION users consume social media to build and participate in online social networks (OSNs), uploading and sharing hundreds of billions of data items. 15 OSNs are not only huge in scale, they are predicted to keep growing in the coming years both in the number of users and in the amount of data users upload and share. The vast amount of data in social media is user-generated and personal most of the time, which clearly calls for appropriate privacy preservation mechanisms that allow users to benefit from social media while adequately protecting their personal information. Protecting users' privacy is not only essential to respect the Universal Declaration of Human Rights but also to serve as a first line of defense to mitigate cybercrime and other illegal activities that leverage the data obtained due to privacy breaches in social media, such as social phishing, identity theft, cyberstalking, and cyberbullying. There have been many efforts devoted to study privacy in social media and how to protect users' personal
Cybercrime has steadily increased over the last years, being nowadays the greatest security concern of most enterprises. Institutions often protect themselves from attacks by employing intrusion detection systems (IDS) that analyze the payload of packets to find matches with rules representing threats. However, the accuracy of these systems is as good as the knowledge they have about the threats. Nowadays, with the continuous flow of novel forms of sophisticated attacks and their variants, it is a challenge to keep an IDS updated. Open Source Intelligence (OSINT) could be explored to effectively obtain this knowledge, by retrieving information from diverse sources. This paper proposes a fully automated approach to update the IDS knowledge, covering the full cycle from OSINT data feed collection until the installation of new rules and blacklists. The approach was implemented as the IDSoSint system and was assessed with 49 OSINT feeds and production traffic. It was able to identify in real time various forms of malicious activities, including botnet C&C servers communications, remote access applications, brute-force attacks, and phishing events.
The proposed model of a multi-agent environment for e-learning due to increase in the cybercrime was found to be susceptible to DOS attacks and personification attacks that is dealt by embedding the mitigation mechanism of IP filtering with the proposed model. Developing the e-learning environment in which an algorithm has been implemented to filter the spoofed IPs from the legitimate IPs. The multi-agent environment implemented with all the agents has been fortified along with the IP filtering algorithm which proves to be improving the performance. An e-learning platform usually concentrates on delivering the content to the legitimate users with an emphasis on the content delivery alone wherein the components of the Information Security need to be concentrated more. In this paper, we have proposed a solution that caters the Confidentiality, Integrity and Availability without any hindrance to the performance of the system. The implementation of Dynamic Hop Count Filtering in the Multi-agent based e-learning environment is a system that mitigates the IP spoofing based DOS attacks on the system.
The Digital Age has brought great benefits for the human race but also some drawbacks. Nowadays, people from opposite corners of the World can communicate online via instant messaging services. Unfortunately, this has introduced new kinds of crime. Sexual predators have adapted their predatory strategies to these platforms and, usually, the target victims are kids. The authorities cannot manually track all threats because massive amounts of online conversations take place in a daily basis. Automatic methods for alerting about these crimes need to be designed. This is the main motivation of this paper, where we present a Machine Learning approach to identify suspicious subjects in chat-rooms. We propose novel types of features for representing the chatters and we evaluate different classifiers against the largest benchmark available. This empirical validation shows that our approach is promising for the identification of predatory behaviour. Furthermore, we carefully analyse the characteristics of the learnt classifiers. This preliminary analysis is a first step towards profiling the behaviour of the sexual predators when chatting on the Internet.
Technology has become a pivotal point in our society, this dependency is becoming increasingly more critical on a daily basis. This ranges from people to businesses and on a larger scale government organisations who are now increasingly focusing on becomingmore cyber resilient. This paper intends to provide an overview as to why a comprehensive knowledge management framework is necessity for SMEs on tackling cyber and cyber-enabled crimes. The paper explores new sources of data to reliably understand the importance as to why such a framework is required. This type of system can pave the way for SME's to devise their cyber strategy and to be able to respond efficiently to cyber-related incidents. One of the cyber weakness and vulnerabilities for the SME's are through their interactivity and or engagement with their suppliers and customers. Namely the interactions which take place via their respective internet sites, email communications, ports (using external devices, USB, CD drive, SD cards etc..) or the router (The use of their WIFI systems). The benefits of this framework model will be primarily to educate SME's in becoming more cyber resilient and provide them with the knowledge, awareness and techniques to identify weaknesses and vulnerabilities in their computer networks, devices and internet usage.
The COVID-19 pandemic was a catalyst for many different trends in our daily life worldwide. While there has been an overall rise in cybercrime during this time, there has been relatively little research done about malicious COVID-19 themed AndroidOS applications. With the rise in reports of users falling victim to malicious COVID-19 themed AndroidOS applications, there is a need to learn about the detection of malware for pandemics-themed mobile apps.. In this project, we extracted the permissions requests from 1959 APK files from a dataset containing benign and malware COVID-19 themed apps. We then created and compared eight unique models of four varying classifiers to determine their ability to identify potentially malicious APK files based on the permissions the APK file requests: support vector machine, neural network, decision trees, and categorical naive bayes. These classifiers were then trained using Synthetic Minority Oversampling Technique (SMOTE) to balance the dataset due to the lack of samples of malware compared to non-malware APKs. Finally, we evaluated the models using K-Fold Cross-Validation and found the decision tree classifier to be the best performing classifier.
Financial crimes are debilitating problems for economies, especially emerging ones. The scourge of financial crimes includes money laundering, fraud, drug and human trafficking, terrorism financing, bribery, embezzlement, market manipulation, tax evasion, identity theft, forgery and cybercrime. These problems are so intractable and potentially destructive that the collective effort to prevent or contain them has gone global. The imperative of enhanced transparency and financial system integrity, not only in national financial systems but also in the international financial order, has become inevitable. This has resulted in the landmark frameworks of the United Nations Convention against Illicit Traffic in Narcotic Drugs and Psychotropic Substances and the G7's Financial Action Task Force. This paper discusses the legal combat of financial crimes in two major African economies: Nigeria and South Africa, with particular emphasis on money laundering and terrorism financing due to their direct negative macro-economic implications for any economy. The focus on the twin problems in those two economies is based on their pre-eminent position in Africa. The paper examines the legal frameworks for the prevention or containment of the scourge in the two countries and interrogates measures that could engender their effective control.
The steady growth of cryptowallets users and the widespread of cryptocurrencies adoption has inadvertently risen the numbers of cybercrime. The decentralized and pseudoanonymous nature of cryptocurrencies impose a unique challenge to the investigators. Unlike investigation on fiat currency where banks can be contacted to freeze account, cryptocurrencies do not have a centralized entity that can be contacted. On top of that, studies have shown that using a generic digital forensics methodology to collect and preserve cryptowallets, which often involves imaging and seizing, are ineffective for cryptocurrencies investigation. This is because criminals can recover his seized cryptowallets into other devices and thus continue to make transactions for illegal activities. This defeats the purpose of halting the criminals from conducting further crimes. Hence in this study we propose a methodology for preserving cryptowallets at crime scene. We then conducted evaluation on the methodology by using real case and simulation exercise on different types of cryptowallets. The result shows that our methodology can be used to properly preserve cryptocurrencies evidence. This study aims to identify gaps in cryptocurrencies investigation and address them by proposing a proper methodology.
This paper presents the first empirical study based on ground-truth data of a major Bullet-Proof Hosting (BPH) provider, a company called MaxiDed. BPH allows miscreants to host criminal activities in support of various cybercrime business models such as phishing, botnets, DDoS, spam, and counterfeit pharmaceutical websites. MaxiDed was legally taken down by law enforcement and its backend servers were seized. We analyze data extracted from its backend databases and connect it to various external data sources to characterize MaxiDed's business model, supply chain, customers and finances. We reason about what the inside view reveals about potential chokepoints for disrupting BPH providers. We demonstrate the BPH landscape to have further shifted from agile resellers towards marketplace platforms with an oversupply of resources originating from hundreds of legitimate upstream hosting providers. We find the BPH provider to have few choke points in the supply chain amendable to intervention, though profit margins are very slim, so even a marginal increase in operating costs might already have repercussions that render the business unsustainable. The other intervention option would be to take down the platform itself.
The last decades have been characterized by unprecedented technological advances, many of them powered by modern technologies such as Artificial Intelligence (AI) and Machine Learning (ML). The world has become more digitally connected than ever, but we face major challenges. One of the most significant is cybercrime, which has emerged as a global threat to governments, businesses, and civil societies. The pervasiveness of digital technologies combined with a constantly shifting technological foundation has created a complex and powerful playground for cybercriminals, which triggered a surge in demand for intelligent threat detection systems based on machine and deep learning. This paper investigates AI-based cyber threat detection to protect our modern digital ecosystems. The primary focus is on evaluating ML-based classifiers and ensembles for anomaly-based malware detection and network intrusion detection and how to integrate those models in the context of network security, mobile security, and IoT security. The discussion highlights the challenges when deploying and integrating AI-enabled cybersecurity solutions into existing enterprise systems and IT infrastructures, including options to overcome those challenges. Finally, the paper provides future research directions to further increase the security and resilience of our modern digital industries, infrastructures, and ecosystems.
The emergence of digital technologies contributed to the emergence and rapid development of digital commerce, and at the same time, the number of electronic payments, the use of digital and virtual currencies increased. The article presents an analysis of the legal nature of such a financial instrument as cryptocurrency, characterizes the distinctive features, highlights the advantages and disadvantages. The purpose of the work is to consider the regulatory legal position of cryptocurrency in the modern world, to highlight the legal practice in cases of the circulation of cryptocurrency, to study the role of cryptocurrency in transnational offenses, to explore possible options for combating cybercrime, which is carried out using the use of cryptocurrency. The methodology of the work is represented by a set of methods and techniques, operations that are used to study the topic and achieve the set goal, namely: hermeneutic, historical, extrapolation, comparative-legal, comparison and generalization, analysis, synthesis, deduction. Results of the work: in today's reality there is no unified international legal regulation of cryptocurrency, which complicates the prevention and fight against transnational offenses, the means or subject of which are cryptocurrencies and mining.
"Fintech's development has amplified cybercrime, prompting trust and security concerns in banking. While earlier research predominantly viewed Fintech adoption through a tech-centric lens, emphasising its benefits, there is a paucity of studies on cognitive resistance arising from Fintech controversies. This review synthesises previous Fintech literature on behavioural intentions in banking, emphasising the role of trust, security, and other factors, and highlights existing research gaps. Utilising the ROSES (RepOrting standards for Systematic Evidence Syntheses) framework, a Systematic Literature Review was conducted, analysing 26 articles from Scopus and Web of Science (WoS) databases (2009-2022). Thematic analysis produces five primary themes (UTAUT2 variables; risk; trust; quality; and other), branching into 24 sub-themes. The weight analysis emphasises the best well-utilised predictors like performance expectancy, trust, security, perceived usefulness, and attitude. In addition, the review identifies research gaps and offers recommendations for future studies using the TCCM (Theory, Context, Constructs, and Method) framework. This research provides insights to Fintech companies and regulatory authorities on the preferred attributes of Fintech services that can enhance their adoption within the banking sector."
Fast and accurate malicious domain detection is an essential research theme to prevent cybercrime, and machine learning is an attractive approach for detecting unseen malicious domains in the past decade. In this paper, we present MADMAX (MAchine learning-baseD MAlicious domain eXhauster), a browser-based application leveraging extreme learning machine (ELM) for malicious domain detection. In contrast to the existing work of ELM-based domain detection, MADMAX newly introduces two methods, i.e., selection of optimized features to provide higher accuracy and throughput based on permutation importance and real-time training to retrain a model with an updated malicious dataset for continuous malicious domain detection. We demonstrate that MADMAX fairly outperforms the existing work with respect to accuracy and throughput by virtue of the selection of optimized features. Moreover, we also confirm a model with real-time training stably detects even unseen malicious domains, whereas accuracy of a model without the real-time training decreases due to the unseen domains. The source codes of MADMAX is publicly available via GitHub.
Cyber scam, an on-trend subculture among Nigerian urban youth, has posed a major security and economic threat to the global community. As part of the strategies developed to 'institutionalise' their illicit business and evade the punitive hand(s) of the Nigerian and international law enforcement agencies, cyber scammers in Nigeria have devised various strategies, including the deployment of anti-language and slangy expressions in their transactional and social interaction. Extant studies on cybercrime in the Nigerian context have largely addressed the phenomenon from the sociological, economical and information technological perspectives with little attention paid to it from a linguistic perspective. Therefore, this study, gathering data with the deployment of ethnographic techniques, and drawing from Halliday's (1976) concept of anti-language, investigates the linguistic strategies employed by cyber scammers in Southwestern Nigeria in their social and transactional interaction. Findings reveal slangy coinages and overlexicalisation, and relexicalisation (semantic extension) are anti-language phenomena characterising cyber scammers' language in region. These are achieved through the deployment of linguistic sub-strategies such as reduplication, clipping, blending and acronym (initialism).
A lot of research has been devoted to understanding the technical properties of amplification DDoS attacks and the emergence of the DDoS-as-a-service economy, especially the so-called booters. Much less is known about the consequences for victimization patterns. We profile victims via data from amplification DDoS honeypots. We develop victimization rates and present explanatory models capturing key determinants of these rates. Our analysis demonstrates that the bulk of the attacks are directed at users in access networks, not at hosting, and even less at enterprise networks. We find that victimization in broadband ISPs is highly proportional to the number of ISP subscribers and that certain countries have significantly higher or lower victim rates which are only partially explained by institutional factors such as ICT development. We also find that victimization rate in hosting networks is proportional to the number of hosted domains and number of routed IP addresses and that content popularity has a minor impact on victimization rates. Finally, we reflect on the implications of these findings for the wider trend of commoditization in cybercrime.
Photo Response Non-Uniformity (PRNU) is one of the most effective fingerprints used to detect the source camera of an image. Image Anonymization on the other hand, is a task of fooling the source camera identification, in order to protect the user's anonymity in sensitive situations involving whistleblowers, social activists etc. To protect the privacy of users especially over the web, image anonymization is of huge importance. Counter-Forensic attacks on source camera identification try to make an image anonymous by nullifying the detection techniques. For almost every counter-forensic source camera identification attack, anti-counter attacks are being designed and hence there is a need to either strengthen the previous counter-forensic attacks or design a new attack altogether. In this work, we propose a new counter-forensic attack to source camera identification, using the Universal Wavelet Relative Distortion function designed for steganography. The main principle behind UniversalWavelet Relative Distortion is to embed changes in an image in regions such as textures or noisy parts which are crucial to source camera identification. We show through our experiments, when a random bit-string is inserted recursively in an image, the correlation strength of the noise residual based source camera identification gets significantly weak and such methods fail to map the source camera of the image under question. In the proposed method, the visual quality of the modified image is not changed, which makes our method a strong solution to image anonymization.
The online sexual exploitation of children is facilitated by websites that form virtual communities, via hyperlinks, to distribute images, videos, and other material. However, how these communities form, are structured, and evolve over time is unknown. Collected using a custom-designed webcrawler, we begin from known child sexual exploitation (CE) seed websites and follow hyperlinks to connected, related, websites. Using a repeated measure design we analyze 10 networks of 300 + websites each - over 4.8 million unique webpages in total, over a period of 60 weeks. Community detection techniques reveal that CE-related networks were dominated by two large communities hosting varied material not necessarily matching the seed website. Community stability, over 60 weeks, varied across networks. Reciprocity in hyperlinking between community members was substantially higher than within the full network, however, websites were not more likely to connect to homogeneous-content websites. (C) 2016 Elsevier Inc. All rights reserved.
Biometric and biographic exploits can help criminals avoid detection and thwart identification systems, as well as identify potential targets in social networks for commercial purposes.
Hacks are one of the most damaging types of cryptocurrency related crime, accounting for billions of dollars in stolen funds since 2009. Professional investigators at Chainalysis have traced these stolen funds from the initial breach on an exchange to off-ramps, i.e. services where criminals are able to convert the stolen funds into fiat or other cryptocurrencies. We analyzed six hack subnetworks of bitcoin transactions known to belong to two prominent hacking groups. We analyze each hack according to eight network features, both static and temporal, and successfully classify each hack to its respective hacking group through our newly proposed method. We find that the static features, such as node balance, in degree, and out degree are not as useful in classifying the hacks into hacking groups as temporal features related to how quickly the criminals cash out. We validate our operating hypothesis that the key distinction between the two hacking groups is the acceleration with which the funds exit through terminal nodes in the subnetworks.
Digital Forensics is the branch of science dealing with investigation of evidences recovered from digital devices, to safeguard against rapidly increasing cyber crimes in today's digital world. The Source Camera Identification (SCI) problem is to map an image under question correctly to its source device. Following a Digital Forensic approach, the source of an image is detected by post-priori investigation of traces left behind in the image, by the camera. Such traces are generated due to the post-processing operations an image undergoes inside a digital camera, after being captured. In this paper, we model the SCI problem as a machine learning classification problem and focus on the most crucial component of a learning model, i.e. feature selection. We propose three different techniques for feature selection: Filter based approach, Wrapper based approach using Genetic Algorithm (GA), and also a hybrid approach with both Filter and Wrapper methods combined together. We investigate the source detection accuracy that each technique succeeds to achieve. Our experimental results suggest that the proposed methods produced a much compact feature set, hence considerably improve the source detection accuracy and minimize the training time of the learning model, as compared to the state-of-the-art.
Perceptual hashing is widely used to search or match similar images for digital forensics and cybercrime study. Unfortunately, the robustness of perceptual hashing algorithms is not well understood in these contexts. In this paper, we examine the robustness of perceptual hashing and its dependent security applications both experimentally and empirically. We first develop a series of attack algorithms to subvert perceptual hashing based image search. This is done by generating attack images that effectively enlarge the hash distance to the original image while introducing minimal visual changes. To make the attack practical, we design the attack algorithms under a black-box setting, augmented with novel designs (e.g., grayscale initialization) to improve the attack efficiency and transferability. We then evaluate our attack against the standard pHash as well as its robust variant using three different datasets. After confirming the attack effectiveness experimentally, we then empirically test against real-world reverse image search engines including TinEye, Google, Microsoft Bing, and Yandex. We find that our attack is highly successful on TinEye and Bing, and is moderately successful on Google and Yandex. Based on our findings, we discuss possible countermeasures and recommendations.
Big Data is a term associated with large datasets that come into existence with the volume, velocity and variety of data. An ever increasing human dependence on computers and automated systems has caused data to increase massively. The substantial collection of data is not only helpful for researchers but equally valuable to investigators who intend to carry out forensic analysis of data associated with the criminal cases. The conventional methodologies of performing forensic analysis have changed with the emergence of big data because big data forensic requires more sophisticated tools along with the deployment of efficient frameworks. Up till now several techniques have been devised to help the forensic analysis of small datasets but none of the techniques have been studied by coupling them with big data. Hence in this paper different techniques have been studied by closely analyzing their feasibility in the extraction and the forensic analysis of evidence from large amounts of data. In this paper we discuss various sources of data and how techniques such as the MapReduce framework and phylogenetic trees can help a forensic investigator to visualize large data sets to conduct a forensic analysis. Since audio and video are an attractive source of forensic data therefore this paper also discusses the latest techniques that assist in the extraction of useful sound signals from noise infested audio signals. Similar techniques for forensic analysis of the images have also been presented. Based upon interviews conducted with the forensic professionals, the factors affecting big data forensic techniques along with their severity have been identified so that a scenario specific approach can also be adopted based upon the available investigative resources.
"Deterrence, treatment and legal response to online child sexual exploitation material (CSEM) offenders is enhanced by the joint consideration of technological behaviours and cognitions. CSEM offenders choose an environment based on both psychosexual needs and utility, and in turn that environment shapes future behaviour and reinforces cognitive distortions. This paper introduces lawless space theory, a theory of cyber criminality which posits that offenders will primarily choose and utilize a perceived lawless space that best meets their psychosocial and criminogenic needs in the most frictionless way; habituation and differential association in the lawless space will reduce the perceived risk; normalization will increase comfort in a particular lawless space, increasing friction costs that must be overcome to switch technologies; and additional countermeasures will only be implemented by offenders to reduce perceived risk and lower cognitive dissonance, but not at the expense of utility. The theory is explored through the exemplar of CSEM offences and offers explanatory power for the lifecycle of a lawless space, the use of legacy spaces in the presence of objectively more capable options, the simultaneous use of multiple spaces to meet different psychosexual needs, and the adoption of new technologies by offenders. Additionally, the gateway choice and progression of lawless space usage informs investigations, risk assessments, and deterrence efforts and provides behavioural treatment targets."
In recent years, botnet has become a popular technique for deploying cybercrime because it is hard to be prevented and easily cause devastating loss. Therefore, in this paper, we proposed a novel approach that can automatically generate effective payload-based models purely based on the traffic of actual bot instances instead of signatures hand-tuned by human experts. In the learning phase, we group the packets of the botnet traffic and the benign traffic collected in advance according to their payload size and extract the signatures in the payload in order to generate the payload-based models. We then identify the high quality signatures to reduce the size of models via the information gain ratio and the probability. During the matching phase, the proposed approach uses these payload-based models to check each incoming packet. Moreover, these models can efficiently discriminate the malicious botnet traffic from the benign traffic since it doesn't perform any correlation between different packets. The proposed approach was evaluated with several real-world network traces. Experimental results demonstrate that the proposed approach can detect botnet traffic traces successfully (about 96.4%) with high efficiency and an acceptable low false alarm rate (about 0.9%).
The final type of cybercrime to be considered involves crimes that occur in online virtual worlds. While there is considerable literature available on other cybercrimes, as outlined in the previous chapters of this book, relatively little academic literature has been published concerning crime in online virtual worlds (Wall & Williams, 2007). Nevertheless, several cases have come to light concerning specific crimes in these environments, including both property offences (such as theft) and crimes against the person (such as sexual assault). It should be noted that while the term 'crime' will be used in this chapter to describe these events, they may not necessarily be illegal or criminal events, at least so far as the offline world would consider them to be. This chapter aims to describe these types of virtual crimes, and to determine if they could and should be considered criminal events. The effects of the crimes on the victims will also be considered, and the necessity for policing virtual worlds will be discussed. In addition, the online community needs to consider how to deal with virtual offenders - if their offence has real-world consequences, should they be punished offline, or only in the virtual world?
As cybercrime activity increasingly uses anonymous technologies, the dramatic growth of child sexual exploitations on the dark web has posed a challenge to law enforcement agencies. Data were gathered in 2019 and 2020 through face - to - face interviews with a juvenile in New Taipei City, Taiwan. This paper extends the application of social learning theory to online child sexual abuse behavior. Research testing Akers's social learning theory has been confined to sexual offenses against children on the dark web. A social learning concept of differential association, definitions, imitation, and differential reinforcement is illustrated and supported by online child sexual abuse behavior findings. Before it occurs, preventing child sexual abuse has become a critical issue and necessary effort from all areas of society: family caring, school instruction, community-based treatment, and social values. An innovative social learning strategy to battle online child sexual abuse is proposed to reduce juvenile delinquency on the dark web. Every child, parent, teacher, or social worker who works with children should realize what child sexual abuse is and prevent it.
As an effective tool for penetrating firewalls and bypassing supervision and censorship, Shadowsocks is widely used, and there are also illegal network activists. If there is a traffic identification system that can identify applications over Shadowsocks, it can greatly facilitate the supervision, traceability, and evidence collection of cybercrime activities. In this paper, we propose an application over Shadowsocks's traffic identification system, which adds the sliding window JS divergence feature on the basis of the traditional statistics and distribution based on traffic packet length and timestamp. This feature can effectively reduce the impact on smart phone differences without reducing the accuracy of the same individual device application traffic recognition, while maintaining the characteristics of the application as much as possible, thereby greatly improve our traffic recognition the robustness of the system. Experimental results show that our system can achieve an accuracy of 94.5% on the same smart phone. On the data sets collected by different smart phones, our work has achieved an accuracy of 80.4%, which has certain practicality.
This Innovative Practice Full Paper describes an implementation of a Social Engineering course. Social Engineering (SE), which is the process of obtaining unauthorized access to systems by exploiting humans, has been flagged by the FBI and US Banking Industry as a threat to national security. According to a report by McAfee, the global cost of cybercrime has exceeded $1 trillion since 2018, and an additional $ 145 billion was spent on cybersecurity in 2020. Research has shown that 97% of cybersecurity attacks employ some element of social engineering. This Innovative Practice paper will present an effective and hands-on course designed to introduce students to social engineering penetration testing. The course is based on a social constructivism approach to learning, where students first develop an understanding of the individual and social context of social engineering. It uniquely combines the psychology of human behavior, legal precepts, and computer science to identify how psychological principles are applied in cyberspace to exploit individuals and how the same principles can be applied during SE penetration testing to detect vulnerabilities.
There is worldwide concern over the issues of citizenship and the misuse of information technology such as cybercrime and also environmental damage due to global warming. The main factors that cause these problems are the characteristics of humans themselves, and the weakness of digital and ecological citizenship, which ultimately leads to ignorance of the negative effects of technology abuse and also excessive exploitation of nature and environmental destruction without being able to maintain balance. These problems have a very large impact on the survival of humankind. Therefore, it is imperative that appropriate solutions are found. This paper aims to find a solution that has a programmed and sustainable structure in the face of weak digital and ecological citizenship. A suggested solution is innovative learning in civic education (Civics) as a curricular program in schools to shape digital and ecological citizenship for young citizens. Digital citizenship is formed using a value-based learning model often known as the value clarification technique (VCT) by considering nine elements of digital citizenship. Ecological citizenship is strengthened via eco-literacy using a project-based ecological citizen learning model.
ResearchGate has been regarded as one of the most attractive academic social networking site for scientific community. It has been trying to improve user-centered interfaces to gain more attractiveness to scientists around the world. Display of journal related scietometric measures (such as impact factor, 5-year impact, cited half-life, eigenfactor) is an important feature in ResearchGate. Open access publishing has added more to increased visibility of research work and easy access to information related to research. Moreover, scientific community has been much interested in promoting their work and exhibiting its impact to others through reliable scientometric measures. However, with the growing market of publications and improvements in the field of research, this community has been victimized by the cybercrime in the form of ghost journals, fake publishers and magical impact measures. Particularly, ResearchGate more recently, has been lenient in its policies against this dark side of academic writing. Therefore, this communication aims to discuss concerns associated with leniency in ResearchGate policies and its impact of scientific community.
This research addresses the implementation of digital signature technique for electronic prescription to prevent cybercrime problem such as robbery, modification and unauthorized access. In this research, RSA 2048-bit algorithm will be implemented in Java programming and android based system. Secure Electronic Prescription (SEP) application design is intended to combine given services, such as confidentiality, authentication, and non-repudiation. Cryptography is used to ensure the prescription file and QR-Code for detailed information on the prescription that have been given only for the pharmacist. The QR-Code will be encrypted using an asymmetric algorithm based on NIST Standard. In the application, there are two schemes, namely the protection schemes and verification scheme. This research uses black-box testing, and white-box testing to test input values, code and output without testing the process and design that occurs in the system. We demonstrate the implementation of cryptography in Secure Electronic Prescription (SEP). The implementation of digital signature in this research can prevent archive thievery which is shown on implementation and is proven on the test.
Cybercrime has increased as a result of technology improvements and people's increasing reliance on smartphones and other technologies. SMS enables the dissemination of critical information, which is especially significant for non-digital savvy users who are typically the most isolated. Smishing, often known as SMS phishing, is the practice of sending phony text messages to trick someone into divulging personal information or installing malware. In Kenya, smishing crimes have been noted to escalate at a higher rate. However, there is no comprehensive investigation that has been done involving smishing attacks. There are various proposed solutions for mitigating smishing attacks. However, no existing solution authenticates the sender, filters the smishing content from the message, and informs the user of the potentially harmful content. Therefore, this study presents a novel method to authenticate the sender, filter smishing content from the message, and informs the user in case potentially harmful content exists in the message. Python, MYSQL, and Naive Bayes classifier were used to develop the model. The model will help mobile phone users to identify fraudulent messages sent by smishers quickly and effectively.
As cybercrime activity increasingly uses anonymous technologies, the dramatic growth of child sexual exploitations on the dark web has posed a challenge to law enforcement agencies. Data were gathered in 2019 and 2020 through face - to - face interviews with a juvenile in New Taipei City, Taiwan. This paper extends the application of social learning theory to online child sexual abuse behavior. Research testing Akers's social learning theory has been confined to sexual offenses against children on the dark web. A social learning concept of differential association, definitions, imitation, and differential reinforcement is illustrated and supported by online child sexual abuse behavior findings. Before it occurs, preventing child sexual abuse has become a critical issue and necessary effort from all areas of society: family caring, school instruction, community-based treatment, and social values. An innovative social learning strategy to battle online child sexual abuse is proposed to reduce juvenile delinquency on the dark web. Every child, parent, teacher, or social worker who works with children should realize what child sexual abuse is and prevent it.
Children nowadays are exposed to criminal activities and maltreatment, through the use of internet. The speedy and easy access to internet using smart phones becomes the contributing factors of many criminal activities including cyber bullying. Cyber bullying is a cybercrime that happened among children in Malaysia. Based on 2017 statistic, survey conducted by DIGI Cyber Safe found out that 90% of the children are at risk of being cybeibullied. The Malay Mail dated 27 Oct 2018 reported that Malaysia ranked 6th place in a global cyberbullying survey conducted among 28 countries, and 2nd place among the Asia countries in cyberbullying. Therefore, this study is conducted to examine the scenario of children cyber bullying in Malaysia and to identify the related legal provisions in force. This study employs library research method for data collection. The study analyse statutes, books, journals, reports, conference proceedings, and other periodicals. This study found out that Malaysia is still lacking on the provisions of protection of child in cyberbullying. (C) 2020 Published by European Publisher.
"This paper presents an analysis of the problems associated with effects that family has on a child's behavior. Our focus was on correlation between deviant behavior of young people and inadequate parental attachment. Relevance of this study is based on several fairly important, albeit dramatic, features of today including rapid increase in crime rates aggravated by involvement of young people in the new drug-related crimes and cybercrime, and a reported prevalence of disadvantaged and dysfunctional families in Russia. The purpose of this work is to examine various forms of parental attachment and assess their influence on deviant behavior of children. To these ends, we aimed to specify and clarify the conceptual apparatus; identify most common types of attachment within a family and forms of youth deviant behavior hereby; assess the modes and effects of inadequate parental attachment resulting in deviances and disorders among adolescents. Research of authentic sources allowed for a deep comparative analysis on the reported problem. The results confirmed the importance of a family in the individual system of life values, principles and goals."
Ethereum, as the second generation of blockchain technology, it not only brings many advantages, but also spawns various malicious incidents. Ethereum's anonymity makes it a hotbed of cybercrime, causing huge losses to users and severely disrupting the Ethereum ecosystem. To this end, this paper proposes a method for detecting malicious accounts in Ethereum based on ETH tracking tree (ETH-TT). Firstly, based on the transaction history replay mechanism, an ETH tracking algorithm for tracking the transaction amount of Ethereum is designed to obtain the ETH tracking tree, and extract sequence features from it. Then train the LSTM model to reduce the dimension of the sequence features to obtain the output features. Finally, detection is done by a machine learning classifier, fused with manual features from account transaction history. We uses 5576 malicious accounts and 4968 normal accounts as dataset for experiments. The results show that the ETHTT method can achieve an F1-score of 95.4% with the cooperation of the XGBoost classifier, which is better than the detection method using only manual features.
The popularity of mobile payment services lies in the convenient transactions they offer to users. In the age of growing cybercrime, however, mobile payment transactions carry risks of financial and data losses. It thus becomes critical to understand how risk and convenience have contrasting impacts on users' intention to use mobile payments. To investigate this, we consider various dimensions of perceived risk and perceived convenience to understand the net effect of their negative and positive influences on the intention to use. We also examine the actual use behavior predicted by intention along with the influence of habit. The research model, tested using survey responses from a sample of 215 users along with the descriptive answers from the survey respondents, helps us draw crucial insights. The study contributes to the field in a significant manner by providing insights into the balancing effect of risk and convenience on mobile payment service usage, as well as the development of the multi-dimensional scales for the key variables of risk and convenience.
With ever-increasing amounts of data collected from citizens and businesses in Smart City environments, public administration agencies manifest their position as central data holders. However, this great ownership of data makes them a target of cybercriminals on the hunt for illicit enrichment. The predominantly used type of cybercrime is phishing and increasingly spear phishing, a more personal, target-oriented kind of phishing. Such attacks make use of so-called persuasion techniques to lure their victims. In this study, four persuasion techniques, namely Authority, Urgency, Danger and Benefit, were tested for effectiveness in a two-phased field experiment cooperating with four German municipalities. In total, 3452 fake phishing e-mails were sent to 1276 public officials. Results show that the persuasion technique of Authority has worked best and therefore presumably poses the biggest threat to the information integrity of public sector agencies, followed by Urgency, Benefit and Danger. Additionally, the study provides insight on the potential impact of the effects of constant exposure to phishing and shows that the degree of domain-specificity of attacks impacts the susceptibility of victims.
Unarguably, the physical memory acquisition of contemporary computing devices has been an extremely pivotal and indispensable errand for digital forensics specialists because of its volatility of the embedded data, which was deposited from related computing resources. Nowadays, hundreds of millions of global naive or sophisticated end users exploit LINE application program (AP) as the instant messaging toolkit to conduct real-time communications including sending text messages or video clips. Unfortunately, this cutting-edge innovation has been taken advantage by the cybercrime syndicates, terrorists, or the extremists to fulfill their lucrative purposes. Hence, this research provides generic research methodologies for digital forensics experts to conduct whenever digital artifacts become mandatory in order to disclose some cybercrimes or the noncompliance of computer usage concerning LINE AP especially for the witness experts to testify in a court of law. On the other hand, this research paper pinpoints the volatile raw memory contains valuable and imperceptive digital artifacts that could reflect the interactions when LINE AP was utilized under Mac OS X. Copyright (c) 2016 John Wiley & Sons, Ltd.
Research SummaryWe found that romance fraud was rising year-on-year across every region in England and Wales, increasing 55% during the 3 years between October 2018 and October 2021. Fifty percent of all the romance fraud victims in the period resided in 17% of the places where romance fraud had occurred. A total of 439 locations (outward postcode areas) were identified as the power few in the first year of the data set. Of these 439 locations, 162 of them recurred in both of the following years, becoming chronic hot spot locations containing more than one in six of all reported romance fraud victims. The demography of victims in repeat locations differed considerably, but hot spots were more frequently predominantly populated by less affluent populations. Policy ImplicationsWe conclude that the current national one-size-fits-all fraud prevention approach may not be the most efficient or effective way to reach those victims who most require crime prevention advice. The National Fraud Intelligence Bureau, based in the City of London Police, could adopt a tailored approach to providing preventative information to local police forces based on the year-to-year patterns in crime and the associated intelligence provided by sociodemographic data sources such as Acorn.
As organizations accelerate digital transformation with mobile devices, cloud services, social media, and Internet of Things services, cybersecurity has become a key priority in enterprise risk management. While improving cybersecurity leads to higher levels of customer trust and increased revenue opportunities, rapidly evolving data protection and privacy regulations have complicated cybersecurity management. Against the backdrop of rapidly rising cyberbreaches and the emergence of novel cybersecurity technologies such as machine learning and artificial intelligence, this article introduces a cyber risk management framework, discusses a cyber risk assessment process, and illustrates a continuous improvement of cybersecurity performance and cyberinvestment cost analysis with a real-world cybersecurity example. (c) 2021 Kelley School of Business, Indiana University. Published by Elsevier Inc. All rights reserved.
"Various advantages offered by cloud computing business model has made it one of the most significant of current computing trends like personal, mobile, ubiquitous, cluster, grid, and utility computing models. These advantages have created complex issues for forensic investigators and practitioners for conducting digital forensic investigation in cloud computing environment. In the past few years, many researchers have contributed in identifying the forensic challenges, designing forensic frameworks, data acquisition methods for cloud computing systems. However, to date, there is no unique universally accepted forensic process model for cloud computing environment to acquire and analyze data available therein. This paper contributes in three specific areas to expedite research in this emerging field. First is designing a digital forensic architecture for cloud computing systems; second is evidence source identification, segregation and acquisition; and finally methods for partial analysis of evidence within and outside of a virtual machine (VM)."
Network forensics is an extension of the network security model, which traditionally emphasizes prevention and detection of network attacks. It addresses the need for dedicated investigative capabilities for investigation of malicious behavior in networks. Modern-day attackers tend to use sophisticated multi-stage, multihost attack techniques and anti-forensics tools to cover their attack traces. Due to the current limitations of intrusion detection and forensic analysis tools, reconstructing attack scenarios from evidence left behind by the attackers of an enterprise system is challenging. In particular, reconstructing attack scenarios by using the information from IDS alerts and system logs that have a large number of false positives is a big challenge. Many researchers have proposed to aggregate redundant alerts and correlate them to determine multi-step attacks [1]. This method is non-automated and rather ad-hoc. As an improvement, Wang at el. [7] proposed automating the process by using a fuzzy-rule based hierarchical reasoning framework to correlate alerts using so-called local rules and group them using so-called global rules. However, this approach falls apart when evidence is destroyed, and it does not assess the potential of the evidences admissibility so that the constructed attack scenario presented to a judge or jury has legal standing. In this talk, we will present a model [4] that systematically addresses how to resolve the above problems to reconstruct the attack scenario. These problems include a large amount of data including non-relevant data, missing evidence or evidence destroyed by anti-forensic techniques. Our system is based on a Prolog reasoning system MulVAL [6] using known vulnerability databases and an antiforensics database that we plan to extend to a standardized database like the NIST National Vulnerability Database (NVD). In this model, we use different methods, including mapping the evidence to system vulnerabilities, inductive reasoning and abductive reasoning to reconstruct attack scenarios. Besides, for the legal purpose, we codified the federal rules to this tool, aiming to help judge whether the evidence that is used to reconstruct the attack scenarios could be admissible in the court [5]. In addition, in order to help the investigators to quantify the probability of an attack path we use Bayesian Network to calculate the cumulative likelihood of the evidences. The goal of this research is to provide a tool that can reduce the investigators' time and effort in reaching definite conclusion about how an attack occurred. Also, this tool can be used to assist judge/jury or law students to better understand a multi-step, multihost attack towards an enterprise network by using a visual graph and probabilities. Our experimental results indicate that such a reasoning system can be useful for network forensics analysis.
"Identity-related crimes pose a significant problem to both the UK economy and also its citizens because they cause estimated annual losses of around 1.5billion pound. Not only do identity crimes cause considerable public concern, but they also create challenges for policing them; not least, because policing responses, in the broader regulatory sense, are often over-reactive or take the form of dramatic Public Relations gestures rather than coherent policing policy. Yet, the realities of identity-related crimes are quite different from the ways that they are perceived and even more important is that fact that this difference presents many challenges for those whose job is to police' them. This article will look at what identity crimes are and at the very real problems they pose for policing them as non-routine policing activities. The article will, firstly, map out identity crimes and outline the behaviours that we understand as identity crimes and their core characteristics. It will then consider how the characteristics map onto traditional police practice and consider some of the ways that the challenges have been addressed."
At present, the concept of metaverse has sparked widespread attention from the public to major industries. With the rapid development of blockchain and Web3 technologies, the decentralized metaverse ecology has attracted a large influx of users and capital. Due to the lack of industry standards and regulatory rules, the Web3-empowered metaverse ecosystem has witnessed a variety of financial crimes, such as scams, code exploit, wash trading, money laundering, and illegal services and shops. To this end, it is especially urgent and critical to summarize and classify the financial security threats on the Web3-empowered metaverse in order to maintain the long-term healthy development of its ecology. In this paper, we first outline the background, foundation, and applications of the Web3 metaverse. Then, we provide a comprehensive overview and taxonomy of the security risks and financial crimes that have emerged since the development of the decentralized metaverse. For each financial crime, we focus on three issues: a) existing definitions, b) relevant cases and analysis, and c) existing academic research on this type of crime. Next, from the perspective of academic research and government policy, we summarize the current anti-crime measurements and technologies in the metaverse. Finally, we discuss the opportunities and challenges in behavioral mining and the potential regulation of financial activities in the metaverse. The overview of this paper is expected to help readers better understand the potential security threats in this emerging ecology, and to provide insights and references for financial crime fighting.
Law enforcement agencies (LEAs) face a challenging problem to detect the tool marks of Skype traffic. Analyzing digital packets is an essential job in digital forensics. This study collects these packets, traces their routings, analyzes the relevant information, and explores the tool mark identification of Skype packets. It uses experimental data on a personal computer, investigates the characteristics of Skype traffic, and provides a tool mark analysis of network traffic. The study can assist the investigator in identifying the suspect using a Skype communication application to commit the crime.
"Given the prevalence of violence in Ukrainian families, measures to prevent parental abusive treatment against children are urgent. It is important to study today's youth awareness about violence within families in order to enhance a culture of engagement with spouses and children in future. The aim of the study is to examine students` reflective experiences and their attitudes towards forms, frequency, causes and effects of parental abusive treatment. During the research the following methods were used step-bystep: theoretical analyses of scientific resources, anonymous questionnaire, quantitative and correlation analyses. According to result of survey 98 students who were interviewed, none of them fell victim of sexual abuse in their families. However, more than a half (51%) of the students surveyed experienced some forms of parental physical punishment. All the interviewed students encountered psychological cases of parental violence. The mostprevalent forms of parental abuse among the interviewees are criticism, negative comparison, emotional detachment, arrogance, intimidation, blackmail and humiliation. The most prevalent consequences of parental abuse among students are anxiety, low self-esteem, insecurity, impatience, suspiciousness, constraint in communication. Students agree that budget limitations, forced labor, criticism, spanking, emotional detachment, ignoring type of communication, reproach, blackmail are acceptable methods of punishment to use when raising their own children. These results clearly demonstrate the problem of the impact of parental abuse on children and its consequences in the future. A virtual dimension of the actualized problem is considered, namely: - virtualization of aggression and cruelty in the postmodern world. - the level of determining the factor of cruelty from the space of virtual culture. - the mirror image of everyday cruelty in the virtual environment; - the phenomenon of video games as a source and context of representation of the factor of cruelty in behavioral realities; - cybercrime as a virtualized result of cruelty in family and everyday realities. - futurological perspectives of virtualization of cruelty in communicative culture in general and in family relations in particular. The postmodern world is fundamentally different from the traditions and culture of the past, primarily due to the development of computer technologies and the virtualization of life in general. So, for example, virtual communities have become, in a certain way, another world, a second reality of life in general. And certain behavioral factors, in particular cruelty in the private environment, became a projection of such a phenomenon as cybercrime. Video games are a unique modern phenomenon, which multipolarly absorb all facets of human potential, communicative tendencies, behavioral and characterological factors, from the warmth of interpersonal relationships to the extreme degree of cruelty."
This study aims to understand communicative activities of an illicit cyber-trade community on an anonymous social platform, Reddit. Based on the communication-as-constitutive of organization (CCO) perspective, the study identifies participants with different levels of engagement, and examines how their discursive engagement collectively reflects the ways in which illicit market users co-orient themselves to respond to a crisis event (i.e., market shutdown). The empirical case for this study is a subreddit channel dedicated to what was once the largest dark web market: r/AlphabayMarket. We examine two month period's posting activities until market was permanently shut down in July 2017. Our analysis comprises three parts. First, a social network analysis was conducted to identify key and non-key players in the community. Second, a structural topic modeling was computed to inductively infer topic clusters. Third, posts were manually reviewed to articulate the process of co-orientation manifest in the results of topic modeling.
"Online child pornography is a repulsive reality which cannot be ignored; the biggest child pornography manufacturing ring in South African history was cracked with the arrest of eight family members. There are more than 116 000 Internet searches daily for child pornography. Pornography is no longer confined to consenting adults, children are not only being exposed to pornographic material but are increasingly being used as victims of child pornography. In South Africa, the Internet and Cell phone Pornography Bill tabled in 2010 aims to make it illegal for Internet and mobile phone service providers to distribute pornography or permit it to be distributed. The objective of the Bill is to protect children from child pornography and women from the indignity of being seen as objects of pornography. This paper takes as its focal point the continuing crisis which centres on the conceptual framework, the existing legislation regulating online child pornography and measures to curb infringement. (C) 2012 Mpakwana Annastacia Mthembu. Published by Elsevier Ltd. All rights reserved."
"This paper presents several scenarios where digital evidence can be collected from mobile devices, their legal value keeping untouched. The paper describes a robust methodology for mobile forensics developed through on-field experiences directly gained by the authors over the last 10 years and many real court cases. The results show that mobile forensics, digital analysis of smartphone Android or iOS can be obtained in two ways: on the one hand, data extraction must follow the best practice of the repeatability procedure; on the other hand, the extraction of the data must follow the best practice of the non-repeatability procedure. The laboratory study of the two methods for extracting digital data from mobile phones, for use as evidence in court trials, has shown that the same evidence can be obtained even when the procedure of unavailability of file mining activities has been adopted. Indeed, thanks to laboratory tests, the existence of multiple files frequently and continuously subjected to changes generated by the presence of several hashes found at forensic extractions conducted in very short moments of time (sometimes not exceeding 15 min) has been proven. If, on the other hand, the examination of a device is entrusted to a judicial police officer in order to conduct a forensic analysis to acquire data produced and managed by the user (such as images, audio, video, documents, SMS, MMS, chat conversations, address book content, etc.) we have sufficient grounds to believe that such examination can be organized according to the system of repeatable technical assessments."
Outside of criminology, dominant conceptions of postcolonial statehood in the Global South as 'fragile' or 'failed' have long been criticized. In criminology, however, the theoretical outcomes of this critique have been scarce. In this article we therefore ask how ideals and practices of transnational criminal justice are informed by and productive of specific (Global North) conceptions of statehood. Exploring encounters between transnational and local criminal justice in the context of international state-building in Mali and Liberia, we observe frictions in which statehood divergences and global hierarchies become apparent. Through penal aid, we argue, a particular kind of penal statehood is produced wherein the options of how to perform penality are increasingly limited by the embeddedness in global power asymmetries.
"Artificial intelligence (AI; broadly defined to include Machine Learning and Deep Learning) and automation are two current and reciprocal computing disciplines. As such, AI-powered software, programs, operating systems, and devices are developed on a massive scale to automate a wide variety of processes and operations. The principal aims of integrating AI and automation include efficiency, accuracy, and cost-reduction. While there is still an on-going cost associated with automation, the cost is typically many magnitudes smaller than the on-going costs incurred to get the job done manually, which increases the likelihood of generating a high return on investment. One emerging application of AI and automation is digital forensics. For example, US Federal and State Law Enforcement Agencies have started exploring the utility of AI-powered technology to make the job of digital forensics more impactful. This trend can maximize the accuracy of digital forensic investigations, enabling the resolution of more digital investigations. This article is categorized under: Digital and Multimedia Science > Cyber Threat Intelligence Digital and Multimedia Science > Artificial Intelligence Digital and Multimedia Science > Cybercrime Investigation"
The Internet of Things (IoT) connects physical and digital worlds with mobile devices, accompanied by a surge in cybersecurity issues. With the rapid adoption of mobile devices, mobile forensics emerges as a new interdisciplinary field that concerns many forms of sabotage and cybercrime in the context of mobile computing. One of the most common cyberattacks is tampering. Digital watermarking is a tamper-evident technique used to protect data integrity. In this paper, we present an antitamper image watermarking scheme designed for mobile communications with low computational cost. A reference matrix based on cellular network topology is introduced to guide the watermark embedding and extraction processes. This reference matrix serves as a lookup table to reduce computational complexity, thereby enabling efficient implementation on mobile devices. Our scheme is aimed at offering high accuracy in detecting and localizing tampered regions. We also achieve a high watermarking capacity while leaving the visual quality of the carrier images nearly unharmed. Experimental results validate the effectiveness of our scheme against various types of simulated forgery including cropping and copy/paste attacks.
Research examining cybercrime and personality is a critical, but understudied topic. From an applied perspective, such research will inform better practice in cyber defense. It will also contribute to our theoretical and practical understanding of trait expression in a relatively unstructured environment. Building on Trait Activation Theory, we predict that the unstructured environment computer attacks will elicit individual differences in decision making that can be attributed (in part) to differences in personality traits. Here we examine the varying approaches that individuals high in different Dark Triad traits and deception strategies take in executing cyberattacks. Using a hypothetical game scenario, Study 1 found that long-term mimicry deception and Machiavellianism were strong predictors of stealthy attacks, whereas narcissism and psychopathy were associated with shorter term or aggressive (i.e., brute force) attacks. Study 2 replicated this general pattern of results in a realistic lab simulation. The findings speak to the differential nature of the Dark Triad in the unstructured environment of cyber-attacks. These findings also contribute to our knowledge of how different attacks may emanate from individuals with different manipulative personality traits.
Today, the volume of evidence collected per case is growing exponentially, to address this problem forensics investigators are looking for investigation process with tools built on new technologies like big data, cloud services and Deep Learning (DL) techniques. Consequently, the accuracy of artifacts found also relies on the performance of techniques used, especially DL models. Recently, Deep Neural Nets (DNN) have achieved state of the art performance in the tasks of classification and recognition. In the context of digital forensics, DNN has been applied in the domains of cybercrime investigation such as child abuse investigations, malware classification, steganalysis and image forensics. However, the robustness of DNN models in the context of digital forensics is never studied before. Hence, in this research, we design and implement a domain independent Adversary Testing Framework (ATF) to test security robustness of black-box DNN's. By using ATF, we also methodically test a commercially available DNN service used in forensic investigations and bypass the detection, where published methods fail in control settings.
Introduction. The scale and destructive consequences of the unlawful impact on cyberspace is a key problem of modern geopolitics, and cyber reliability is recognized as one of the most important security priorities by the sub-jects of international relations. Problem Statement. Monitoring of cyber incidents and anomalies in information and communication systems and prompt response to risks determined by cyber threats require the development of a system of indicators and criteria for cybersecurity assessment. Purpose. Summarize the international experience of assessing the cybersecurity, to position countries by their level of development in the global space, to identify strengths and weaknesses in cybersecurity management, and to ensure effective protection of cyberspace at the national level. Materials and Methods. Used the component indices of the international rankings characterizing the poten-tial of the digital economy (ICT IDI, NRI, EGDI) and the participation of countries in the field of cybersecurity (GCI and NCSI). Results. It has been argued that cybersecurity ratings play the role of a kind of identifier of the relative ad-vantages and vulnerabilities of the national cyber strategies, and indicate the need for their review in order to strengthen protection against cyber-attacks and improve the cyber risk management system. In countries with a high level of economic development, which is largely based on the contribution of IT tech-nologies to the national production, the cybersecurity potential is significantly higher, regardless of geolocation. The discovered correlation between GCI, information society development indices (IDI, NRI, EGDI) and GDP per capita confirms that the digital transformation of the economy and society acts as a key driver of economic development if the information-and cyber-security are assured only. The best practices are highlighted, and critically weak segments of the national cybersecurity are identified. Conclusions. Using the NCSI indicators, the preparedness of Georgia and Ukraine to prevent the implementa-tion of fundamental cyber threats and to manage cyber incidents and large-scale cyber crises is assessed.
As the technical skills and costs associated with the deployment of phishing attacks decrease, we are witnessing an unprecedented level of scams that push the need for better methods to proactively detect phishing threats. In this work, we explored the use of URLs as input for machine learning models applied for phishing site prediction. In this way, we compared a feature-engineering approach followed by a random forest classifier against a novel method based on recurrent neural networks. We determined that the recurrent neural network approach provides an accuracy rate of 98.7% even without the need of manual feature creation, beating by 5% the random forest method. This means it is a scalable and fast-acting proactive detection system that does not require full content analysis.
Over the last few years, exploit kits (EKs) have become the de facto medium for large-scale spread of malware. Drive-by download is the leading method that is widely used by EK flavors to exploit web-based client-side vulnerabilities. Their principal goal is to infect the victim's system with a malware. In addition, EK families evolve quickly, where they port zero-day exploits for brand new vulnerabilities that were never seen before and for which no patch exists. In this paper, we propose a novel approach for categorizing malware infection incidents conducted through EKs by leveraging the inherent overall URL patterns in the HTTP traffic chain. The proposed approach is based on the key finding that EKs infect victim systems using a specially designed chain, where EKs lead the web browser to download a malicious payload by issuing several HTTP requests to more than one malicious domain addresses. This practice in use enables the development of a system that is capable of clustering the responsible EK instances. The method has been evaluated with a popular and publicly available dataset that contains 240 different real-world infection cases involving over 2250 URLs, the incidents being linked with the 4 major EK flavors that occurred throughout the year 2016. The system achieves up to 93.7% clustering accuracy with the estimators experimented.
The vigorous development of e-commerce breeds cybercrime. Online payment fraud detection, a challenge faced by online service, plays an important role in rapidly evolving e-commerce. Behavior-based methods are recognized as a promising method for online payment fraud detection. However, it is a big challenge to build high-resolution behavioral models by using low-quality behavioral data. In this work, we mainly address this problem from data enhancement for behavioral modeling. We extract fine-grained co-occurrence relationships of transactional attributes by using a knowledge graph. Furthermore, we adopt the heterogeneous network embedding to learn and improve representing comprehensive relationships. Particularly, we explore customized network embedding schemes for different types of behavioral models, such as the population-level models, individual-level models, and generalized-agent-based models. The performance gain of our method is validated by the experiments over the real dataset from a commercial bank. It can help representative behavioral models improve significantly the performance of online banking payment fraud detection. To the best of our knowledge, this is the first work to realize data enhancement for diversified behavior models by implementing network embedding algorithms on attribute-level co-occurrence relationships.
The World Wide Web has become the most essential criterion for information communication and knowledge dissemination. It helps to transact information timely, rapidly and easily. Identity theft and identity fraud are referred as two sides of cybercrime in which hackers and malicious users obtain the personal data of existing legitimate users to attempt fraud or deception motivation for financial gain. E-Mails are used as phishing tools in which legitimate looking emails are sent making the genuine users identity with legitimate content with malicious URLs. It helps to steal consumers personal data such as user names, account numbers, passwords and other financial account credentials. Spam E-Mails emerges or transforms as Phishing mails. Spoofed Mails plays a vital role in which the hackers pretends to be a legitimate sender posing to be from a legitimate organization which divulges the user to give his personal credentials. The content may escape from Content based filters or the email may be without any body of the message except malicious URL in it. This paper identifies malicious URLs in email through reduced feature set method. (C) 2013 The Authors. Published by Elsevier B.V.
Computer crime has increased exponentially in recent years as hardware, software, and network resources become more affordable and available to individuals from all walks of life. Software piracy is one prevalent type of cybercrime and has detrimentally affected the economic health of the software industry. Moreover, piracy arguably represents a rend in the moral fabric associated with the respect of intellectual property and reduces the financial incentive of product creation and innovation. Deindividuation theory, originating from the field of social psychology, argues that individuals are extricated from responsibility for their actions simply because they no longer have an acute awareness of the identity of self and of others. That is, external and internal constraints that would typically regulate questionable behavior are rendered less effective via certain anonymizing and disinhibiting conditions of the social and environmental context. This exploratory piece seeks to establish the role of deindividuation in liberating individuals to commit software piracy by testing the hypothesis that persons who prefer the anonymity and pseudonymity associated with interaction on the Internet are more likely to pirate software. Through this research, it is hoped that the empirical identification of such a social psychological determinant will help further illuminate the phenomenon.
Violence against women is a problem worldwide, with economic costs ranging from 1% to 4% of global gross domestic product. During the coronavirus disease 2019 lockdowns, the United Nations coined the term the Shadow Pandemic to describe the increase in global violence against women. Here, using variation in the intensity of government-mandated lockdowns in India, we show that domestic violence complaints increase significantly in districts with the strictest lockdown rules. We find similarly large increases in cybercrime complaints. However, rape and sexual assault complaints decrease in districts with the strictest lockdowns, consistent with decreased female mobility in public spaces, public transport and workplaces where they might be at greater risk for rape and sexual assault. Medium-term analysis shows that increases in domestic violence complaints persist 1 year later, while other complaints related to rape, sexual assault and cybercrimes return to pre-lockdown levels. Lockdowns may help control disease, but also come with potential costs. Domestic violence complaints in India increased in districts with the strictest lockdown rules, and remained higher 1 year later, even after restrictions were loosened.
The Darkweb has become the largest repository of unauthorized information compared to the surface web because of its benefit of anonymity and privacy. With these anonymity and privacy features, the dark web is also becoming a safe place for illegal activities and hence an increase of dark web usage and size of the onion-based URLs. With the increasing use of dark web users, it is the need for cybercrime investigators across the globe to classify dark web data for understanding various illegal activities to control and categorize URLs hosting such illicit activities with feature engineering. In this research, the Support Vector Machines (SVM) algorithm is used to understand the algorithm's efficiency for a proposed model to classify dark web data with optimization techniques. Text-based keywords from more than 1800 websites were collected by applying feature engineering techniques and the system's performance was evaluated with the SVM approach. The results are very encouraging as the Precision, Recall, and F-measure values are 0.83, 0.90 & 0.96 achieved with a dataset of 1800 URLs.
Forensic document analysis has become an important aspect of investigation of many different kinds of crimes from money laundering to fraud and from cybercrime to smuggling. The current workflow for analysts includes powerful tools, such as Palantir and Analyst's Notebook, for moving from evidence to actionable intelligence and tools for finding documents among the millions of files on a hard disk, such as Forensic Toolkit (FTK). Analysts often leave the process of sorting through collections of seized documents to filter out noise from actual evidence to highly labor-intensive manual efforts. This paper presents the Redeye Analysis Workbench, a tool to help analysts move from manual sorting of a collection of documents to performing intelligent document triage over a digital library. We will discuss the tools and techniques we build upon in addition to an in-depth discussion of our tool and how it addresses two major use cases we observed analysts performing. Finally, we also include a new layout algorithm for radial graphs that is used to visualize clusters of documents in our system.
The role of citizens in securing their smart homes is critical. We develop a research model, based on health related fear appeal frameworks, to examine the factors that motivate users to take security precautions. Our model synthesizes the protection motivation theory and the extended parallel process model. This synthesis distinguishes between users' danger-control and fear-control coping mechanisms. Further, we draw from the criminology literature to refine fear's compatibility with information threats as opposed to health ones. We also develop and incorporate the Internet of Self into our model to account for smart homes' personal relevance to users. The theoretical foundation, contributions and hypotheses are discussed.
study aimed to determine an efficient framework that caters to the security and consumer satisfaction for digital wallet systems. A quantitative online survey was carried out to test whether the six factors (i.e., transaction speed, authentication, encryption mechanisms, software performance, privacy details, and information provided) positively or negatively impact customer satisfaction. This questionnaire was divided into two sections: the respondents' demographic data and a survey on security factors that influence customer satisfaction. The questionnaires were distributed to the National University of Malaysia's professors and students. A sample of 300 respondents undertook the survey. The survey results suggested that many respondents agreed that the stated security factors influenced their satisfaction when using digital wallets. Previous studies indicated that financial security, privacy, system security, cybercrime, and trust impact online purchase intention. The proposed framework in this research explicitly covers the security factors of the digital wallet. This study may help digital wallet providers understand the customer's perspective on digital wallet security aspects, therefore motivating providers to implement appropriately designed regulations that will attract customers to utilize digital wallet services. Formulating appropriate security regulations will generate long-term value, leading to greater digital wallet adoption rates.
Online anonymity is considered as one of the great gifts of the Internet, but it also brings dangers to society, such as cybercrime, online sexual abuse and bullying, and love scams. Many people are fond of chatting online to make new friends, but how can they be sure that the person sitting behind the other computer is really the person they claim to be? By studying stylometry and keystroke dynamics features from chat data, it is feasible to reveal the actual gender of an online user. In this paper, we examined stylometry and keystroke dynamics features from chat data, and proposed a Random Forest based gender prediction approach by analyzing these features. In order to evaluate the effectiveness of the proposed approach, a data acquisition was conducted to capture the keystroke dynamics and text information when participants were chatting remotely via Skype. All participants were invited to chat freely on any topic they prefered in order to get to know each other. Based on our experimental result, the proposed approach achieved 72% prediction accuracy by analyzing on this free-text data captured only in 15 minutes.
With the popularity of computer and Internet, a growing number of criminals have been using the Internet to distribute a wide range of illegal materials and false information globally in an anonymous manner, making criminal identity tracing difficult in the cybercrime investigation process. Consequently, automatic authorship attribution of online messages becomes increasingly crucial for forensic investigation. Although researchers have got many achievements, the accuracies of authorship attribution with tens or thousands of candidate are still relatively poor which is generally among 20%similar to 40%, and cannot be used as evidence in forensic investigation. Instead of asserting that a given text was written by a given user, this paper proposes a novel authorship attribution model combining both profile-based and instance-based approaches to reduce the size of the candidate authors to a small number and narrow the scope of investigation with a high level of accuracy. To evaluate the effectiveness of our model, we conduct extensive experiments on a blog corpus with thousands of candidate authors. The experimental results show that our algorithm can successfully output a small number of candidate authors with high accuracy.
The extraordinary current craze around NFTs reflects their perceived value as a technological development that can bring greater certainty to questions of ownership and authenticity in fields like art and other collectibles. This is, among other things, the promise of crime prevention through technology, as ownership and authenticity are in the art world closely tied to criminal legal matters like theft, handling stolen goods and fraud. The crime prevention promise looks to fall flat though, as the technology seems to be less capable of delivering these benefits than has been assumed by its promoters. Much of the attraction of NFTs is therefore not actually based on effective crime prevention, but rather on hype. This paper explores the hype, and its relationship to the crime prevention promise of NFTs, through the lens of 'the social lives of things'. We argue that as well as social lives, things have criminal lives. Analysis sensitive to the criminal lives of things finds an NFT trading scene heated by emotion: excitement, attraction, temptation, speculative euphoria and acquisitive, possessive sentiment. This creates a sense of object agency more active than the cold traditional vision of material structure presented in standard criminological treatments of things-in-the-world as passive opportunity structures. The hyped NFT market trades in affecting objects that create crime in emotional as well as structural ways. We therefore arrive at a conclusion opposite to starting assumptions: far from preventing crime, NFTs are making it.
"This chapter offers short expert views on different aspects of police and judicial cooperation, including De Vries' contribution on cooperation regarding counterterrorism and the fight against organised crime; Bossong and Rieger's exploration of future data sharing options for police cooperation; Trauner's piece on the negotiations regarding the UK's future participation in Europol; Wilson and Carr's discussion of future cooperation in Forensic Science; and MacKenzie's considerations on the UK's role in EU counter-terrorism policy. This chapter also encompasses a number of expert views that focus specifically on security threats, including Sergi's analysis of Brexit creating opportunities for transnational criminal networks; Farrand's exploration of Brexit's impact on combatting counterfeiting; and Porcedda's and Lavorgna's pieces on the consequences of Brexit for the UK and the EU in the area of cybersecurity."
Technical Support Scams delivered through malicious webpages and linked advertisements are an endemic problem on the web. To avoid detection, scammers systematically vary the URLs, web page designs, and phone numbers of scam delivery. We describe a web -scale pipeline powered by Cloud AI services that continuously detects and links evidence of such scams. By integrating this evidence in a graph structure and exposing it for forensic analysis in a user interface, we enable investigators and law enforcement partners to track the evolving scope and sophistication of scam operations. This approach automates and scales investigative tradecraft that contributed to major enforcement actions by the FTC in 2016. From 2016-2018, it helped reduce consumer exposure to such scams by 5 percent.
The term biosocial in this study stands for a social variable that can potentially influence violence or cybercrime through a biological mechanism. This study adopted Owen (2014a)'s Genetic-Social and meta-theoretical framework to conceptualize cybercrimes and build a model of cyber violence that could help in predicting aggressive online behavior and the role of the state legislatures to prevent these types of cybercrimes. The study adopted a historical-normative research method which involved documentation search and archive segmentation according to the research variables. In this qualitative study, the data was collected from various sources including cyber law archives, databases and repositories. The data was analyzed with two research methods: system analysis and synthesis of internal and external documentation on the topic under study. The findings of the study revealed that cybercrimes and the related delinquent and criminal behavior are the results of the interaction between the human biological predispositions and social interactions. The study recommends establishing a National Cybersecurity Authority or A national level Cyber Security Guidance bureau to develop, implement, and supervise administrative and legal mechanism and strategies.
This paper gives a loom towards the growing spatial resolution necessary to beat the limitations of the imaging technology in surveillance and security disciplines. It has been observed that metropolis cities worldwide invest huge sum of money in surveillance camera system but few are closely observing the benefits and the costs of those investments and to measure the overall impact of surveillance cameras on crime rates. The low resolution coupled with poor quality optics is not be enough to identify the subject of interest in crowd, from a distance, in bad weather and any other limiting factor. In this paper we have introduced multi-frame super-resolution technique that does not require explicit motion estimation and will be useful for producing imagery evidence that the police might reasonably accept as proof of someone's identity. Mostly the research is done in this area by taking a SR image and then after adding their own noise patterns where as our algorithm are working on actual LR images of surveillance camera and getting a SR image while removing the original blur and noise. Our algorithm requires the training set of Low resolution (LR) images from a still camera to produce High resolution (HR) image data and enhances it using anisotropic Diffusion and De-noising. In the image based representations, this technique of super resolution provides a great step towards resolution independence. The application of this method was successfully demonstrated for the restoration from a short low resolution set of images into a super resolved image. This super resolution algorithm works best when the Diffusion is applied and noise reduction filters are applied.
Text authentication serves a vital role in the defense of digital identity and content against various types of cybercrime. The use of a digital signature is a common cryptographic technique for text authentication. Linguistic steganography can be applied to further conceal a digital signature within the corresponding text to facilitate data management. However, steganographic distortion lurking in the text, albeit almost imperceptible, has the potential to cause automatic computing machinery to make biased decisions. This has led to an interest in the pursuit of reversibility, the ability to reverse a steganographic process and remove distortion. In this article, we propose a reversible steganographic system for natural language text. We use a pre-trained transformer neural network for masked language modeling and embed messages in a reversible manner via predictive word substitution. Furthermore, we derive an adaptive steganographic route by taking account of predictive uncertainty, which is quantified based on a theoretical framework of Bayesian deep learning. Experimental results show that the proposed steganographic system can attain a proper balance between capacity, imperceptibility, and reversibility with close semantic and sentimental similarities between cover and stego texts.
Any digital device generates information that may become valuable evidence in the event of a cybercrime incident, security incident, or cyber-attack, but often the collection, management and preservation of this information is not done properly. In the legal field, once information has been obtained from the devices, it is very important to maintain it and preserve it from the initial time, through investigation, until the trial or investigation is concluded, and to preserve it for long term use, in order to avoid it to be tainted, damaged, changed or manipulated and so assuring reliability through the whole process. Preservation of digital evidence is an important aspect when deciding its admissibility in a trial in process, or in any future process, reopened by appeal, or as source of historical information. This paper contains a review of the state of the art about digital preservation in institutions dedicated to criminal investigation, analyzing concepts, related projects, tools and legal support in this area. The motivation of this paper is the idea of finding how close we are to having a framework useful to preserve digital evidence, ensuring integrity, hence increasing its admissibility, and supported by long term preservation technique.
Phishing is an attack that deceit online users by means of masquerading as a genuine website to pilfer their classified or personal information. This is one among the recognized cybercrime. Disparate phishing website detection systems were recently developed for the purpose of detecting the phishing websites. However, they fail to attain the desired output and are suffered from countless drawbacks like lower accuracy and higher training time. For trouncing such drawbacks, this paper proposes an effectual Hybrid Deep Learning (HDL)-centric Phishing Detection System (PDS) using the MCS-DNN classifier. At first, pre-processing is done on the input dataset for ameliorating its quality. Subsequently, clustering and feature selection (FS) are performed to lessen the processing time and elevate the accuracy using CoK-means and CM-WOA, respectively. The features which are chosen during FS are fed into the MCS-DNN classifier, which classifies the legitimate websites and phishing websites. Lastly, the K-fold cross-validations (KCV) are employed for effectively predicting the proposed system's accurateness. The outcomes highlight the robustness and predictive ability of the proposed PDS to distinguish the phishing as well as legitimate sites.
Speedy development has been witnessed in communication technologies and the adoption of the Internet across the world. Information dissemination is the primary goal of these technologies. One of the rapidly developing nations in the Middle East is Saudi Arabia, where the use of communication technologies, including mobile and Internet, has drastically risen in recent times. These advancements are relatively new to the region when contrasted to developed nations. Thus, offenses arising from the adoption of these technologies may be new to Saudi Arabians. This study examines cyber security awareness among Saudi Arabian citizens in distinct settings. A comparison is made between the cybersecurity policy guidelines adopted in Saudi Arabia and three other nations. This review will explore distinct essential elements and approaches to mitigating cybercrimes in the United States, Singapore, and India. Following an analysis of the current cybersecurity framework in Saudi Arabia, suggestions for improvement are determined from the overall findings. A key objective is enhancing the nationwide focus on efficient safety and security systems. While the participants display a clear knowledge of IT, the surveyed literature shows limited awareness of the risks related to cyber security practices and the role of government in promoting data safety across the Internet. As the findings indicate, proper frameworks regarding cyber security need to be considered to ensure that associated threats are mitigated as Saudi Arabia aspires to become an efficient smart nation.
The computer-controlled nature of contemporary road vehicles has reached a point where the in-vehicle computer control system manages a significant proportion of motor vehicle dynamic operation. In highly automated vehicles the automated driving system (ADS), undertakes the dynamic driving functions and overarching responsibility for the vehicle's journey. Central to Australia's future road transport system is the reliable operation of ADSs. ADS are networked, transmitting, and receiving data. This is a weakness that could be exploited to cause harm to individuals or the transport system. This article examines whether the Commonwealth criminal law is adequate for protecting against, or deterring from, unauthorised interference with ADSs. Three scenarios of interference are investigated. Scenario 1 is the intentional unauthorised interference with an ADS to cause harm. Scenario 2 involves unknowingly uploading of software containing harmful malware to an ADS. Scenario 3 is a third party installing unauthorised spyware on an ADS. This article argues that overall Commonwealth criminal law, specifically the cybercrime offences relating to unauthorised and malicious interference with computer systems, appear able to apply to various forms of interference with ADSs.
Previous empirical work has shown that the inclusion of low self:control in multiple regression models reduces the statistically significant independent effects of gender on general crime and delinquency to non-significant levels, even when controlling for known theoretical predictors of crime. In support of Gottfredson and Hirschi's (1990) claims, this work has also shown that opportunity interacts with low self-control in explaining deviant behavior. To date though, the relationship between gender, opportunity, and self-control in explaining cybercrime is still relatively unknown. Using a sample of Korean Adolescents (N = 1,091), the current study attempts to fill these gaps in the literature by examining the effects of opportunity and low self-control in relation to gendered disparities in rates of online harassment. Inconsistent with the theory, findings suggest that girls and boys report similar levels of self-control and that LSC interacts with opportunity to negatively predict online harassment. However, consistent with general theory of crime, results show that opportunity and LSC are significant predictors of online harassment in separate gender models while controlling for a host of other theoretical predictors of crime and delinquency.
"Thousands of cyber-attacks (fraudulent online activities to acquire users' sensitive information via email, during online transactions, live video streaming, online gaming and browsing) are launched every day against Internet users across the world. To prevent these attacks, researchers have responded with a number of protection systems. Currently, the methods which cyber-attackers use to conduct attacks is associated with exploiting humans. Such attacks are recorded more frequently than before, and they are more challenging to control. Traditional security countermeasures are unable to prevent breaches targeting the human element. This paper describes the state of the art of cyber security attacks, countermeasures, and protection tools related to everyday online activities. It provides a useful cyber-attack taxonomy and classification which helps to involve in a protection process to identify attacks and measures for cyber security. Existing protection schemes that target the cyber threats and risks are evaluated against three of our criteria for an effective measure: resilience to cyber-attacks' countermeasures; real-time support and needs-based action; and training and educational materials to increase users' awareness of cybercrimes. Potential features of smart solutions to cybercrime are also identified."
Over the past several years there has been a noticeable rise in the number of reported targeted attacks, which are also commonly referred to as advanced persistent threats (APTs). This is seen by security experts as a landscape shift from a world dominated by widespread malware that infect indiscriminately, to a more selectively targeted approach with higher gain. One thing that is clear about targeted attacks is that they are difficult to detect, and not much research has been conducted so far in detecting these attacks. In this paper, we propose a novel system called SPuNge that processes threat information collected on the users' side to detect potential targeted attacks for further investigation. We use a combination of clustering and correlation techniques to identify groups of machines that share a similar behavior with respect to the malicious resources they access and the industry in which they operate (e.g., oil & gas). We evaluated our system against real data collected by an antivirus vendor from over 20 million customers installations worldwide. Our results show that our approach works well in practice and is helpful in assisting security analysts in cybercrime investigations.
Phishing attacks are a type of cybercrime that has grown in recent years. It is part of social engineering attacks where an attacker deceives users by sending fake messages using social media platforms or emails. Phishing attacks steal users' information or download and install malicious software. They are hard to detect because attackers can design a phishing message that looks legitimate to a user. This message may contain a phishing URL so that even an expert can be a victim. This URL leads the victim to a fake website that steals information, such as login information, payment information, etc. Researchers and engineers work to develop methods to detect phishing attacks without the need for the eyes of experts. Even though many papers discuss HTML and URL-based phishing detection methods, there is no comprehensive survey to discuss these methods. Therefore, this paper comprehensively surveys HTML and URL phishing attacks and detection methods. We review the current state-of-art deep learning models to detect URL-based and hybrid-based phishing attacks in detail. We compare each model based on its data preprocessing, feature extraction, model design, and performance.
Recently cognitive radio technology gets attention to enhance the performance of smart grid communication networks. In this paper, we present a cognitive radio enabled smart grid architecture. We then discuss major cyber security challenges in smart grid deployment and additional challenges introduced by cognitive radio technology. Spectrum sensing is one of the important aspect for opportunistic spectrum access in cognitive radio enabled smart grid networks. Cooperative spectrum sensing can improve the sensing performance in which multiple cognitive radio users cooperate to sense primary user bands. However, cooperative spectrum sensing is vulnerable to incumbent emulation and spectrum sensing data falsification (SSDF) attacks. Thus, we propose a two-stage scheme for defense against SSDF attacks. Simulation results show that the proposed two-stage scheme can identify and exclude the attackers accurately.
Government agencies and standard setters require organizations operating in critical infrastructure sectors to disclose cybersecurity incidents, yet little is known about whether organizations report these incidents to law enforcement. This study examines this issue based on data from the 2017-2021 periods of the Canadian Survey of Cybersecurity and Cybercrime administered to Canadian organizations. We assessed the effects of governance determinants along with cyber incidents and their impacts using partial least squares equation modelling to identify the relationships between these factors and cybersecurity incidents reported to police services. To conceptualize these relationships, we developed a framework based on resource-dependence theory, protection motivation theory, and previous empirical evidence. The overall governance determinants as well as the impacts of the incidents explained 51% of the intention to report cybersecurity incidents to police, and the intensity of the impacts explained 30% of these intentions to signal incidents to law enforcement. The results also revealed that the intensity of cyber incident impacts dictates the attitudes of organizations towards reporting digital attacks. This study makes a significant theoretical contribution to the information security literature and has practical implications for standard setters and government agencies that aim to combat cybersecurity incidents.
Critical infrastructure in South Africa remains highly vulnerable to cybercrime threats due to a poor cyber-crime fighting capacity and a lack of a strong cybersecurity policy. South Africa appears to have fallen behind in securing and protecting cyberspace, considering the country's dependability as well as the interconnectedness to the internet. Globally, the water and wastewater sector were ranked number four in the global security incidents. This study presents the findings of a systematic literature review conducted to assess the cybersecurity knowledge necessary for a general employee in the water sector. The study proposes a framework for determining the minimum knowledge that a general employee in the water sector should have. The frameworks start by defining the eight different types of cybersecurity challenges, then move on to mitigation strategies for dealing with such attacks. Several approaches and strategies were provided for mitigating various cybersecurity challenges. To deal with such risks, mitigations such as cybersecurity knowledge and skills, cybersecurity awareness, and cybersecurity training were proposed. The strategies for developing knowledge to deal with various sorts of dangers were provided at both the individual and organizational levels.
Cybercrime has become a big money business with sensitive data being a hot commodity on the dark web. In this paper, we introduce and evaluate a filesystem (DcyFS) capable of curtailing data theft and ensuring file integrity protection by providing subject-specific views of the filesystem. The deceptive filesystem transparently creates multiple levels of stacking to protect the base filesystem and monitor file accesses, hide and redact sensitive files with baits, and inject decoys onto fake system views purveyed to untrusted subjects, all while maintaining a pristine state to legitimate processes. A novel security domain model groups applications into filesystem views and eliminates the need for filesystem merging. Our prototype implementation leverages a kernel hot-patch to seamlessly integrate the new filesystem module into live and existing environments. We demonstrate the utility of our approach through extensive performance benchmarks and use cases on real malware samples, including ransomware, rootkits, binary modifiers, back-doors, and library injectors. Our results show that DcyFS adds no significant performance overhead to the filesystem, preserves the filesystem data, and offers a potent new tool to characterize the impact of malicious activities and expedite forensic investigations.
Email phishing is a serious and potentially catastrophic threat to organisations and individuals. Understanding what factors may influence individual susceptibility to phishing attacks is essential to protecting against cybercrime. We investigated the potential interplay between conscientiousness and cue utilisation in individuals' ability to accurately differentiate between phishing and legitimate emails. University students (N = 255) completed a phishing detection task, the Mini International Personality Item Pool, and the phishing edition of the Expert Intensive Skill Evaluation (2.0) battery. After, they were sent simulated phishing emails to their student email address. A Signal Detection Theory approach revealed that higher cue utilisation was associated with a greater ability to tell whether an e-mail was phishing or not in the detection task. For the simulated phishing emails, participants with lower conscientiousness were more likely to click an embedded link in an unsophisticated phishing email, however cue utilisation had no association with email engagement in a naturalistic setting. The findings provide insight into why some people are more susceptible to phishing scams and reveal important differences in phishing sensitivity as a function of context, which has implications to interventions.
Payload attribution systems (PAS) are one of the most important tools of network forensics for detecting an offender after the occurrence of a cybercrime. A PAS stores the network traffic history in order to detect the source and destination pair of a certain data stream in case a malicious activity occurs on the network. The huge volume of information that is daily transferred in the network means that the data stored by a PAS must be as compact and concise as possible. Moreover, the investigation of this large volume of data for a malicious data stream must be handled within a reasonable time. For this purpose, several techniques based on storing a digest of traffic using Bloom filters have been proposed in the literature. The false positive rate of existing techniques for detecting cybercriminals is unacceptably high, i.e., many source and destination pairs are falsely determined as malicious, making it difficult to detect the true criminal. In order to ameliorate this problem, we have proposed a solution based on compressed bitmap index tables and traffic downsampling. Our analytical evaluation and experimental results show that the proposed method significantly reduces the false positive rate.
Cyberspace operates on a geographically borderless platform, thus often rendering national laws ineffective in regulating the impact of cyber-related activities outside South African borders. Recognising this issue, South Africa adopted the Cybercrimes Act, which permits the exercise of extra-territorial jurisdiction over trans-national cyber-related offences. The enforcement and effectiveness of extra-territorial jurisdiction and extradition law have, however, proven to be challenging and controversial in the international sphere. Issues such as internet fragmentation, contrasting municipal laws, and uncoordinated regulatory actions across state boundaries have undermined existing provisions regulating trans-national cybercrimes. These issues are furthered by the increased recognition of human rights, such as the right to privacy, which has deterred international cooperation and collaboration as states are subsequently required to subject their own citizens and entities to increased interception and scrutiny. The main thesis of this investigation is aimed at reviewing the practical implications surrounding the enforcement of extra-territorial jurisdiction and extradition law over trans-national cybercrimes. To this end, states are implored to develop both domestic and multilateral cybercrime laws and to improve existing enforcement mechanisms outlined in extradition law and mutual assistance agreements.
Cyberbullying is a critical issue in society worldwide, however in Ecuador is not given necessary importance to mitigate this cybercrime. It has been proposed to develop an exhaustive analysis of the laws that are currently considered to sanction cyberbullying when denouncing this fact. The deductive method and exploratory research were employed to make the analysis of the information consulted from the various sources that are obtained on the network about the topic discussed. The investigation revealed how cyberbullying cases arise, from which it is obtained that only 0.07% have been reported, with this result it can be deduced that the number of reported cases is very low in relation to the total number of cellphones activated in Ecuador and people who may be victims of this cyber-crime. In addition, a Criminal Process Diagram was obtained that determines the sequence of how the judicial process work. The applied method resulted the following: Can be created a law project that battles each type of derived cyberbullying. It was concluded that several institutions in Ecuador work together with organizations to prevent cyberbullying, however, when this happens, the laws are not enough to punish the act.
"The high utilization rate of mobile devices highlights the problem of vulnerability. As a result, new cybercrime techniques are created, in response to which new forensic techniques must be created, so we can deduct the importance of this paper. For some years now, there has been a significant growth in the use of mobile devices in daily life, since they allow to carry personal data in a practical, easy and comfortable way. These data are, in many cases, the target of malicious people, who, taking advantage of the vulnerabilities that these devices present, are capable of illegal actions, usually for unlawful purposes. The current research proposes, using a comparative method to allow us to formulate a forensic analysis to mobile devices with Android operating system; based on the chain of custody guidelines, compliance stages, and phases and to detect findings, nonconformities, locate vulnerabilities. Based on this process we can determine the origin of the leading causes of different types of events or crimes committed from a mobile device. Additionally, using a decision matrix, the best software for performing the forensic analysis is chosen and using Balanced Scorecard, indicators are evaluated."
Real-time prediction of domain names that are generated using the Domain Generation Algorithms (DGAs) is a challenging cyber security task. Scope to collect the vast amount of data for training favored data-driven techniques and deep learning architectures have the potential to address this challenge. This paper proposes a deep learning framework using long short-term memory (LSTM) architecture for prediction of the domain names that are generated using the DGAs. Binary classification had benign and DGA domain names and multiclass classification was performed using 20 different DGAs. For the binary classification, LSTM model gave accuracy of 98.7% and 71.3% on two different test data sets and for the multi-class classification, it gave accuracy of 68.3% and 67.0% respectively. Two diversified data sets were used to analyze the robustness of the LSTM architecture.
With the increasing usage of information technology on the criminal side, the digital forensic analysis, especially multimedia forensics, becomes an emerging technique for cybercrime investigators to improve examination efficiency. The study focuses on the digital triage problem for evidence location during the automatic forensic process. After defining the multi-scale knowledge base for storing digital forensic investigators' prior knowledge, a variable scale case-based reasoning method (VSCBR) is proposed to support investigators predicting evidential areas. The variable-scale clustering algorithm based on the scale transformation strategy (VSC-STS) is also put forward, which could identify highly similar past cases containing candidate evidence in the case reuse and revise phase. A case study is established using a real 15.9 GB bidding case dataset, which contains both text bidding documents and image technical drawings. Numerical experimental results show that the validation of the proposed VSC-STS is significantly improved compared with the traditional single-scale clustering algorithm, and it is insensitive to the initial parameter threshold. Moreover, the proposed method VSCBR is able to help investigators locate suspicious rule-violating evidences in practice. (C) 2021 Elsevier B.V. All rights reserved.
"For nearly a decade, security researchers have highlighted the grave risk presented by Internet-connected Industrial Control Systems (ICS). Predictions of targeted and indiscriminate attacks have yet to materialise despite continued growth of a vulnerable population of devices. We investigate the missing attacks against ICS, focusing on large-scale attacks enabled by Internet-connected populations. We fingerprint and track more than 10 000 devices over four years to confirm that the population is growing, continuously-connected, and unpatched. We also track 150 000 botnet hosts, monitor 120 global ICS honeypots, and sift 70 million underground forum posts to show that the cybercrime community has little competence or interest in the ICS domain. Attackers may be dissuaded by the high cost of entry, the fragmented ICS population, and limited onboard resources; however, this justification is incomplete. We use a series of case studies to develop a security economics model for large-scale attacks against Internet-connected populations in general, and use it to explain both the current lack of interest in ICS and the features of Industry 4.0 that will make the domain more accessible and attractive to attackers."
Cyber-safety behaviors are important in preventing the loss of an individual's digital assets and ensuring the safety of important daily online activities. Individuals' cyber-safety is also critical for national cybersecurity. The issue is highly relevant for Israel, a country that relies on the digital capabilities of its workers for its major technology industries and is also often a target of cyberwarfare and cybercrime attacks. The purpose of this study is to identify the determinants of cyber-safety behavior. We investigate the role of age, gender and education in the use of safety-related digital skills and antivirus software. Using a 2014 survey of a national sample of Internet users in Israel (N=1850), we found that age, gender, education and quality of access are associated with the level of users' digital security skills. In addition, these skills and the frequency of conducting financial activities online are the main determinants of antivirus behaviors. Our results expand the understanding of cyber-safety by showing that social and digital disparities are reproduced in the use of measures to prevent online threats, putting the digitally disadvantaged at greater risk of becoming victims of online threats.
Phishing is an illegal cybercrime, wherein a target gets victimized for sacrificing their personal and corporate information. It is one of the most straightforward forms of cyber-attack for hackers, as well as one of the simplest for victims to fall for. It can also provide hackers with the required information that are needed to access their targets' personal and corporate accounts. For the past decade, machine-learning techniques have become consistent standards for classifying phishing and legitimate URLs. But deep learning algorithms have the advantage of automatic extraction of complex features and characterization of handling massive data. Considering the above-listed advantages, this work provides state-of-the-art accuracy in the detection of malicious URLs using recurrent neural networks (RNN). Unlike previous studies, which looked at online content, URLs, and traffic numbers, this work aims to focus only on the text in the URL which makes it quicker, and thereby zero-day assaults could be caught at the earliest. The RNN has been optimized so that it might be utilized on tiny devices like Mobiles, and Raspberry Pi without sacrificing the inference time.
The COVID19 pandemic situation has opened a wide range of opportunities for cyber-criminals, who take advantage of the anxiety generated and the time spent on the Internet, to undertake massive phishing campaigns. Although companies are adopting protective measures, the psychological traits of the victims are still considered from a very generic perspective. In particular, current literature determines that the model proposed in the Big-Five personality traits (i.e., Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism) might play an important role in human behaviour to counter cybercrime. However, results do not provide unanimity regarding the correlation between phishing susceptibility and neuroticism. With the aim to understand this lack of consensus, this article provides a comprehensive literature review of papers extracted from relevant databases (IEEE Xplore, Scopus, ACM Digital Library, and Web of Science). Our results show that there is not a well-established psychological theory explaining the role of neuroticism in the phishing context. We sustain that non-representative samples and the lack of homogeneity amongst the studies might be the culprits behind this lack of consensus on the role of neuroticism on phishing susceptibility.
Visualizing digital evidence in an easy and constructive manner is a major problem because of the advanced techniques for hiding, wiping, encrypting and deleting digital data developed during the last few years. To tackle this problem, a system for visualizing digital data in 3-Dimensional (3D) mode has been developed. XML was used as a common language to allow fine-grained management of digital data with flexibility and ease. The extensibility of the implementation makes it particularly suitable as a research and development platform in the sector of open source computer forensics tools for the future. This article examines real-life problems that benefit from using this tool in a congenial and constructive manner to validate its key underlining concept. The design decisions that have been taken in producing the system architecture, and the features it supports are elaborated upon. To determine the effectiveness of the tool, an actual case study is presented which examines the results of the tool and why it is necessary to go for an open source model as a standard. The paper concludes with performance measurements of the tool and suggests possible extensions to make the tool even smarter. (C) 2007 Published by Elsevier B.V.
Steganography is a promising technique for covert communications. However, illegal usage of this technique would facilitate cybercrime activities and thereby pose a great threat to information security. Therefore, it is crucial to study its countermeasure, namely, steganalysis. In this paper, we aim to present an efficient steganalysis method for detecting adaptive-codebook based steganography in adaptive multi-rate (AMR) speech streams. To achieve this goal, we first design a new low-dimensional feature set for steganalysis, including an improved calibrated Markov transition probability matrix for the second-order difference of pitch delay values (IC-MSDPD) and the probability distribution of the odevity for pitch delay values (PDOEPD). The dimension of the proposed feature set is 14, far smaller than the feature set in the state-of-the-art steganalysis method. Employing the new feature set, we further present a steganalysis scheme for AMR speech based on support vector machines. The presented scheme is evaluated with a large number of AMR-encoded speech samples, and compared with the state-of-the-art one. The experimental results show that the proposed method is effective, and outperforms the state-of-the-art one in both detection accuracy and computational overhead.
"Authorship Attribution, (AA) is a process of determining a particular document is written by which author among a list of suspected authors. Authorship attribution has been the problem from last six decades; when there were handwritten documents needed to be identified for the genuine author. Due to the technology advancement and increase in cybercrime and unlawful activities, this problem of AA becomes forth most important to trace out the author behind online messages. Over the past, many years research has been conducted to attribute the authorship of an author on the basis of their writing style as all authors possess different distinctiveness while writing a piece of document. This paper presents a comparative study of various machine learning approaches on different feature sets for authorship attribution on short text. The Twitter dataset has been used for comparison with varying sample size of a dataset of 10 prolific authors with various combinations of feature sets. The significance and impact of combinations of features while inferring different stylometric features has been reflected. The results of different approaches are compared based on their accuracy and precision values."
Web applications are one of the mostly attacked platforms today, and because of that new ways to break into the web applications are being invented almost on a daily basis, allowing attackers to steal user's personal data, credit card numbers, and conduct many other frauds related to data and applications hosted on the Internet servers and databases. Some of the reasons that web applications are constantly attacked is 24/7 availability, mix of technologies used to provide needed functionality, interesting data in the backend databases and easy way to avoid punishment for crimes committed against web sites and website users/owners. There is also an aspect related to cybercrime and cyber warfare that is marching throughout the planet in the last few years, exposing more and more personal data in highly sophisticated and targeted attacks. This paper will try to summarize few different ways that web application could be written in order to identify, isolate and track the hacker during the attack process. The concept presented in this paper is so called honeytoken - a value the application is using in databases, files, parameters, etc., which should never be changed or touched by the application in normal application lifecycle.
Today, phishing attacks represent one of the biggest security threats targeting users of the digital world. They consist of an attempt to steal sensitive information, such as a user?s identity or credit and debit card details, using various methods that include fake emails, fake websites, and fake social media messages. Protecting the user?s security and privacy therefore becomes complex, especially when those users are children. Currently, children are participating in Internet activity more frequently than ever before. This activity includes, for example, online gaming, communication, and schoolwork. However, children tend to have a less well-developed knowledge of privacy and security concepts, compared to adults. Consequently, they often become victims of cybercrime. In this paper, the effects of security awareness on users who are children are investigated, looking at their ability to detect phishing attacks in social media. In this approach, two Experiments were conducted to evaluate the effects of security awareness on WhatsApp application users in their daily communication. The results of the Experiments revealed that phishing awareness training has a significant positive effect on the ability of children using WhatsApp to identify phishing messages and thereby avoid attacks.
Network attacks have been around since the beginning of the Internet and they are still relevant due to the numerous attempts of independent hackers, cybercrime organizations and state-sponsored hacking squads to intrude into others computer networks. Similarly, the industry and academia have been providing constantly evolving solutions to try to thwart the ever-sophisticated network attack techniques. Machine learning (ML) and deep learning have shown promising results for detecting and preventing network attacks. In this paper, we investigate a Long Short-Term Memory (LSTM)-based network attack detection. Our goal is to find the optimal values for network attack detection by fine-tuning different LSTM hyper-parameters: optimizers, loss functions, learning rates and activation functions, and by comparing their performance by using the CICIDS2017 labeled dataset. Our results show that LSTM can effectively detect the network attacks with high accuracy and reasonable detection rates. With the optimized hyper-parameter values, the proposed LSTM model can detect DoS attacks with an effective detection accuracy of 99.08% and a detection rate of 0.93. We achieved BoT, DDoS, and port scan attacks detection accuracy of 99.54% and detection rate of 0.84.
In today's online forums and marketplaces cybercrime activity can often be found lurking in plain sight behind legitimate posts. Most popular criminology techniques are either manually intensive, and so do not scale well, or focus on statistical summaries across websites and can miss infrequent behaviour patterns. We present an inter-disciplinary (computer science, criminology and conservation science) socio-technical artificial intelligence (AI) approach to information extraction from the long tail of online forums around internet-facilitated illegal trades of endangered species. Our methodology is highly iterative, taking entities of interest (e.g. endangered plant species, suspects, locations) identified by a criminologist and using them to direct computer science tools including crawling, searching and information extraction over many steps until an acceptable resulting intelligence package is achieved. We evaluate our approach using two case study experiments, each based on a one-week duration criminology investigation (aided by conservation science experts) and evaluate both named entity (NE) directed graph visualization and Latent Dirichlet Allocation (LDA) topic modelling. NE directed graph visualization consistently outperforms topic modelling for discovering connected entities in the long tail of online forums and marketplaces.
Online notepad services allow users to upload and share free text anonymously. Reviewing Pastebin, one of the most popular online notepad services websites, it is possible to find textual content that could be related to illegal activities, such as leaks of personal information or hyperlinks to multimedia files containing child sexual abuse images or videos. An automatic approach to monitor and to detect these activities in such an active and a dynamic environment could be useful for Law Enforcement Agencies to fight against cybercrime. In this work, we present Pastes Content Classification 17K (PasteCC 17K), a dataset of 17640 textual samples crawled from Pastebin, which are classified in 15 categories, being 6 of them suspicious to be related to illegal ones. We used PasteCC 17K to evaluated two well-known text representation techniques, ensembled with three different supervised approaches to classify the pastes of the Pastebin website. We found that the best performance is achieved ensembling TF-IDF encoding with Logistic Regression obtaining an accuracy of 98.63%. The proposed model could assist the authorities in the detection of suspicious content shared in Pastebin.
Given the increasing financial impact of cybercrime, it has become critical for companies to manage information security risk. The practitioner literature has long argued that the internal audit function (IAF) can play an important role both in providing assurance with respect to information security and in generating insights about how to improve the organization's information security. Nevertheless, there is scant empirical evidence to support this belief. Using a unique data set, this study examines how the quality of the relationship between the internal audit and the information security functions affects objective measures of the overall effectiveness of an organization's information security efforts. The quality of this relationship has a positive effect on the number of reported internal control weaknesses and incidents of noncompliance, as well as on the numbers of security incidents detected, both before and after they caused material harm to the organization. In addition, we find that higher levels of management support for information security and having the chief information security officer (CISO) report independently of the IT function have a positive effect on the quality of the relationship between the internal audit and information security functions. (C) 2018 Elsevier Ltd. All rights reserved.
Botnets, although technically based on long lasting well established attacking models, currently represent an increasing threat, moving huge amounts of capitals from legal system to criminals. This is mainly due to its adaptability, based on Crime-as-a-Service model, where different, transnational, actors are located in the different rings of the crime supply chain. Moreover, botnet success has been enabled by two main factors: the weak countermeasures adoption, reinforced by the well-known dominance of software attacker versus defender and the revenue model, which considers the target of the attack out of the victim (ICT users) control. Finally, the losses are typically in charge of silent financial/insurance organizations. These botnet pillars are available for renting at low-cost by criminal organizations, exploiting the dark side of the success factor of the Internet business players, the network externality, where targets, e.g. Internet two sided markets, can be easily predicted but not yet adequately protected. In this paper, the authors will describe, by Zeus and other botnet examples, the revenue model and its related costs as cybercrime, focusing on the concerning evolution of this threat and proposing some strategies to cope with it.
"With the functioning of the global Internet, the geopolitical struggle between the states has intensified significantly in the information sphere. Transformations of the security space in modern conditions are leading to extraordinary events in cyberspace in Europe and other countries, which are becoming more frequent and large-scale. This situation requires intensification of international cooperation in the field of information space protection. A significant part of the risks in the information sphere arises due to the lag of legal regulation from scientific and technological progress. This has led to problems of protection of personal data of citizens and ensuring the sustainable operation of information and telecommunications systems of critical infrastructure. One of the main ways to overcome the lag is timely and proper regulation of these processes. Effective international cooperation to protect the information space will be facilitated by: improving coordination of actions and cooperation within international organizations in order to strengthen cyber resilience; purposeful fight against cybercrime in Ukraine and the world; development of cybersecurity dialogue at the national and international levels; close public-private partnership in the institutional provision of information and cybersecurity management."
The objective of this research was to identify aspects of warnings that will reduce online disclosure of personal information, specifically aspects that people do not consider sensitive. The vast majority of online commercial websites collect personal data, and many consumers report cybercrime events, such that identity theft is a significant risk. Most people can be uniquely identified by the combination of their birth date, age, and gender. This is problematic, since people do not believe these identity elements to be important to safeguard. Participants were asked to provide personal information to receive an online insurance quote. Participants in experimental conditions were warned not to disclose their date of birth. The experimental manipulations were (a) the vividness of the warnings and (b) whether an alternative to disclosure was recommended. Analyses indicated that providing an alternative to disclosure proved to be an effective strategy to reduce disclosure. Personal information disclosure can be reduced through warnings, but warnings need to be carefully designed and tested. Clear recommendations for alternative behavior may be especially effective. Designers of warnings and alerts can use the principles identified in this research to make their messages more effective.
Malicious domain names are useful for cybercrime, but can be easily blocked by blacklists. To avoid a single point of failure, cybercriminals use domain generation algorithm to generate a large number of malicious domains. Once the victim's machine is infected with malware, the malware tends to connect to malicious domain names to commit cybercrimes, such as waiting for remote control commands or sending malware feedback. Therefore, how to detect these malicious connections has been a hot research topic in information security. In this paper, a new method of tracking malicious domain and victim machine by scalability system named CC-Tracker (Cyber Criminal Tracker) based on HTTP is presented. CC-Tracker extracts 12 features from HTTP traffic using MapReduce framework based Interaction Profiling Bipartite Graph mining. Experimental results show that CC-Tracker can reach 99% AUC in the evaluation benchmark. In addition in the deployment environment found new malicious domain of network traffic, and dig out the hidden in the enterprise, the victims of the machine these malicious domain are a threat to other online reputation system can't identify. The scalability and applicability of CC-Tracker are demonstrated by experiments on the real-world environment.
"As individuals increasingly engage with the digital landscape, they face a multitude of risks associated with their online activities and the security of their personal information. Individuals seek guidance in balancing the benefits and risks of the digital transformation. To effectively mitigate these risks, it is essential to establish a comprehensive Digital Risk Assessment Framework tailored to individual users. In this research, an a interpretive study have been carried out to propose a novel Digital Security Management Framework. The main contribution of this study is providing a novel approach by examining the recent recorded threats against individuals, quantifying these threats, and proposing a novel digital risk framework detailing the list of threats and the corresponding risk treatment options tailored for individuals. The scenario of the case study is a family that use personal computers to access banking and investment accounts online, engage in online shopping and also frequently use social media to share artwork and opinions. 17 types of digital risks were identified and the probability of loss and impact of each risk have been quantified using Bernoulli distribution f(L;p). The quantified values were used to prioritise mitigation measures. According to the results, and the proposed framework, suitable treatment option(s) was recommended for each risk. The results show that online scams present the biggest financial risk to individuals, that security incidents present a moderate risk, and that communication-based harms (e.g. bullying and radicalization) are difficult to quantify."
The rapid growth and usefulness of Internet of Things (IoT) has seen it being deployed in critical and strategic infrastructure sectors like healthcare, transport, agriculture, home automation, and smart industries among many others. The benefits of comfort and reliability of IoT technologies to human beings have brought with them security concerns. This is due to its large-scale connectivity and over reliance on the internet for communication making it susceptible to cyberattacks. Digital forensics experts face a daunting task of handling these cyberattacks because of the unique and complex challenges posed by IoT. Recently, researchers have been drawn to finding solutions to these challenges, however, this is still in its infancy. This paper carries out a Systematic Literature Review (SLR) of the current research advancements in IoT forensics. We define key IoT fundamentals, IoT applications, the need for IoT forensics, identify the key factors affecting IoT forensics, and review the practicality of the available IoT forensics frameworks, models, and methodologies. The SLR reveals research gaps indicating that most of the current research is more theoretical than practical. There is a need for more practical approaches to tackle the unique IoT forensics challenges. Finally, for future research directions from the SLR, we have highlighted and discussed the open challenges and requirements for IoT forensics. (c) 2021 Elsevier Ltd. All rights reserved.
As we now see digital evidence play a role in many investigative scenarios, it is imperative that those seeking to rely upon it as part of criminal justice processes can do so, absent any concern regarding its validity. Interpreting the meaning of digital data and its potential value to a criminal inquiry as part of a digital forensic examination is a complex and multifaceted process requiring the practitioner to possess the relevant knowledge, experience, and insight needed to determine the case-significance of a given data trace accurately. Erroneously interpreted data that is communicated to a client and subsequently relied upon can have far-reaching consequences for all those involved in the investigative process. This work discusses the process of forming investigative opinions in digital forensic science examinations, what this means in practice, and the ways in which it can be achieved. Focus will be given to the process of forming an investigative opinion when underpinned through the reconstruction and testing of a suspect system/setup, with a formal three-stage methodology for doing this outlined. This article is categorized under: Digital and Multimedia Science > Cybercrime Investigation
Cyberbullying has been defined as the process of using the internet, cell phones or other devices to send or post text or images intended to hurt or embarrass another person. The word cyberbullying is often used interchangeably with cyber stalking and in fact the Cybercrimes (Prohibition, Prevention, etc.) Act 2015 of Nigeria, uses the word cyber stalking which it defines as any course of conduct directed at a specific person that would cause a reasonable person to feel fear. By the provisions of the Act, the transmission of any communication through the means of a computer to bully, threaten or harass another person where such communication places another person in fear of death, violence or bodily harm amounts to cyber stalking. Cyberbullying is becoming a common phenomenon in Nigeria as more people engage in it especially on social media platforms. This is carried out in various ways and a common trend is posting indecent imagery of persons online such as naked pictures or videos of persons in order to humiliate them. When posted by a person, the communication is shared by others thereby causing circulation on social media. This act amounts to cyber stalking where the intention consists of those elements stated under the Cybercrimes Act. In other situations where it is shared without the aim of humiliating the victim, such act can still be incriminated under some other laws in Nigeria such as the Criminal Code Act and the Penal Code Act which for instance both criminalise obscene publications. It is worthy of note there have been reported cases where victims of cyberbullying have committed suicide as a result of fear or shame. A major observation is that cyberbullying has gained normalcy and many internet users engaged in it do not seem to be aware of the criminal connotation of their actions. This paper examines the effectiveness of legal responses to cyberbullying in Nigeria. It discusses the forms of cyberbullying commonly perpetrated in Nigeria by citing some real life instances that have happened in the past. The paper notes that most forms of cyberbullying can be prosecuted under the Cybercrimes Act, however, there has not been any notable enforcement of the law in terms of prosecution of cyberbullying cases. It appears that the lack of prosecution of offenders has fostered the act of cyberbullying especially under the present circumstances where there is widespread ignorance among internet users. The paper also notes that the absence of image rights is a precursor in many respects to some forms of cyberbullying especially when photographs and videos of victims are involved. The paper advocates the implementation and enforcement of the Cybercrimes Act as well as other laws relating to cyberbullying in Nigeria. The paper also posits that the protection of image rights will go a long way to assist in curbing the act of cyberbullying in Nigeria.
Child Pornography is an increasingly visible rising cybercrime in the world today. Over the past decade, with rapid growth in smart phone usage, readily available free Cloud Computing storage, and various mobile communication apps, child pornographers have found a convenient and reliable mobile platform for instantly sharing pictures or videos of children being sexually abused. Within this new paradigm, law enforcement officers are finding that detecting, gathering, and processing evidence for the prosecution of child pornographers is becoming increasingly challenging. Deep learning is a machine learning method that models high-level abstractions in data and extracts hierarchical representations of data by using a deep graph with multiple processing layers. This paper presents a conceptual model of deep learning approach for detecting child pornography within the new paradigm by using log analysis, file name analysis and cell site analysis which investigate text logs of events that have happened in the smart phone at the scene of the crime using physical and logical acquisition to assists law enforcement officers in gathering and processing child pornography evidence for prosecution. In addition, this paper shows an illustrative example of logical and physical acquisition on smart phones using forensics tools.
The vulnerabilities in a software can be exploited and can result in damaging important resources. To prevent such exploitations, anti-virus companies introduces new virus signatures in their databases. The approach requires network system to be analyzed constantly to discover harmful traffic. The harmful traffic can consist of single or multiple threats that can use vulnerability in one or another way. To discover vulnerability at earlier stage, is important because it can give time to remove the weaknesses from the system thus closing the way for intruders to exploit it. In this paper, we propose a Honeynet Vulnerabilities Detector system (HVD) to detect unknown vulnerabilities in software. To mitigate the risk of detection of HVD (fingerprint identification), we provide the attackers unlimited outbound connections. The increased number of connections can give rise to security threat for protection networks (non-Honeynet systems) and to prevent such threats, we propose a method to protect such systems The HVD will be able to detect the unknown vulnerabilities in software. The detection of unknown vulnerabilities will result in providing opportunities to software developers to fix/remove them.
Purpose: This paper explores the concept of criminal expertise within the context of online pedophile community and applies rational choice theory to understand the decision-making processes of offenders. Specifically, this study aims to explore the cybersecurity concerns of dark web pedophile forum users known to be a hard-to-reach offender population.Methods: Sequential generalized linear model and Cox proportional hazard regression model were used to examine cybersecurity themes predicting both the lifespan and survival of 290 cybersecurity-related threads extracted from three pedophile forums identified on the dark web.Results: Results showed that that risk factors for law enforcement identification-related topics were the most predictive of threads' lifespan and survival. Moreover, topics related to proactive protection strategies were more predictive than those related to reactive ones. Finally, threads with a superior skill level were more likely to survive than other types of content.Conclusion: This study builds upon both criminal expertise and cybercrime literature, particularly on the appli-cability of the criminal expertise framework to dark web pedophile forum users and provide a better under-standing of the enculturation process among pedophile community.
Today mobile phones provide a wide range of applications that make our daily life easy. With popularity, smartphones have become a target for cybercrime where malicious apps are developed to acquire sensitive information or corrupt data. To mitigate this issue and to improve the security in mobile devices, different techniques have been used. These techniques can be broadly classified as static, dynamic and hybrid approaches. In this paper, a static-based model MalDuoNet is proposed to detect Android malwares, which uses a DualNet framework to analyze the features from the API calls. In the MalDuoNet model, one sub-network is focused to learn the features relevant to malicious behavior and the other sub-network is focused to learn the features in general. Thus it enables the model to learn complementary features which in turn helps get richer features for analysis. Then the features from the two sub-networks are combined in the final fused classifier for the final classification. In addition, each of the feature extractors has a separate classifier so that each sub-network can optimize its performance separately. The experimental results demonstrate that the MalDuoNet model outperforms the two baseline models with single network.
New information technologies are beneficial for ship operations in terms of safety and utilisation of company resources. However, new cybercrime threats have emergedaffecting, both ship safety and security that need to be assessed and evaluated. At the moment, actions of the maritime industry to keep pace dealing with such threats are slow when compared with other business sectors. As a high concern, maritime pirates could take advantage of cybersecurity breaches to monitor ship activity and gain information for potential protective failures. In 2021 companies and seafarers should be able to demonstrate knowledge and safeguard policies of their companies. Nevertheless, there is limited discussion on how companies will educate seafarers for existing threats. Therefore, in this study, a risk-based methodology is proposed for evaluation of cybersecurity threats in the context of a piracy attack. STPA-SafeSec's analysis is used to identify security threats, and FAHP is utilised for evaluating the severity of each security constraint. Audits on 15 ships with 315 seafarers indicated that there are significant security gaps mainly due to lack of awareness from operators and seafarers. However, physical security and network protection that already apply to ships are significant security strengths.
In recent years, many human activities have made cyberspace their preferred environment. This study focuses on the betting environment, specifically on fixed-match informing websites (FMIWs). These sites claim to be capable of selling tips about fixed sports events. They essentially act as vendors of confidential sources, allowing punters to place 100% sure bets. We hypothesize that cyber places for match-fixing tips facilitate deviant behaviors. Through systematic observation, we describe and quantify a set of 15 environmental features they share, which do not always belong to regulated online betting platforms. Findings from 78 FMIWs corroborate our hypothesis, as they support the relevance of Environmental Criminology theories applied to cybercrime. Additional exploration through hyperlink network analysis shows that FMIWs are highly homogeneous and have similar characteristics to the Tor network but differ from other illicit online environments such as sexual child exploitation networks or white supremacist communities. The characteristics of the network suggest that the business is more similar to a fraud scheme than an illicit market. Finally, the practical implications of the results for crime prevention and the directions for future research are outlined.
Performing cyber maneuvers in an operational environment is not easy. We need a cyber-situational awareness framework to perform its maneuvers to protect the cyberspace and to cope with its attacks. The battlefield provided has essential information for detecting cybercrime events. The present study resolved the challenges of implementing these maneuvers through dynamic simulation of the cyber battlefield. The cyber battlefield contains detailed information on cyberspace elements, including the vulnerability knowledge repository, the tangible and intangible components of the cyberspace allowing maneuvering, penetration testing, injection attacks, tracking attacks, visualization, evaluation of the impact of cyberattacks, and risk evaluation. By injecting attacks and using the proposed algorithms, an impact assessment of each attack step on each of the elements of the environment has been done to identify potential threats. Using the proposed algorithms, an impact assessment has been performed on each of the environmental elements in order to identify potential threats. A dynamic updating simulator engine has been designed to update the vulnerability knowledge base automatically and change the topology and features of elements, accesses, services, hosts, and users. Modeling and simulation were evaluated using a qualitative research method and creating a focus group.
The volume and frequency of new cyber attacks have exploded in recent years. Such events have very complicated workflows and involve multiple criminal actors and organizations. However, current practices for threat analysis and intelligence discovery are still performed piecemeal in an ad-hoc manner. For example, a modern malware analysis system can dissect a piece of malicious code by itself. But, it cannot automatically identify the criminals who developed it or relate other cyber attack events with it. Consequently, it is imperative to automatically assemble the jigsaw puzzles of cybercrime events by performing threat intelligence fusion on data collected from heterogeneous sources, such as malware, underground social networks, cryptocurrency transaction records, etc. In this paper, we propose an Automated Threat Intelligence fuSion framework (ATIS) that is able to take all sorts of threat sources into account and discover new intelligence by connecting the dots of apparently isolated cyber events. To this end, ATIS consists of 5 planes, namely analysis, collection, controller, data and application planes. We discuss the design choices we made in the function of each plane and the interfaces between two adjacent planes. In addition, we develop two applications on top of ATIS to demonstrate its effectiveness.
This study examines how the COVID-19 emergency has impacted crime across different locations in Nigeria. Data were collected from a sample of residents from across Nigeria and analysed using mean ratings, percentages and chi-square. Based on the residents' perceptions, certain crime types have decreased (e.g. home break-ins and assaults), some remain unchanged (e.g. stealing and pilfering) and others have increased (e.g. cybercrime and domestic violence). The findings show concentrations of crime in urban centres, states on total lockdown and geographical areas with poor economic indicators. The times that most crimes are perpetrated remain unchanged, except for the night time, where there has been a significant increase. Generally, individual responses to crime remain unchanged, although the use of security guards and special security door locks has changed significantly. Conversely, neighbourhood-level responses have changed significantly, particularly with the use of vigilante groups, police and military patrols as well as restrictions of human and vehicular movement. Although some of the causes of this crime change existed before COVID-19, new crime opportunities are also acknowledged. The findings suggest that Nigerian cities may face a severe battle to recover from crime even after the COVID-19 emergency has passed. Policy and further research implications are discussed.
This paper investigates the economic perspective analysis of protecting security and privacy of big data. Traditionally, the pressing cyberthreats appear from emailed attachments. Recently, cyberattacks increasingly stealing or compromising data and are the potentials for physical damage to critical infrastructure. The risks of the data breach or compromised data collection are often favored by potential financial benefits (e.g., blackmail, fraud, false information, intellectual property thefts, business competition). That is, an important factor for current and future economical investments is due to the motivation of cybercrime activities. In this paper, we first analyze a question about our effort on security and privacy in terms of economic perspectives. That is, do we need to protect big data in a secure, private, and most effective manner, while the growing amount of security threats, attacks, and data breaches together with the increasing market for security products arises? Secondly, we perform the investigation from several perspectives: the economic perspective of big data security and privacy, investment decisions, fighting cybercrimes through big data, and cyberinsurance for big data. Our objective is to provide economic justification of technical decisions taken to protect the big data and the amount of costs that organizations often spend for it. (C) 2019 Elsevier B.V. All rights reserved.
One of the problems often associated with online anonymity is that it hinders social accountability, as substantiated by the high levels of cybercrime. Although identity cues are scarce in cyberspace, individuals often leave behind textual identity traces. In this study we proposed the use of stylometric analysis techniques to help identify individuals based on writing style. We incorporated a rich set of stylistic features, including lexical, syntactic, structural, content-specific, and idiosyncratic attributes. We also developed the Writeprints technique for identification and similarity detection of anonymous identities. Writeprints is a Karhunen-Loeve transforms-based technique that uses a sliding window and pattern disruption algorithm with individual author-level feature sets. The Writeprints,technique and extended feature set were evaluated on a testbed encompassing four online datasets spanning different domains: email, instant messaging, feedback comments, and program code. Writeprints outperformed benchmark techniques, including SVM, Ensemble SVM, PCA, and standard Karhunen-Loeve transforms, on the identification and similarity detection tasks with accuracy as high as 94% when differentiating between 100 authors. The extended feature set also significantly outperformed a baseline set of features commonly used in previous research. Furthermore, individual-author-level feature sets generally outperformed use of a single group of attributes.
Due to the widespread use of multimedia data over the internet, it has become important to protect the data from unauthorized access and make the information secure. Digital watermarking is the solution to prevent unauthorized access and to protect the copyright. In this paper, a naive watermarking scheme has been proposed. The authors for the first time have introduced Ranklets which is a new domain for watermarking along with discrete wavelet transform (DWT). To achieve better robustness and imperceptibility, the proposed scheme makes use of the Ranklets to select the mid -rank coefficients for watermark embedding. The wilxon statistics have been used for dispersion in Rank transform. Extensive experiments have been carried out to prove the claim of better robustness and imperceptibility.
"Revenge pornography is the online distribution of sexually uncensored images or videos of another person without consent and to cause embarrassment or torment. Victims of revenge pornography suffer significant harm, including losing jobs and, in extreme cases, committing suicide. The public blames the victim for the role they played. Rather than victim-blaming, victims deserve a takedown order and criminal liability for uploaders of such images. This study adopts a doctrinal approach; it examines key statutes and their interpretation by Nigerian courts while juxtaposing it with the law and practice in the United Kingdom. The United Kingdom was undertaken as a case study because it has a developed jurisprudence which can provide lessons for Nigeria. This study found that the current state of laws in Nigeria is ill-equipped to tackle the menace of revenge pornography. The objective of this study is to offer insights on the prevalence of revenge pornography in Nigeria and suggest legal solutions to address this phenomenon. It canvasses for a non-consensual pornography provision that would criminalise the act of revenge pornography in Nigeria. It also makes a case for cyber-censorship of contents by internet service providers and the need for third-party liability."
The methods and tools used by the European Union to counter hybrid threats are identified: from the fight against terrorism to measures aimed at combating economic competitors and political opponents (mainly, to squeeze Russia and China out of European markets). It is concluded that it is not by chance that neither EU institutions nor the research community have worked out a comprehensive definition of operations to combat hybrid threats. A broad understanding of hybrid threats as practically any (depending on the political situation) actions of the opponent serves to justify the application of any counteraction tool. In the fight against global threats such as terrorism, cybercrime, and the spread of false medical data, the EU takes a systemic approach, which makes it possible to assess the level and degree of the convergence of threats to critical infrastructure and the infosphere, as well as the possibilities of counteraction. At the same time, attempts to use economic, legislative, political, and informational tools to achieve one-sided economic, political, and military advantages do not reduce the degree of tension in the EU's relations with Russia, China, and some other countries, only increasing the number and strength of hybrid threats. This reduces the EU's ability to achieve strategic autonomy.
Since the 1980s, the digital revolution has been both a negative and positive force. Within a few weeks of the Covid-19 outbreak, lockdown accelerated the adoption of digital solutions at an unprecedented pace, creating unforeseen opportunities for scaling up alternative approaches to social and economic life. But it also brought digital risks and threats that placed new demands on policymakers. This article assembles evidence from different areas of social science expertise about the impacts of Covid-19 in digitised societies and policy responses. The authors show how the pandemic supported changes in data collection techniques and dissemination practices for official statistics, and how seemingly insuperable obstacles to the implementation of e-health treatments were largely overcome. They demonstrate how the ethics of artificial intelligence became a primary concern for government legislation at national and international levels, and how the features enabling smart cities to act as drivers of productivity did not necessarily give them an advantage during the pandemic. At the micro-level, families are shown to have become 'digital by default', as children were exposed to online risks and opportunities. Globally, the spread of the pandemic provided a fertile ground for cybercrime, while digital disinformation and influencing risked becoming normalised and domesticated.
There has been a lot of research in developing technologies and strategies to make Smart Grid a practical reality. Research has focussed on a wide range of domains like communication technologies and protocols, infrastructure reliability and availability, designing of intelligent applications and distribution automation. However, to make Smart Grid to be widely accepted and adopted by the community, security at all levels of the smart grid will have to be ensured. Although, existing security approaches could be applied to control cybercrime on the smart grid, the limited latency tolerance of communications within certain levels of the smart grid make it challenging. With an aim to address this problem, the paper makes three important contributions. First, it introduces a classification of devices used in the smart grid based on their computational capabilities. Second, it highlights the challenges faced in addressing the high performance computational requirements of cybersecurity in the smart grid. Third, it proposes a few practical solutions from research in advanced processor and accelerators architectures and parallel computing paradigms commonly used in the field of High Performance Computing in addressing them. Overall, a strong case is presented to revisit the standards that specify the minimum processor requirements of devices connected to the smart grid at all levels to increase cybersecurity on the smart grid.
In this paper we exploit market features proper of a leading Russian cybercrime market for user impersonation at scale to evaluate attacker preferences when purchasing stolen user profiles, and the overall economic activity of the market. We run our data collection over a period of 161 days and collect data on a sample of 1'193 sold user profiles out of 11'357 advertised products in that period and their characteristics. We estimate a market trade volume of up to approximately 700 profiles per day, corresponding to estimated daily sales of up to 4'000 USD and an overall market revenue within the observation period between 540k and 715k USD. We find profile provision to be rather stable over time and mainly focused on European profiles, whereas actual profile acquisition varies significantly depending on other profile characteristics. Attackers' interests focus disproportionally on profiles of certain types, including those originating in North America and featuring Crypto resources. We model and evaluate the relative importance of different profile characteristics in the final decision of an attacker to purchase a profile, and discuss implications for defenses and risk evaluation.
With the increased proliferation of internet-enabled mobile devices and large internet use, cybercrime incidents have grown exponentially, often leading to huge financial losses. Most cybercrimes are launched through malware attacks, phishing attacks, denial/distributed denial of attacks, looting people's money, stealing credential information for unauthorized use, personal identity thefts, etc. Timely detection of malware can avoid such damage. However, it requires an efficient and effective approach to detecting such attacks. This study attempts to devise a malware detection approach using transfer learning and machine learning algorithms. A hybrid approach has been adopted where pre-trained models VVG-16 and ResNet-50 extract hybrid feature sets from the data to be used with the machine learning algorithms. In doing so, this study contrives the Bi-model architecture where the same models are combined sequentially in the stacked form to obtain higher performance as the output of the first model is used to train the second model. With the Bi-model structure, 100% accuracy is obtained for a 25 classes problem. Performance comparison with state-of-the-art models and T-test proves the superior performance of the proposed approach. (c) 2022 Elsevier Inc. All rights reserved.
Social media is an important platform for accessing information and connecting with friends and family, yet in 2016, only 34% of Americans over the age of 64 had ever used online networking sites. Older people face significant barriers to usage of social media and are particularly vulnerable to internet-based scams, yet risk suffering from loneliness and isolation while they go through major life changes associated with aging. Research suggests that social media could be an important tool to reduce this isolation, and help people stay connected and supported while they age. This essay discusses the heightened relevance of this topic in the context of the Covid-19 pandemic, how older people most often interact with social media, the digital divide and how it disproportionately affects seniors, and some important upsides and downsides of older people being active on social media. Upsides include the capacity of social media to reduce isolation among vulnerable populations and improve various health outcomes, as well as being a chance to stay current with family members' lives. The most pressing downside is the dangers faced by older users who may not know how to protect themselves against internet-based fraud and identity theft. For those who wish to encourage older people to use social media to connect, there is therefore a strong need for digital literacy programs to help them learn to do so safely. Librarians can and will play a huge role in the future of this issue.
Phishing attacks against financial institutions constitutes a major concern and forces them to invest thousands of dollars annually in prevention, detection and takedown of these kinds of attacks. This operation is so massive and time critical that there is usually no time to perform analysis to look for patterns and correlations between attacks. In this work we summarize our findings after applying data analysis and clustering analysis to the record of attacks registered for a major financial institution in the US. We use HTML structure and content analysis, as well as domain registration records and DNS RRSets information of the sites, in order to look for patterns and correlations between phishing attacks. It is shown that by understanding and clustering the different types of phishing sites, we are able to identify different strategies used by criminal organizations. Furthermore, the findings of this study provide us valuable insight into who is targeting the institution and their modus operandi, which gives us a solid foundation for the construction of more and better tools for detection and takedown, and eventually for forensic analysts who will be able to correlate cases and perform focused searches that speed up their investigations.
Tor traffic tracking is valuable for combating cybercrime as it provides insights into the traffic active on the Tor network. Tor-based application traffic classification is one of the tracking methods, which can effectively classify Tor application services. However, it is not effective in classifying specific applications due to more complicated traffic patterns in the spatial and temporal dimensions. As a solution, the authors propose FlowMFD, a novel Tor-based application traffic classification approach using amount-frequency-direction (MFD) chromatographic features and spatial-temporal modelling. Expressly, FlowMFD mines the interaction pattern between Tor applications and servers by analysing the time series features (TSFs) of different size packets. Then MFD chromatographic features (MFDCF) are designed to represent the pattern. Those features integrate multiple low-dimensional TSFs into a single plane and retain most pattern information. In addition, FlowMFD utilises a cascaded model with a two-dimensional convolutional neural network (2D-CNN) and a bidirectional gated recurrent unit to capture spatial-temporal dependencies between MFDCF. The authors evaluate FlowMFD under the public ISCXTor2016 dataset and the self-collected dataset, where we achieve an accuracy of 92.1% (4.2%up arrow) and 88.3% (4.5%up arrow), respectively, outperforming state-of-the-art comparison methods.
"The rate and diversity of cybercrime is growing rapidly. It has become more commonplace, more sophisticated, and more damaging, costing the world's economy billions of euro in losses every year. To counteract this rising menace, organisations are continuously increasing their investment in information security and cybersecurity, including ethical hacking or penetration testing services. Traditionally, ethical hackers have always used expensive computationally powerful hardware to run special security-oriented Linux distributions. However, the advent of single board computers in the last decade, has offered new possibilities for ethical hacking. This research aims to explore, in a practical manner, the actual feasibility of performing ethical hacking duties in a non-traditional way, i.e., following a novel approach that still uses the same Linux security distributions, but installing them on these low-cost, highly portable small boards instead. The outcome is a comprehensive study that contributes to the body of work in the literature. We conducted extensive performance tests on 4 devices (3 boards and 1 laptop); results are varied and show that the small boards are capable of performing many jobs, albeit with a larger execution time than a laptop, but in general they are limited in terms of running heavy GUIs."
Online Grooming is a growing phenomenon within online environments. One of the major problems encountered in qualitative internet research of chat communication is the issue of anonymity which is being exploited and greatly enjoyed by chatters. An important question that has been asked in the literature is 'How can a researcher be sure to analyse the communication of children and adolescents and not the chat communication of adults who pretend to be under 18?'. Our reply to this question would be the field of Authorship Analysis. Authorship Analysis offers a way to unmask the anonymity of cyber predators. Stylometry, as used in this chat log analysis, is a type of Authorship Analysis that is not based on an author's handwriting but includes contextual clues from the content of their writings. This research paper will analyse the application of different authorship attribution techniques to chat log from a forensic perspective.
Cybersecurity is a major issue today. It is predicted that cybercrime will cost the world $6 trillion annually by 2021. It is important to make logins secure as well as to make advances in security in order to catch cybercriminals. This paper will design and create a device that will use Fuzzy logic to identify a person by the rhythm and frequency of their typing. The device will take data from a user from a normal password entry session. This data will be used to make a Fuzzy system that will be able to identify the user by their typing speed. An application of this project could be used to make a more secure log-in system for a user. The log-in system would not only check that the correct password was entered but also that the rhythm of how the password was typed matched the user. Another application of this system could be used to help catch cybercriminals. A cybercriminal may have a certain rhythm at which they type at and this could be used like a fingerprint to help officials locate cybercriminals.
The botnet, a group of computers infected with malicious software and remotely controlled without their owners' knowledge, is a ubiquitous tool of cybercrime. Law enforcement can take over botnets, typically by seizing their central command and control servers. They can then manipulate the malware installed on private computers to shut the botnet down. This Note examines the Fourth Amendment implications of the government's use of remote control of malware on private computers to neutralize botnets. It finds that the government could take more intrusive action on infected computers than it has previously done without performing a search or seizure under the Fourth Amendment. Most significantly, remotely finding and removing malware on infected computers does not necessarily trigger Fourth Amendment protections. Computer owners have no possessory interest in malware, so modifying or removing it does not constitute a seizure. Additionally, even if the government's efforts cause some harm to private computers, this will rarely produce a seizure under the Fourth Amendment because any interference with the computer will be unintentional. Remotely executing commands on infected computers does not constitute a search under the Fourth Amendment unless information is returned to law enforcement.
Malware, a lethal weapon of cyber attackers, is becoming increasingly sophisticated, with rapid deployment and self-propagation. In addition, modern malware is one of the most devastating forms of cybercrime, as it can avoid detection, make digital forensics investigation in near real-time impossible, and the impact of advanced evasion strategies can be severe and far-reaching. This makes it necessary to detect it in a timely and autonomous manner for effective analysis. This work proposes a new systematic approach to identifying modern malware using dynamic deep learning-based methods combined with heuristic approaches to classify and detect five modern malware families: adware, Radware, rootkit, SMS malware, and ransomware. Our symmetry investigation in artificial intelligence and cybersecurity analytics will enhance malware detection, analysis, and mitigation abilities to provide resilient cyber systems against cyber threats. We validated our approach using a dataset that specifically contains recent malicious software to demonstrate that the model achieves its goals and responds to real-world requirements in terms of effectiveness and efficiency. The experimental results indicate that the combination of behavior-based deep learning and heuristic-based approaches for malware detection and classification outperforms the use of static deep learning methods.
One Time Password (OTP) and Text Password are becoming less important in the current age of cybercrime because of the rapid development of new security systems. A user authentication system that is easy, robust, scalable, and cost-effective is a must. For both security and surveillance purposes, keystroke biometrics is a viable option. Behavior biometrics, of which keystroke biometrics is a subset, is used to identify individuals based on the way they type. Typing habits are not set in stone and are subject to change depending on the scenario, the device being used, and the user's emotional state. As a result, the performance of a keystroke biometrics-based user authentication system is influenced by how well the retrieved information from typing and classification algorithms is processed. Using a keyboard with an array of pressure sensors, this research presents a unique way to keystroke dynamics-based authentication. The goal of this study is to develop user profiles that are unique and different in order to improve the suggested system's efficiency. A real-world dataset is used to test the suggested method. The outcome is achieved with a 97% success rate in experiments.
This paper studies the behaviour analysis of distinct operating systems for the purpose of forensics investigation in the IPv6 network and ensures the detection as well as identification of the network host. The network forensics parameters help to capture, filter, analyse, and information reporting about the computer-based incidents and activities of cybercrime. IPv6 supports tackling the complication of traffic in a network environment, such as dual-stack, tunnel, and translation. This research sheds light on the IPv6 network, assesses the automatic and manual transition in order to characterise network behaviour. This paper proposes a flexible and automated method architecture to analyse operating systems behaviour by observing the system function calls, performing network investigation by using PCAP file analysis to help detect and identify the host, sessions, and open ports in the virtual environment. Through the experimental result on the network traffic, PCAP files dataset of the University of New Haven, the proposed model can archive identify network host in IPv6 network with high accuracy rate, the result shows the robustness of the NetworkMiner in terms of behaviour analysis with efficacy as compared to other state-of-the-art schemes.
Cloud computing is a revolutionary technology that provide computing and storage resources as a service accessible via network connections. Cloud computing has two foremost models which are services and deployment. One of the significant deployment models is a private cloud which is used by large organizations for building a corporate cloud inside their internal departments to provide computing services such as Software as a Service (SaaS), Platform as a Service (PaaS), and Infrastructure as a Service (IaaS). They adopted the internal private cloud to reduce cost and save time. In last decades, number of severe attacks and threats against the private cloud is increasing which make it facing complex challenges in the security province so that a forensic approach for cybercrime investigation should be systematized. Therefore, this paper presents a digital forensic approach for investigation of cybercrimes in a private cloud environment. Here, focus on using virtualization technology solutions from VMware for building experimental environment. From the experiment analysis, the proposed approach can help digital investigators and practitioners in acquisition and collection of digital evidence from the private cloud infrastructures especially virtual machine which is considered the core element of virtualized cloud systems.
The pace of development of the digital economy is growing rapidly from year to year. Internet, related digital technologies, as well as automation processes are widespread and change the entire system of economic activity. In the new conditions, it is necessary to assess the effects of digitalization, their impact on human life and business activities. In the event of risks and threats to economic security, the state should act as a regulator and include compensatory protection mechanisms. The article reveals the nature of the interaction of the digital economy, institutional changes and economic security. It is proved that the root causes of risks and threats to economic security, such as an increase in costs for each group of subjects due to excessive load and the institutional manifestation of regulatory arbitrage, the change in the labor market and the release of labor resources, intensive development of cybercrime are inadequate institutional structure of society, the imbalance of formal and informal institutions, their low efficiency. The analysis of positive (technological, economic and social) effects of digitalization is made, and also the risks and threats which are manifestation of negative effects of digital development are systematized.
Today, small and medium-sized enterprises (SME) can be considered as the new big target for cyber attacks, while the cybercrime prevention is often neglected within their environment. This paper aims to investigate the characteristics of cybersecurity threats in the Digital Innovation Hub (DIH) ecosystem of a Smart-Home/Office environment being constituted by SMEs that contains various smart -devices and IoT equipment, smart-grid components, employees'workstations and medium sized networking equipment. As the Cyber-security in such an ecosystem is greatly demanding and challenging because of the various communication layers and the different supported IoT devices, we introduce a more robust, resilient and effective cybersecurity solution that can be effortlessly tailored to each individual enterprise's evolving needs and can also speedily adapt/respond to the changing cyber threat landscape. Thus, this Cyber-security framework will be evaluated through three major types of Smart-Home/Office datasets and will be supported from SME/ICT clusters under the framework of the Secure and Private Smart Grid (SPEAR) H2020 project. The first promising results of our work indicate the potential of implementing strong defence mechanisms for SMEs' environments within DIHs.
PurposeThe purpose of the study is to show that divergent perceptions among regulators, the regulated and the associated regulatory bodies across multiple jurisdictions regarding the nature and functionality of cryptocurrencies hamper the development of a more comprehensive and coherent regulatory framework in curbing crimes and other related risks associated with cryptocurrencies. Design/methodology/approachThe study has used a descriptive doctrinal legal research method to investigate and understand the insights of existing laws and regulations in four selected jurisdictions concerning cryptocurrencies and how these laws could be further improved and developed to reduce crypto-related crimes. Furthermore, the study has also used a comparative research method to conceptualize the contours of the new legal discourse emerging from cryptocurrencies to adopt and implement a sound regulatory framework. FindingsThe study illustrated that divergent regulatory treatment among different jurisdictions might suffocate novel digital innovations such as cryptocurrency. These fragmented regulatory approaches by various jurisdictions question the sustainability of the present national legislation adopted to regulate cryptocurrencies. Looking into other jurisdictional developments in regulating cryptocurrencies, it is apparent that a concerted regulatory approach is needed to minimize the abuse of this innovation. Research limitations/implicationsThe study has implications for regulators and policymakers to review the current regulatory framework for regulating cryptocurrencies to prevent regulatory arbitrage. The divergent legislative measures concerning cryptocurrency among different jurisdictions question the sustainability of these legislative initiatives, considering the evolving and borderless nature of cryptocurrency. Therefore, this paper will help regulators to consider the present legislative gaps in establishing a common global regulatory approach in the crypto sphere. Originality/valueThe study contributes to the existing body of literature by examining the regulatory frameworks of four jurisdictions, namely, the USA, Canada, China and the EU, related to cryptocurrencies, with a discussion on the development of cryptocurrencies-related laws among these four jurisdictions and their sustainability in curbing crimes in the Darknet.
Network flow monitoring has been a part of network security for the last dozen years. It is constantly evolving to keep pace with changes in network operation and innovative network attacks. The thesis contributes to the continuous efforts by exploring the possibilities unlocked by extending the flow data with application-specific information. We show how the construction of flows is affected by processing of application data, present the benefits to traffic analysis, and assess the inevitable performance loss caused by additional data processing. To compensate for the lost performance, several novel optimisation techniques are proposed for the flow monitoring process. Recognising that the increasing deployment of encryption is going to limit the benefits of application flow monitoring, we perform a survey of methods for measurement of encrypted traffic. The thesis is concluded by an outlook towards future possibilities for flow monitoring advancement.
Cybercrime is growing at a rapid pace, and its techniques are becoming more sophisticated. In order to actively cope with such threats, new approaches based on machine learning and requiring less administrator intervention have been proposed, but there are still many technical difficulties in detecting security attacks in real time. To solve this problem, we propose a new machine learning-based real-time intrusion detection algorithm. Unlike the existing approaches, the one proposed can detect the presence of an attack every time a packet is received, enabling real-time detection. In addition, our algorithm effectively reduces the system load, which may significantly increase from real-time detection, compared to non-real-time detection. In the algorithm, the increase in the number of memory accesses can be minimized (to below 30 %) compared to conventional methods. Since the proposed method is pure software-based approach, it has excellent scalability and flexibility against various attacks. Therefore, the proposed method cannot support the high classification performance of the hardware-based method but also the high flexibility of the software-based method simultaneously, it can effectively detect and prevent various cyber-attacks.
With the progressive increase of network application and electronic devices (computer, mobile phones, android, etc),attack and intrusion detection is becoming a very challenging task in cybercrime detection area. in this context, most of existing approaches of attack detection rely mainly on a finite set of attacks. However, these solutions are vulnerable, that is, they fail in detecting some attacks when sources of informations are ambiguous or imperfect. But, few approaches started investigating toward this direction. Following this trends, this paper investigates the role of machine learning approach (ANN, SVM)in detecting TCP connection traffic as normal or suspicious one. But, using ANN and SVM is an expensive technique individually. In this paper,combining two classifiers has been proposed, where artificial neural network (ANN) classifier and support vector machine (SVM) were employed. Additionally, our proposed solution allows to visualize obtained classification results. Accuracy of the proposed solution has been compared with other classifier results. Experiments have been conducted with different network connection selected from NSL-KDD DARPA dataset. Empirical results show that combining ANN and SVM techniques for attack detection is a promising direction.
The non-consensual dissemination of sexually explicit images or videos for no legitimate purpose represents a serious sexual violation by means of breaching an individual's ability to control their own sexual identity. This article argues that the only adequate legal response to this behaviour is targeted criminal regulation, and that action on a regional level is within the European Union's competence by virtue of Article 83(1) TFEU. The approaches of EU Member States to the phenomenon through targeted criminal law are examined in order to extract positive and negative elements of the respective regulatory systems. Drawing on the experiences of these States, as well as on existing EU criminal directives, a draft directive is included to illustrate what form a European regional system of criminal regulation may take. (C) 2018 David JJ Ryan. Published by Elsevier Ltd. All rights reserved.
Botnets play an important role in malware distribution and they are widely used for spreading malicious activities in the Internet. The study of the literature shows that a large subset of botnets use DNS poisoning to spread out malicious activities and that there are various methods for their detection using DNS queries. However, since botnets generate domain names quite frequently, the resolution of domain names can be very time consuming. Hence, the detection of botnets can be extremely difficult. This chapter propose a novel deep learning framework to detect malicious domains generated by malicious Domain Generation Algorithms (DGA). The proposed DGA detection method, named, Deep Bot Detect (DBD) is able to evaluate data from large scale networks without reverse engineering or performing Non-Existent Domain (NXDomain) inspection. The framework analyzes domain names and categorizes them using statistical features, which are extracted implicitly through deep learning architectures. The framework is tested and deployed in our lab environment. The experimental results demonstrate the effectiveness of the proposed framework and shows that the proposed method has high accuracy and low false-positive rates. The proposed framework is a simple architecture that contains fewer learnable parameters compared to other character-based, short text classification models. Therefore, the proposed framework is faster to train and is less prone to over-fitting. The framework provides an early detection mechanism for the identification of Domain-Flux botnets propagating in a network and it helps keep the Internet clean from related malicious activities.
The deployment of Voice over Internet Protocol (VoIP) in place of traditional communication facilities has helped in huge reduction in operating costs, as well as enabled adoption of next generation communication services-based IP. At the same time, cyber criminals have also started intercepting environment and creating challenges for law enforcement system in any Country. At this instant, we propose a framework for the forensic analysis of the VoIP traffic over the network. This includes identifying and analyzing of network patterns of VoIP-SIP which is used for the setting up a session for the communication, and VoIP-RTP which is used for sending the data. Our network forensic investigation framework also focus on developing an efficient packet reordering and reconstruction algorithm for tracing the malicious users involved in conversation. The proposed framework is based on network forensics which can be used for content level observation of VoIP and regenerate original malicious content or session between malicious users for their prosecution in the court.
Controlling the flow of sensitive data has been widely acknowledged as a critical aspect for securing web information systems. A common limitation of previous approaches for the implementation of the information flow control is their proposal of new scripting languages. This makes them infeasible to be applied to existing systems written in traditional programming languages as these systems need to be redeveloped in the proposed scripting language. This paper proposes a methodology that offers a common interlinqua through the use of Semantic Web technologies for securing web information systems independently of their programming language.
Information can be considered as an invaluable commodity for all business entities, and has brought about the development of various security architectures devoted to its protection. Corporations have tended to react to the exploitation of information security (InfoSec) vulnerabilities through the implementation of technological measures. Indeed, most security policies and procedures are highly technologically inclined, making use of hardware and software to protect and safeguard the confidentiality, integrity and availability of data. Unfortunately, these tactics have achieved limited success because of inattention to the opportunistic aspects of crime commission. Situational crime prevention can address the importance of these aspects by concentrating on the circumstances associated with a crime, and how the setting, conditions and context can be modified to preclude its manifestation. Its specific application to cyber and InfoSec in a corporate setting is advantageous in developing competent proactive strategies to reduce the presence and attractiveness of criminal possibilities for would-be offenders.
This article introduces the concept of Artificial Intelligence (AI) to a criminological audience. After a general review of the phenomenon (including brief explanations of important cognate fields such as 'machine learning', 'deep learning', and 'reinforcement learning'), the paper then turns to the potential application of AI by criminals, including what we term here 'crimeswithAI', 'crimesagainstAI', and 'crimesbyAI'. In these sections, our aim is to highlight AI's potential as a criminogenic phenomenon, both in terms of scaling up existing crimes and facilitating new digital transgressions. In the third part of the article, we turn our attention to the main ways the AI paradigm is transforming policing, surveillance, and criminal justice practices via diffuse monitoring modalities based on prediction and prevention. Throughout the paper, we deploy an array of programmatic examples which, collectively, we hope will serve as a useful AI primer for criminologists interested in the 'tech-crime nexus'.
"This paper presents a study of the Nigeria's presence in Cyberspace. The Cyberspace (Internet) is now critical to every nation's socio-economic, cultural and political activities. When it is disrupted or fails, can grind a nation to a standstill. On the contrary, its correct functioning and pliability is transforming modern society with exceptional pecuniary and social benefits. With nearly all traditional activities increasingly moving to the Internet, Cyberspace has become a new stage for innovations, enterprises, social networking, criminality and war. For this study, the United Nations (UN) e-governance framework was used, the highlights of the United Nations E-Government Survey report of 2014 was analyzed to show the ranking of the world leaders, West African countries and where Nigeria stands. The Internet penetration growth and evolving Internet infrastructure provisioning in Nigeria were reviewed and a critical assessment of Nigeria's presence on the Cyberspace was carried out using the UN online presence index methodology between the months of August and September 2015. The web content, interactivity, the currency of information, downloadable documents, etc. were used to compare various sectors of the Nigerian economy; all tiers of government, academia, and the organized private sector. The study revealed that the organized private sector and private educational institutions are doing better than government educational institutions and ministries. Based on international best practices, a list of recommended actions for increasing cyberspace presence and achieving e-governance for improved services and productivity in Nigeria and similar developing countries is proffered."
Visualizing digital evidence in an easy and constructive manner is a major problem because of the advanced techniques for hiding, wiping, encrypting and deleting digital data developed during the last few years. Oo tackle this problem, a system for visualizing digital data in 3dimensional (3D) mode has been developed. XML was used as a common language to allow fine-grained management of digital data with flexibility and ease. The extensibility of the implementation makes it particularly suitable as a research and development platform in the sector of open source computer forensics tools for the future. This article examines real-life problems that benefit from using this tool in a congenial and constructive manner to validate its key underlining concept. The design decisions that have been taken in producing the system architecture, and the features it supports are elaborated upon. To determine the effectiveness of the tool, an actual case study is presented which examines the results of the tool and why it is necessary to go for an open source model as a standard. The paper concludes with performance measurements of the tool and suggests possible extensions to make the tool even smarter. (c) 2007 Elsevier B.V, All rights reserved.
"Phishing is the most prevalent method of cybercrime that convinces people to provide sensitive information; for instance, account IDs, passwords, and bank details. Emails, instant messages, and phone calls are widely used to launch such cyber-attacks. Despite constant updating of the methods of avoiding such cyber-attacks, the ultimate outcome is currently inadequate. On the other hand, phishing emails have increased exponentially in recent years, which suggests a need for more effective and advanced methods to counter them. Numerous methods have been established to filter phishing emails, but the problem still needs a complete solution. To the best of our knowledge, this is the first survey that focuses on using Natural Language Processing (NLP) and Machine Learning (ML) techniques to detect phishing emails. This study provides an analysis of the numerous state-of-the-art NLP strategies currently in use to identify phishing emails at various stages of the attack, with an emphasis on ML strategies. These approaches are subjected to a comparative assessment and analysis. This gives a sense of the problem, its immediate solution space, and the expected future research directions. (C) 2021 The Authors. Published by Elsevier B.V."
Identity theft is the most recurrent twenty-first century cybercrime. Thus, authentication is of utmost significance as the number of hackers who seek to intrigue into legitimate user's account to obtain sensitive information is increasing. Identity based authentication operates to corroborate the identity of the user so that only the legitimate user gets access to the service. This paper proposes a quantum identity based authentication and key agreement scheme for cloud server architecture. Quantum cryptography based on the laws of quantum physics is a vital technology for securing privacy and confidentiality in the field of network security. A formal security analysis has been performed using AVISPA tool that confirms the security of the proposed scheme. The security analysis of the proposed protocol proves that it is robust against all security attacks. To confirm applicability of quantum key distribution in cloud computing, a practical long-distance entanglement-based QKD experiment has been proposed. This experiment confirms successful generation of shifted keys over distance of 100 km of optical fiber with a key rate of 4.11 bit/s and an error rate of 9.21 %.
"Most communications in the new era are dependent on Information and Communication Technology (ICT). In addition, infrastructure is becoming increasingly interconnected. This not only makes lives easier, but also leaves technology users more vulnerable. Cybercrime, digital espionage and other cyber disturbances dictate the news reports on a daily basis. In general, cyber-attacks are no longer confined to small-scale rogue hackers. Cyber-attacks are now a part of organised crime and the underground economy, posing a real threat to critical infrastructure; possibly with state actors driving these actions. The responsibility to protect ICT stretches beyond individual companies, sectors and even beyond nations. The authors of this paper propose a Cybersecurity Centre Of Innovation (CCOI) as a central point for the South African government, business and academia to create a secure cyber space for the country: a cyber space without crime that is resilient and resistant to disruptions; a cyber space that promotes innovation, helps the economy and enhances national security. The key driver of the proposed CCOI is collaboration; solutions to cyber risks require a combined approach. This paper describes the organisational structure, functions, activities and benefits of a CCOI."
Fraud detection procedures for national and international economies have become quite an important task. Ensuring the security of transactions carried out by banks and other financial institutions is one of the major factors affecting the reputation and profitability of such organizations. However, since people who perform fraudulent transactions change their methods constantly in order not to get caught up, it gets more difficult to identify and detect this type of transactions. Detecting this type of transactions makes the support of technology compulsory, considering high volume and intensity of transactions. In this paper, we explore practicality of using location data to aid finding better business rules where they can easily be deployed with a rule-based fraud detection and prevention system for retail banking. In order to study the importance of location data, we first compiled a set of anonymized automated teller machine (ATM) usage data from a mid-size bank in Turkey. Depending on how much mobile the card owners are, we can easily devise business rules to detect the anomalies. Such anomalies can be directed to appropriate business units to be analyzed further or account owners may be required additional authorizations for banking activities (such as internet money transfers and payments). We have shown in this paper that a significant bulk of ATM users does not leave the vicinity of their living place. We also give some brief use cases and hints regarding what types of business rules can be extracted from location data.
The shared concern expressed in the two quotes below is that modern technologies provide criminals with a capability to evade investigation. This comment piece examines some of the policy and legal options available to governments and law enforcement agencies to try to address this concern. While accepting the claim that this phenomenon represents a real challenge to law enforcement agencies, we currently have insufficient evidence to show the true extent of the problem. What this piece does not accept is the implication contained in the quotes, and often made explicit by others, that the use of encryption represents a fundamental and irreversible shift in the balance of power between criminals and their investigators from what previously prevailed. Such claims tend to lack historical perspective, which is one of the themes of this 200th issue of Computer Law and Security Review. (C) 2018 Ian Walden. Published by Elsevier Ltd. All rights reserved.
"The growth of internet in Pakistan is exceptional; it reached to the heights of popularity for an ever changing medium of information and communication in a major conservative society. This brings freedom of expression, communication and information across the country which struggled to provide free speech and information access to its people. The internet penetration is increasing very rapidly; it is very important to observe the impact of it on the society; especially when the government is constantly trying to formulate the regulation for controlling the cyber space. A well regulated cyber space leads to the expansion of information technology services speedily. This paper focuses on the prevailing development of governance policies for cyber space and defies among the agencies and information technology venders. Moreover, the case study of national bank of Pakistan and other cyber crime survey shows that a lot of law making is needed in order to reduce the chances of cyber crime."
The complexity, multifacetedness, and interdisciplinary status of ensuring the collection of evidence leads to the need to study it in the coordinate system set by different levels of scientific methodology. With this in mind, we consider it necessary to analyze the current issues of study methodology in ensuring evidence collection on the basis of cybercrimes. The purpose of the work is studying the criminal-legal, criminal-procedural, and forensic aspects of the methodological foundations of research in securing evidence on the example of cybercrimes. The research methodology includes such methods as the general system-structural method, the dialectical method, the historical method, the system method, the comparative-legal method, logical methods (deduction, induction, analogy, analysis, synthesis), the structural-logical method, and the modeling method. As a conclusion of the conducted research, the regulatory and legal factors that constitute the legal basis of the organizational and technical principles of the investigation of crimes in the field of cybercrime were considered, and the problematic issues of the research methodology were identified. It was concluded that the methodological support of the research of evidence collection is a complex, multifaceted task of scientific research, it can be solved only on the basis of a systematic approach to this problem.
"Online harassment, particularly cyberbullying and the non-consensual sharing of intimate images, is a widespread phenomenon among adolescents and young adults. Descriptive research was carried out to investigate any differences among Italian school classes in the perception of cybercrime through a real-case scenario. Following the Italian school system, the final sample of 1777 adolescents (Mage = 15.37, SD = 1.65; Male = 52%) was divided into three groups based on the school class attended: middle school (N = 562; Mage = 13.37, SD = 0.48); high school biennium (N = 728; Mage = 15.55, SD = 0.50), and triennium (N = 487, Mage = 17.40, SD = 0.71). Participants completed a self-report questionnaire investigating the use of the Internet and the perception of a real case scenario involving the non-consensual sharing of intimate images and cyberbullying received by the National Centre for Combating Child Pornography Online (NCPO). Results showed differences among the three groups' perceptions of the event's features, motivations underlying the offense, victim-blaming and harassment justification (e.g., cyberbullying, in particular non-consensual sharing of intimate images, is recognized as a crime as age increases). The findings provide significant insights for future research and age-specific factors to consider when developing prevention programs for online risks."
From the viewpoint of cybercrime, young people represent a particularly vulnerable category of Internet users: children, adolescents and students. Young people are, undoubtedly, the most common and the most gullible users of social networks. Due to lack of education related to dangers they are exposed to on social networks, inexperienced users recklessly post information and multimedia contents on their profiles which can be misused by differently motivated Internet users. Apart from being exposed to the risk from the violation of personal privacy and misuse of personal data, young people are exposed to the risk from political or ideological manipulation. Various studies on social networks and secondary school students have been conducted in the Republic of Serbia. However, no study in this field which would look at university students has yet been conducted. The purpose of this study was to discover the extent to which students use social networking sites, but also the sources and ways students perceive online security risks associated with social networking. Study results show that the media has a dominant role in educating young people on the risks associated with social networking and that the impact of the media is greater than the impact of other educational factors such as family, school or university.
The contribution of new technology is synonym with developed nations when compares to the developing countries that commonly adopt the new technology. This paper attempts to empirically investigate the link between technology adoption and the economic growth of twelve selected developing Asian countries. The sources of technology adoption are from Mobile Cellular Subscription, Passenger Car in Use, Secure of internet Server and Fixed Telephone Line that could help in promoting economic growth of the nations. First, this study will apply three competing econometric models known as the Pooled OLS, the Random Effects Model and Fixed Effects Model to identify the suitable model that best explained the data of these nations. Next, from the model we could determine the significant sources which have effect on the economic growth of the nations. The empirical results indicate that Secure of Internet Server is the only source showing a positive significant impact on the Gross Domestic Product of these nations. Despite being the critical factor of technology adoption that would promote growth through e-commerce activities, countries are worry with the rapid increase of cybercrime threats. Hence, the movement toward sovereign control has become necessary for some countries to protect themselves from malicious cyber-attacks.
Sha Zhu Pan ((sic)), or Pig-Butchering scam, is a new online romance scam that mainly targets Chinese-speaking individuals to gain emotional trust and persuade victims to invest in a fictitious website controlled by the scammer group. The substantial economic loss forced the Chinese government to launch a series of judicial campaigns against this crime. Regardless, Sha Zhu Pan has not been significantly curbed but intensified and spread from mainland China to Southeast Asia, North America, and other regions. When procuring answers to this conundrum, no empirical studies have attempted to explore judicial aspects among Sha Zhu Pan cases in China. The purpose of this paper is to utilize focal concern theory, a conceptual framework generally applies to explain sentencing disparity, to explore whether the sentencing of Sha Zhu Pan offenders is guided by legal factors outlined in Chinese sentencing guidelines. To answer this question, this study quantitatively analyzes 172 legal cases collected from China Judgments Online. Although few inconsistent findings exist, the empirical evidence reveals that the judgments in Sha Zhu Pan cases are primarily driven by the three legal elements of the focal concern theory. The outcomes expand the focal concern theory to cybercrime sentencing in a cross-cultural context while offering insights to guide future research.
"Saudi Arabia has witnessed an exponential growth in smartphone adoption and penetration. This increase has been accompanied with an upward trend in cyber and mobile crimes. This calls to efforts that focus on enhancing the awareness of the public to security-related risks. In this study, we replicated the study performed by Albayram et al. published in SOUPS 2017; however, our study targetted participants in Saudi Arabia. We also investigated different fear appeal video designs that were more suited for this population (customized video, Arabic dubbed, and captions for the original video). The results from the original study, conducted in the United States, showed that 50% of participants in the treatment group and 21% in the control group enabled screen lock. The reason for replicating the original paper was to increase Saudis' awareness regarding the importance of sensitive data, especially with the increasing level of cybercrime. Our results showed that the Saudi-customized video was extremely effective in changing our participants' locking behavior (72.5% of participants enabled the screen lock), based on customized applications and Saudi culture. The dubbed video was the second-most effective (62.5%) locking behavior. Finally, we have illustrated our data comparison analysis in detail."
The unprecedented growth of the Internet has given rise to the Dark Web, the problematic facet of the Web associated with cybercrime, hate, and extremism. Despite the need for tools to collect and analyze Dark Web forums, the covert nature of this part of the Internet makes traditional Web crawling techniques insufficient for capturing such content. In this study, we propose a novel crawling system designed to collect Dark Web forum content. The system uses a human-assisted accessibility approach to gain access to Dark Web forums. Several URL ordering features and techniques enable efficient extraction of forum postings. The system also includes an incremental crawler coupled with a recall-improvement mechanism intended to facilitate enhanced retrieval and updating of collected content. Experiments conducted to evaluate the effectiveness of the human-assisted accessibility approach and the recall-improvement-based, incremental-update procedure yielded favorable results. The human-assisted approach significantly improved access to Dark Web forums while the incremental crawler with recall improvement also outperformed standard periodic- and incremental-update approaches. Using the system, we were able to collect over 100 Dark Web forums from three regions. A case study encompassing link and content analysis of collected forums was used to illustrate the value and importance of gathering and analyzing content from such online communities.
Phishing is a cybercrime that deceives online users and steals their confidential information by impersonating a legitimate website or URL. Several machine learning-based strategies have been proposed to detect phishing websites. These techniques utilize a set of features extracted from website samples, the structure and syntax of URLs, the content of the pages, and querying external resources. In this work, we use a dataset of 11,430 samples with 87 extracted features that are designed to be used as a benchmark for machine learning-based phishing detection systems. Our classification is based on an ensemble learning technique, which is further optimized using grid search. The experiments provide a detailed description of tuning the model's hyperparameters and its optimization. We evaluate the model using well-known evaluation metrics of accuracy, precision, recall, F1-score, and area under the ROC curve. The findings indicate that our optimized ensemble model classifies legitimate and phishing URLs with an accuracy of 95.36%, a precision of 96.29%, a recall of 94.24%, an F1 score of 95.26%, and the area under the ROC curve of 0.9876.
Investments in Indian Medical Tourism (MT) have been growing at a greater rate than the global standards. However, unfortunately, Indian legislations have not managed to keep pace with this phenomenal growth, and have thereby been lacking in forming specific amendments, coupled with both preventive and compensative decrees. Moreover, safety and security have emerged as an aggregated inter-web of complex multi-dimensional components, encompassing aspects such as public and personal data safety, legal protection of tourists, consumers protection, and so on. The internet has affected data integration, storage, exchange exposing thereby the possibilities of security breaches. All of these, in amalgamation, call for elevating the overall hygiene of India's growing medical tourism industry. Based upon this backdrop, this study looks into the theories of security, privacy, and consumerism, and the need for preventive cybercrime laws in India's medical tourism sector, especially considering several technical vulnerabilities that do exist. In this endeavor, I reviewed and analyzed journals and government policies, and have looked to identify some of the fallacies and gaps. Additionally, I also attempted to identify some of the needs that exist in MT's cybersecurity space, both within the Indian and international contexts.
The growing digitization of healthcare institutions and its increasing dependence on Internet infrastructure has boosted the concerns related to data privacy and confidentiality. These institutions have been challenged with specific issues, namely the sensitivity of data, the specificity of networked equipment, the heterogeneity of healthcare professionals (nurses, doctors, administrative staff and other) and the IT skills they have. In this paper we present the results obtained with a study made with healthcare professionals on evaluating their awareness level with the information security, namely by assessing their attitudes and behaviours in cybersecurity. The methodology consisted in translating, adjusting and applying two previously validated and already published Likert-type response scales, in a healthcare institution in Portugal, namely Centro Hospitalar Barreiro Montijo (CHBM). The scales used were cybersecurity risky behaviour (RScB) and cybersecurity and cybercrime in business attitudes (ATC-IB). Although there were no significant statistical differences between the sociodemographic factors and the scores obtained on both scales, the results showed a relationship between acquired behaviours and the attitudes of involvement with work and organizational commitment, establishing a bridge for the quantification in awareness.(C) 2021 The Authors. Published by Elsevier B. V.
The digital economy has revolutionised financial service provision and compelled financial services institutions to adopt technologies that help deliver quality service at a minimal cost. Due to the recent disturbance in the financial sector in the country, there is an urgent need for financial service providers to adopt relevant technologies to help reduce operational inefficiencies and maintain service quality. This article analyses the implications of the digital economy for enhancing the efficiency of financial markets. Drawing the existing literature, it concludes that the financial institutions in the country have quite embraced and integrated digitisation into their activities. Given the perceived high level of digital illiteracy among the populace, it was surprising to discover the number of people who have signed on unto such financial products. It was recommended financial institutions integrate their operations with the necessary digital technologies as it helps reduce operational costs. Furthermore, they need to invest in digital infrastructure and skills of their staff to embrace technology in service delivery. It is also suggested that policymakers strengthen the laws, policies, and regulations such as data protection and cybercrime to enable the players of the market to operate freely and confidently.
The extreme growth of Internet resources leads to several kinds of attacks. Cybercrime is one of the dominant threats apart from data defence mechanism, which enhances the economy, resource management, and service quality. Among them, HTTP flooding attacks in the cloud are one of the most prevalent threats as it depletes the cloud resources and services. It is difficult to distinguish the anomalous traffic by extracting the actual payload since most of the payload could not be accessed as they are encrypted and varies dynamically based on the user input. Hence, the proposed method uses web server logs that can be easily accessed to detect the attacks. This study highlights the detection methods by extracting the features from the web server logs and also deals with the reduction in the dimensionality of the features using diffusion map. The anomalies are detected by affinity propagation clustering technique and also by monitoring the status of the virtual machine. Furthermore, the Dempster-Shafer theory focuses on the identification of the suspicious user. It is inferred from the experimental results that the proposed method enhances the detection performance with very few false alarms than existing methods.
The past few years have witnessed millions of credit/debit cards flowing through the underground economy and ultimately causing significant financial loss. Examining key underground economy sellers has both practical and academic significance for cybercrime forensics and criminology research. Drawing on social media analytics, we have developed the AZSecure text mining system for identifying and profiling key sellers. The system identifies sellers using sentiment analysis of customer reviews and profiles sellers using topic modeling of advertisements. We evaluated the AZSecure system on eight international underground economy forums. The system significantly outperformed all benchmark machine-learning methods on identifying advertisement threads, classifying customer review sentiments, and profiling seller characteristics, with an average F-measure of about 80 percent to 90 percent. In our case study, we identified the famous carder, Rescator, who was affiliated with the Target breach, and captured important seller characteristics in terms of product type, payment options, and contact channels. Our research leverages social media analytics to probe into the underground economy in order to help law enforcement target key sellers and prevent future fraud. It also contributes to our understanding of the use of information technology in detecting deception in online systems.
In the past five years cybercrime has grown to become one of the most significant threats to the safety of the nation and its economy. The government's call to arms has been eagerly accepted by business enterprises and academia. But training cyber security professionals raises a unique set of challenges. Cost, space, time and scalability are among the issues identified and possible solutions proposed. As a cyber-security professionals, we have realized the importance of practical experience which can be hard to deliver in a lecture based environment. The primary aim of this project is to evaluate and recommend a platform for Virtual handson Labs which may be used to provide a secure environment for cyber security students to evaluate and receive hands-on experience on possible threats and countermeasures. There are similar labs setup in different universities across the world but we have not been able to find any studies evaluating the virtualization platforms for their merit in order to run a virtual lab. Hence we study three of the most popular virtualization platforms and recommendations are provided to guide anyone who desires to setup such a lab.
Cybersecurity finds widespread applications across diverse domains, encompassing intelligent industrial systems, residential environments, personal gadgets, and automobiles. This has spurred groundbreaking advancements while concurrently posing persistent challenges in addressing security concerns tied to IoT devices. IoT intrusion detection involves using sophisticated techniques, including deep learning models such as convolutional neural networks (CNNs), recurrent neural networks (RNNs), and anomaly detection algorithms, to identify unauthorized or malicious activities within IoT ecosystems. These systems continuously monitor and analyze network traffic and device behavior, seeking patterns that deviate from established norms. When anomalies are detected, security measures are triggered to thwart potential threats. IoT intrusion detection is vital for safeguarding data integrity, ensuring users' privacy, and maintaining critical systems' reliability and safety. As the IoT landscape evolves, effective intrusion detection mechanisms become increasingly essential to mitigate the ever-growing spectrum of cyber threats. Practical security approaches, notably deep learning-based intrusion detection, have been introduced to tackle these issues. This study utilizes deep learning models, including convolutional neural networks (CNNs), long short-term memory (LSTM), and gated recurrent units (GRUs), while introducing an ensemble deep learning architectural framework that integrates a voting policy within the model's structure, thereby facilitating the computation and learning of hierarchical patterns. In our analysis, we compared the performance of ensemble deep learning classifiers with traditional deep learning techniques. The standout models were CNN-LSTM and CNN-GRU, achieving impressive accuracies of 99.7% and 99.6%, along with exceptional F1-scores of 0.998 and 0.997, respectively.
Although it is generally acknowledged that the development of the Internet created new criminal opportunities, the ways this is experienced by the general population has received limited attention. The current study seeks to explore the degrees to which people report fear of online crime, its correlates, and how online fear is related to protective and avoidance behaviour. We focus on online financial crimes. Results based on a large, representative, survey indicate an intermediate level of fear of online crime among the general Dutch population. Various sociodemographic characteristics and victimization experiences are shown to predict fear of online financial crime. We also find online fear to be a strong predictor of avoidance behaviour, given negative relationships with online purchasing and banking, thereby impeding individuals' perceived online freedom and opportunities. Finally, the results found no indications that fear may stimulate protective behaviour of one's computer.
"Cyberattacks are committed by one or more people using a computer equipped with many viruses. The damage can be physical or moral. Cyberattacks are also often attacks on the information system with the purpose of changing, adding, or falsifying data, causing the disruption of a computer, or destroying information programs, whether economic, commercial, or military. This paper aimed to identify the international responsibility in incidents of cyberattacks in the light of contemporary international law. The study used a thematic content analysis research design to identify a few constructs namely legal adaptations of cyber-attacks, technological development; jurisprudence and international responsibility. The data was collected through juridical and descriptive methods from documentation and other legal sources. The study found that countries lack cyber legislation, there are significant legal gaps it is very difficult to place cyberattacks within the existing international legal framework. The study recommends that nations should work to achieve cyber security and preserve the rights resulting from the legitimate use of computers and information networks. It is also necessary to address the legislative gaps in fighting cybercrime and the necessity of enacting legislation that covers this gap to reach safe cyberspace."
"The preparation of a graduate level cyber security and forensics course in a computer science department that addresses theory, policy, and application for a multidisciplinary student audience can be daunting when the majority of students in the class do not have a computer science background. The course takes a holistic approach to broaden knowledge and deepen understanding of the domain of cyber security using cross disciplinary teams to gain understanding and experience taking theory to practice and practice to theory. A framework of understanding is built through the examination of the body of scholarly conceptual and technical works and hands on experience with hardware and software platforms and networks. Computer Science provides the theoretical underpinnings and technical details, methods, and tools to examine security concepts; Forensic Science provides the approach to critical analysis of digital evidence; and Behavioral Analysis provided a way to synthesize knowledge and scientific method to gain some understanding of criminal behavior as well as the breadth and economic impact of cybercrime. This approach resulted in students who gained technical proficiency and perspective and experience working with people with divergent backgrounds, abilities and knowledge sets."
The modern digital internet economy and billions of dollars of trade are made possible by the internet security which is provided by operating system and web browser developers. This paper provides a survey of how this security is implemented through the use of digital certificates and the Public Key Infrastructure. Documented cases of the abuse of these digital certificates are given. It is shown that these problems arise from a combination of commercial pressures and a failure of the designers of internet security to consider the fundamental security principal of least privilege. Measures which are used to mitigate these problems are noted and new PKI architectural components which are designed to correct existing problems are examined.
The prevalence and rapid growth of cybercrime are largely attributed to hacker communities on the dark web, where cybercriminals extensively exchange hacking resources, share hacking knowledge, and organize cyberattacks. Such streams of hacker-generated content constitute an invaluable data source for developing threat intelligence that can inform organizations of cybersecurity risks and facilitate proactive cyber defense. Drawing upon the design science paradigm, we propose a novel nonparametric emerging topic detection (NPETD) framework for detecting emerging topics in streams of hacker -generated content. Our framework extends the state-of-the-art nonparametric topic model to inductively model topics without having to specify the number of topics a priori. Moreover, our framework features an efficient algorithm to jointly infer topics and detect topic emergence. We conducted experiments to rigorously evaluate the effectiveness and efficiency of our framework in comparison with the state-of-the-art baseline methods. Our framework outperformed the baseline methods in detecting the listings of emerging threats in darknet marketplaces on recall, F-measure, topic coherence, and processor time. The practical utility of our framework is further demonstrated in a major hacker forum, where we identified several notable emerging topics with important implications for victim companies and law enforcement. The proposed framework contributes to cybersecurity, topic detection and tracking, and design science.
School systems may pay attention to the fact that individuals and companies using smart devices are increasingly at risk of becoming victims of cybercrime. The literature on how effective students in developed countries such as the Netherlands are taught about cyber security skills during their school career is scarce. Although curriculum materials are available, scaling up computer science education is behind. Therefore, this study explores to what extent Dutch students develop cyber secure behavior at elementary and high school. A questionnaire was used for self-assessment of cyber security behavior. After the questionnaire was completed, two group interviews were conducted to improve the interpretation of the questionnaire results. The study findings revealed that the Dutch school curriculum hardly pays attention to this topic and that students acquire their online behavior mainly through experience, instructions on the internet, through parents, and through siblings. In addition, many students developed more reckless behavior over time. We recommend that cyber security education should start at elementary school as soon as children begin to use online equipment. A subject that deserves special attention is recognizing phishing emails and phishing websites. The learners should be convinced that risky behavior on the internet may turn against them and against the organization to which they belong.
Advances in Artificial Intelligence (AI) have influenced almost every field including computer science, robotics, social engineering, psychology, criminology and so on. Although AI has solved various challenges, potential security threats of AI algorithms and training data have been stressed by AI researchers. As AI system inherits security threats of traditional computer system, the concern about novel cyberattack enhanced by AI is also growing. In addition, AI is deeply connected to physical space (e.g. autonomous vehicle, intelligent virtual assistant), so AI-related crime can harm people physically, beyond the cyberspace. In this context, we represent a literature review of security threats and AI-related crime. Based on the literature review, this article defines the term AI crime and classifies AI crime into 2 categories: AI as tool crime and AI as target crime, inspired by a taxonomy of cybercrime: Computer as tool crime and Computer as tool crime. Through the proposed taxonomy, foreseeable AI crimes are systematically studied and related forensic techniques are also addressed. We also analyze the characteristics of the AI crimes and present challenges that are difficult to be solved with the traditional forensic techniques. Finally, open issues are presented, with emphasis on the need to establish novel strategies for AI forensics.
The digitalization of our society is only possible in secure software systems governing ongoing critical processes. The understanding of mutual interdependencies of events and processes is crucial for cybersecurity. One of the promising ways to tackle these challenges is process mining, which is a set of techniques that aim to mine knowledge from processes. However, it is unclear how process mining can be practically used in the context of cybersecurity. In this work, we investigate the potential of applying process mining in cybersecurity and support research efforts in this area via collecting existing applications, discussing current trends, and providing promising research directions. To this end, we have conducted a systematic literature review covering all relevant works between 2014 and 2020.
Cybersecurity research relies on relevant datasets providing researchers a snapshot of network traffic generated by current users and modern applications and services. The lack of datasets coming from a realistic network environment leads to inefficiency of newly designed methods that are not useful in practice. This data article provides network traffic flows and event logs (Linux and Windows) from a two-day cyber defense exercise involving attackers, defenders, and fictitious users operating in a virtual exercise network . The data are stored as structured JSON, including data schemes and data dictionaries, ready for direct processing. Network topology of the exercise network in NetJSON format is also provided. (C) 2020 The Author(s). Published by Elsevier Inc.
This article aims to provide more insight into pedophilic attraction and risk and protective factors for offending in nonclinical pedophiles. Fifteen participants were interviewed about sexuality, coping, and sexual self-regulation. Many participants struggled with acknowledging pedophilic interest in early puberty and experienced psychological difficulties as a result. Furthermore, many committed sex offenses during adolescence when they were still discovering their feelings. Early recognition of risk factors and early start of interventions seem vital in preventing offending. Moreover, results suggest that risk for offending can be diminished by creating more openness about pedophilia and by providing pedophiles with social support and control.
With the increase in reliance upon technology in our everyday lives, users are more vulnerable than ever to cybercrime and data security breaches. Whilst it is important, and valued, to develop technology-based interventions to mitigate this risk, it is also important to consider the impact of human error on cyber safety, and how this can be measured. Data collected from a diverse sample of 189 participants using an alternative measurement scale to more traditional Likert scales, the Visual Analogue Scales (VAS), was adopted for previously researched measures of individual differences (age, gender, education level, personality, decision-making style, risk-taking preferences, acceptance of the internet, and related Theory of Planned Behavior and Protection-Motivation Theory concepts) to expand understanding of the relationships between individual differences and user-end cybersecurity behaviors, and explore the significance of this alternative measure in the field of Cyber Psychology. Findings demonstrate the use of VAS can be a reliable and valid method capable of identifying a variety of potential human vulnerabilities and strengths on an individual level. These findings highlight the importance of considering a human-centered approach to cyber-security, and future research should consider then importance of these individual differences in tailoring practical interventions.
The article discusses the main trends in information security, reflected in the judicial practice of the Russian Federation in the last period. The authors made a conclusion that modern judicial practice today considers various aspects of cases related to information security. These are cases related to violation of confidentiality of information, disclosure of personal data, falsification of electronic documents, unauthorized access to computer information, distribution of malicious computer programs, violation of information protection rules, illegal activities in the field of information protection, disclosure of information with limited access, violation of order posting information, etc. Every year the number of such cases pending before the courts is growing. This confirms the relevance of the considered problem and indicates the need to return to its consideration in the near future.
Authorship analysis has been used successfully to analyse the provenance of source code files in previous studies. The source code for Zeus, one of the most damaging and effective botnets to date, was leaked in 2011. In this research, we analyse the source code from the lens of authorship clustering, aiming to estimate how many people wrote this malware, and what their roles are. The research provides insight into the structure the went into creating Zeus and its evolution over time. The work has potential to be used to link the malware with other malware written by the same authors, helping investigations, classification, deterrence and detection.
Image reconstruction is particularly difficult when the type of image degradations are unknown. This may be the case if the acquisition device is unknown or the images stem from an uncontrolled environment like the internet. Yet, it may be important to reconstruct a specific piece of information from the image, such as digits from signs or vehicle license plates. Existing works incorporate such prior information with a sequential super-resolution and classification pipeline. However, this approach is prone to error propagation. In this work, we propose a new approach of connecting classification and super-resolution in parallel within a multi-task network. We show that this architecture is able to preserve structures and to remove noisy pixels although the network itself has never been trained on noisy data. We also show that this design allows to transparently trade classification and super-resolution quality. On upsampling by factor 4, we outperform sequential approaches in terms of SSIM by 10% and improve classification by 69%.
Abusers increasingly use spyware apps, account compromise, and social engineering to surveil their intimate partners, causing substantial harms that can culminate in violence. This form of privacy violation, termed intimate partner surveillance (IPS), is a profoundly challenging problem to address due to the physical access and trust present in the relationship between the target and attacker. While previous research has examined IPS from the perspectives of survivors, we present the first measurement study of online forums in which (potential) attackers discuss IPS strategies and techniques. In domains such as cybercrime, child abuse, and human trafficking, studying the online behaviors of perpetrators has led to better threat intelligence and techniques to combat attacks. We aim to provide similar insights in the context of IPS. We identified five online forums containing discussion of monitoring cellphones and other means of surveilling an intimate partner, including three within the context of investigating relationship infidelity. We perform a mixed-methods analysis of these forums, surfacing the tools and tactics that attackers use to perform surveillance. Via qualitative analysis of forum content, we present a taxonomy of IPS strategies used and recommended by attackers, and synthesize lessons for technologists seeking to curb the spread of IPS.
Phishing attack is a cybercrime that can lead to severe financial losses for Internet users and entrepreneurs. Typically, phishers are fond of using fuzzy techniques during the creation of a website. They confuse the victim by imitating the appearance and content of a legitimate website. In addition, many websites are vulnerable to phishing attacks, including financial institutions, social networks, e-commerce, and airline websites. This paper is an extension of our previous work that leverages the favicon with Google image search to reveal the identity of a website. Our identity retrieval technique involves an effective mathematical model that can be used to assist in retrieving the right identity from the many entries of the search results. In this paper, we introduced an enhanced version of the favicon-based phishing attack detection with the introduction of the Domain Name Amplification feature and incorporation of addition features. Additional features are very useful when the website being examined does not have a favicon. We have collected a total of 5,000 phishing websites from PhishTank and 5,000 legitimate websites from Alexa to verify the effectiveness of the proposed method. From the experimental results, we achieved a 96.93% true positive rate with only a 4.13% false positive rate.
Cybersecurity controls are deployed to manage risks posed by malicious behaviours or systems. What is not often considered or articulated is how cybersecurity controls may impact legitimate users (often those whose use of a managed system needs to be protected, and preserved). This oversight characterises the 'blunt' nature of many cybersecurity controls. Here we present a framework produced from consideration of concerns across methods from cybercrime opportunity reduction and behaviour change, and existing risk management guidelines. We illustrate the framework and its principles with a range of examples and potential applications, including management of suspicious emails in organizations, and social media controls. The framework describes a capacity to improve the precision of cybersecurity controls by examining shared determinants of negative and positive behaviours in a system. This identifies opportunities for risk owners to better protect legitimate users while simultaneously acting to prevent malicious activity in a managed system. We describe capabilities for a novel approach to managing sociotechnical cyber risk which can be integrated alongside elements of typical risk management processes. This includes consideration of user activities as a system asset to protect, and a consideration of how to engage with other stakeholders in the identification of behaviours to preserve in a system.
Prevention of cybercrime is one of the missions of Law Enforcement Agencies (LEA) aiming to protect and guarantee sovereignty in the cyberspace. In this regard, online sex crimes are among the principal ones to prevent, especially those where a child is abused. The paper at hand proposes C3-Sex, a smart chatbot that uses Natural Language Processing (NLP) to interact with suspects in order to profile their interest regarding online child sexual abuse. This solution is based on our Artificial Conversational Entity (ACE) that connects to different online chat services to start a conversation. The ACE is designed using generative and rule-based models in charge of generating the posts and replies that constitute the conversation from the chatbot side. The proposed solution also includes a module to analyze the conversations performed by the chatbot and calculate a set of 25 features that describes the suspect's behavior. After 50 days of experiments, the chatbot generated a dataset with 7199 profiling vectors with the features associated to each suspect. Afterward, we applied an unsupervised method to describe the results that differentiate three groups, which we categorize as indifferent, interested, and pervert. Exhaustive analysis is conducted to validate the applicability and advantages of our solution.
Today, the Internet covers worldwide. All over the world, people prefer an E-commerce platform to buy or sell their products. Therefore, cybercrime has become the center of attraction for cyber attackers in cyberspace. Phishing is one such technique where the unidentified structure of the Internet has been used by attackers/criminals that intend to deceive users with the use of the illusory website and emails for obtaining their credentials (like account numbers, passwords, and PINs). Consequently, the identification of a phishing or legitimate web page is a challenging issue due to its semantic structure. In this paper, a phishing detection system is implemented using deep learning techniques to prevent such attacks. The system works on URLs by applying a convolutional neural network (CNN) to detect the phishing webpage. In paper vertical bar 19 vertical bar the proposed model has achieved 97.98% accuracy whereas our proposed system achieved accuracy of 98.00% which is better than earlier model. This system doesn't require any feature engineering as the CNN extract features from the URLs automatically through its hidden layers. This is other advantage of the proposed system over earlier reported in vertical bar 19 vertical bar as the feature engineering is a very time-consuming task.
In our current society, the role of IT&C has become and is becoming increasingly prevalent in all activity fields: medicine, pharmacy, industry, finance - banking, public administration, education, research, aeronautics, national security. The transfer of a substantial volume of the daily activity in the IT&C field inherently imposed the transfer of an equivalent volume of information in the same informatics environment, which has led to the emergence and expansion of the IT&C criminal phenomenon. Due to the immense negative impact they have in terms of the degree of expansion, of the amount of damages, of the difficulty of investigating offences belonging to the cybercrime field, of the difficulty of locating and identifying the offenders, as well as due to the transnational character of these activities, offences pertaining to the informatics criminality are regarded in the new criminal legislation as being on the same level of gravity as genocide against humanity and war crimes, offences against national security, against the fighting capacity of the armed forces, offences relating to the Romanian state border, offences relating civil aeronautics activities and those that may endanger the safety of flights and aviation security, offences against the financial interests of the European Union [1].
Biometrics-based authentication systems are presented as an alternative solution to the traditional authentication techniques, these systems have more security advantages compared to a password as biometric data cannot be forgotten or wasted. However, the protection of biometric systems against different types of attacks is not yet guaranteed. While many studies are proposed in order to increase the security of biometric systems, several attacks are developed in order to exploit the weaknesses of these systems. In this paper, we present a new alteration attack on biometric authentication systems. We suppose that the impostor has altered image of the real user and he presents it as request in order to gain unlawful access to the system. Altered image can be recuperated from a fingerprint trace in case of fingerprint based systems or user photograph for attacking face based authentication systems. Next, we study the impact of alteration level on the matching score for fingerprint and face based biometric systems. Our analysis shows that both systems are vulnerable to the proposed attack and the alteration level has serious impact on the security of biometric systems. The impostor can have a matching score greater than 90% for the tested fingerprint based biometric system and 190/194 associations are matched for the tested face based biometric authentication system.
In the context of contemporary data, the processing of information is crucial. This paper proposes an extension to the traditional database relational algebra, which enriches the data model and provides additional complex-data operations. Specifically, we focus on analytical operators from the areas of data mining and similarity search, such as frequent pattern mining or similarity search queries. The proposed approach can be easily extended by additional algebraic operators. To demonstrate the capabilities of our analytical algebra, we show three practical use cases with different levels of the expression complexity.
With the increasing adoption of microservice architecture and popularity of Platform as a Service (PaaS) cloud, software architecture design is in many domains leaning towards composition of loosely interconnected services hosted in the PaaS cloud, which in comparison to traditional multitier applications introduces new design challenges that software architects need to face when aiming at high scalability and resilience. In this paper, we study the key design decisions made during microservice architecture design and deployment in PaaS cloud. We identify major challenges of microservice architecture design in the context of the PaaS cloud, and examine the effects of architectural tactics and design patterns in addressing them. We apply selected tactics on a sample e-commerce application, constituting of microservices operated by Azure Service Fabric and utilizing other supportive PaaS cloud services within Microsoft Azure. The impact of the examined design decisions on the throughput, response time and scalability of the analyzed application is evaluated and discussed.
The purpose of email spam is to advertise to sell, phishing attacks, DDOS attacks and many more. Many solutions of various kinds such as blacklisting, whitelisting, grey-listing, content filtering have been proposed at the sender and receiver levels. There has been some level of success but the email spam still hits the inbox and more so the problem is false positives and false negatives. The current filtering solutions used are mostly a combination of few of the available techniques, the most common being a combination of listing and content filtering techniques. Apart from any attacks, email spam causes a great loss in resources and productivity of the user. Apart from this, this problem of false negatives exists as there is non-existence of filters to address user's preferences and behaviors, timing of the day and year. The filters at the mail servers are trained with spam and ham training data that is generic in nature. Hence, there is need to address this problem. This paper aims to address all of the above i.e. an email that hits a particular users inbox by escaping the mail server spam filtering solutions. To do this, this paper describes the problem of spam, followed by the filtering mechanisms, techniques, learning email filter model and then proposes a model to fine tune the filter to increase the efficiency.
With the growing use of IoT devices in the private domain, people's homes become increasingly connected to and reachable via the Internet. This is convenient, but also makes homes vulnerable and introduces security risks that go beyond classical IT issues. Cyber threats evolve to cyberphysical ones, opening new pathways for cybercriminals. In practice, most cybercrime attacks happen because of the failure to recognize and deal with (technical as well as human) security gaps and operating errors. There is no lack of technological means and advice to ensure secure operation of systems, but it is usually people with technical background who define these concepts only/mainly based on technical reasoning. The actual users are rarely figured in, which increases the probability that security measures are not practical. This paper reports on a multidisciplinary study on the use of smart devices as well as perception of and experience with Internet security in Austrian households. Contrary to usual engineering approaches, the study was designed to be quantitative and representative according to social science research principles. It revealed that the use of smart devices in private homes is still relatively low and that security incidents are less frequent than might be suspected, but that users also have an ambivalent perception of (in)security.
The security of information is among the greatest challenges facing organizations and institutions. Cybercrime has risen in frequency and magnitude in recent years, with new ways to steal, change and destroy information or disable information systems appearing every day. Among the types of penetration into the information systems where confidential information is processed is malware. An attacker injects malware into a computer system, after which he has full or partial access to critical information in the information system. This paper proposes an ensemble classification-based methodology for malware detection. The first-stage classification is performed by a stacked ensemble of dense (fully connected) and convolutional neural networks (CNN), while the final stage classification is performed by a meta-learner. For a meta-learner, we explore and compare 14 classifiers. For a baseline comparison, 13 machine learning methods are used: K-Nearest Neighbors, Linear Support Vector Machine (SVM), Radial basis function (RBF) SVM, Random Forest, AdaBoost, Decision Tree, ExtraTrees, Linear Discriminant Analysis, Logistic, Neural Net, Passive Classifier, Ridge Classifier and Stochastic Gradient Descent classifier. We present the results of experiments performed on the Classification of Malware with PE headers (ClaMP) dataset. The best performance is achieved by an ensemble of five dense and CNN neural networks, and the ExtraTrees classifier as a meta-learner.
In this new era of life, where technology covers our life from the early morning to late night, cybercrime is becoming more developed and challenging for the systems designers. The reason is reflected in the increased number of ways used by criminals. Cloud computing systems are natural goals due to their complexity and increasing popularity. The Cloud system provides an environment with a big number of Virtual Machines (VMs) that available to many users accessing this system via the Internet. This way of access makes cloud systems weaker than physical networks. In order to reduce the number of attacks and secure data storage, any malicious behaviour should be discovered and halted if possible. In this paper, we focus on discovery of malicious behaviour via determining unwanted symptoms rather than via targeting particular malicious behaviour of the system directly. The main motivation for our approach is that malicious behaviour (e.g., a new form of threat) is very often hard to specify directly, but it can be characterized by a set of undesired symptoms. The main contribution of this paper refers to several new mechanisms for monitoring Virtual Machines and further experimental work targeting efficient ways of visiting VMs in order to discover malicious symptoms.
This study is analyzing the relationship between traditional money laundry scheme and the electronic payments development and also the contribution of the Internet development in promoting illegal transactions originated from domestic or foreign markets. The links between terrorism financing, money laundering, cybercrime and traditional criminal activity seems to be clear. In this context, the paper is focusing on the potential money laundering and terrorist financing implications of payment innovations that gave customers the opportunity to carry out payments directly through technical devices such as personal computers, mobile phones or data storage cards. In many cases these payments could be carried out without Me customer needing an individual bank account. The research tries to identify money laundering typologies and red flags indicators that may occur in electronic payments system in order to create behaviour maps, models and suspicious activity rule bases for each of them. Moreover, the study tried to create a link between the theoretical issues and the practical schemes of electronic money laundry. Organized crime, arms smuggling, tax evasion and fraud in its many varieties entail significant laundering of funds. It remains difficult to assess the scale of the money laundering problem, but there is general agreement that it amounts of two to five percent of global GDP would probably be a consensus range.
The proliferations of the mobile intelligent terminal have brought them to the spotlight of the attackers for the sensitive information they carried. However, the current security schemes on the mobile intelligent terminal are fragile. This paper proposed a new transparent authentication and encryption measure to protect the confidentiality and integrity of the data on the mobile intelligent terminal. Our method is user-friendly as no interaction is needed during the process. Also, the result can be applied as evidence to identify who have used the phone.
In an era in which social media platforms are proliferating and becoming primary communication channels, the identification of evidence for crimes from such platforms is crucial for digital forensics and legal proceedings. This paper presents a novel approach for systematically structuring and categorising digital attributes that are interlinked across social media platforms using digital ontologies, as well as a method for user profiling using domain-specific digital artefacts. The ontology models consist of classes with subclass distinctions for text, image, and video types of evidence. These models are flexible and can be expanded to include various social media platforms and evidence categories. Simultaneously, the user profiling method employs mathematical formulas and visual representations to develop comprehensive profiles of individuals based on extracted social media data. This methodology evaluates the relevance of a set of digital artefacts and related attributes, such as interests, location, and activities, using their weights. Additionally, the research addresses the legal and ethical considerations pertinent to the collection of data from social media. Despite the approaches' immense potential for expediting evidence collection and developing insightful profiles, obstacles such as scalability, legal complexities, and data noise are identified. This work makes a substantial contribution to the development of digital forensics and cybercrime investigations involving social media platforms.
The emergence of the Internet and web technologies has magnified the occurrence of disinformation events and the dissemination of online fake news items. Fake news is a phenomenon where fake news stories are created and propagated online. Such events occur with ever increasing frequency, they reach a wide audience, and they can have serious real-life consequences. As a result, disinformation events are raising critical public interest concerns as in many cases online news stories of fake and disturbing events have been perceived as being truthful. However, even at a conceptual level, there is not a comprehensive approach to what constitutes fake news with regard to the further classification of individual occurrences and the detection/mitigation of actions. This work identifies the emergent properties and entities involved in fake news incidents and constructs a disinformation blueprint (DCAM-DB) based on cybercrime incident architecture. To construct the DCAM-DB in an articulate manner, the authors present an overview of the properties and entities involved in fake news and disinformation events based on the relevant literature and identify the most prevalent challenges. This work aspires to enable system implementations towards the detection, classification, assessment, and mitigation of disinformation events and to provide a foundation for further quantitative and longitudinal research on detection strategies.
The advent of the Android system has brought smartphone technology to the doorsteps of the masses. The latest technologies have made it affordable for every section of the society. However, the emergence of the Android platform has also escalated the growth of cybercrime through the mobile platform. Its open source operating system has made it a center of attraction for the attackers. This article provides a comprehensive study of the state of the Android Security domain. This article classifies the attacks on the Android system in four categories (i) hardware-based attacks, (ii) kernel-based attacks, (iii) hardware abstraction layer-based attacks, and (iv) application-based attacks. The study deals with various threats and security measures relating to these categories and presents an in-depth analysis of the underlying problems in the Android security domain. The article also stresses the role of Android application developers in realizing a more secure Android environment. This article attempts to provide a comparative analysis of various malware detection techniques concerning their methods and limitations. The study can help researchers gain knowledge of the Android security domain from various aspects and build a more comprehensive, robust, and efficient solution to the threats that Android is facing.
Phishing is a cybercrime in which, attackers try to fraudulently retrieve users' credentials by mimicking trusted communication channels. The problem with phishing is that attackers still able to bypass anti-phishing automated systems through the human factor. It is not enough, therefore, to only add new technologies, aware users might play the key role in stopping phishing attacks. Based on that, phishing problem requires defense solutions that to be applied at both of the technical (automated systems) and non-technical (human) aspects. Phishing attacks, in general, are initiated through simulated emails with a false claim of being sent from trusted parties. The work in this paper is dedicated to fighting phishing threats at email's level in order to kill this type of attacks in the cradle. Users, therefore, are protected at a level which is prior of browsing phishing web pages. This paper proposes an anti-phishing model that designed based on the general taxonomy of the technical and non-technical aspects of phishing detection approaches. This paper, in addition, presents the general structure of the proposed anti-phishing system that developed based on the herein proposed model. The novelty of this model is the approach of combining both of the automated procedures with users' anti-phishing training method to detect phishing emails.
The anti-forensics (AF) technology has become a new field of cybercrime. The problems of existing forensic technologies should be considered from criminals' perspective, so as to make improvement to existing AF technologies. There are two types of AF methods, namely, data hiding and destruction, where most AF tools are primarily based on data hiding. If the data can be intercepted by investigators during the AF process, the remaining data may be destroyed by the criminal, which would make investigators obtain nothing about data information. To address this issue, this paper proposes an AF scheme with multi-device storage based on Reed-Solomon codes by combining data hiding and data destruction. The data is divided into multiple out-of-order data blocks and parity blocks, where these blocks are stored separately in different devices. This method can reduce the storage cost and protect the privacy of data. Even if the data is destroyed, it allows AF investigators to recover the data. Security analysis showed that this AF method can prevent malicious, erroneous or invalid files while acquired and ensure data security in data stolen. Theoretical analysis indicated that this method was difficult for investigators but easy for AFer in files recovery. Experimental results demonstrated that the proposed method is effective and has practical efficiency.
"In this study we report our research on learning an accurate and easily interpretable classifier model for authorship classification of typewritten digital texts. For this purpose we use Ant Colony Optimization; a meta-heuristic based on swarm intelligence. Unlike black box type classifiers, the decision making rules produced by the proposed method are understandable by people familiar to the domain and can be easily enhanced with the addition of domain knowledge. Our experimental results show that the method is feasible and more accurate than decision trees."
Machine learning (ML) is currently a crucial tool in the field of cyber security. Through the identification of patterns, the mapping of cybercrime in real time, and the execution of in-depth penetration tests, ML is able to counter cyber threats and strengthen security infrastructure. Security in any organization depends on monitoring and analyzing user actions and behaviors. Due to the fact that it frequently avoids security precautions and does not trigger any alerts or flags, it is much more challenging to detect than traditional malicious network activity. ML is an important and rapidly developing anomaly detection field in order to protect user security and privacy, a wide range of applications, including various social media platforms, have incorporated cutting-edge techniques to detect anomalies. A social network is a platform where various social groups can interact, express themselves, and share pertinent content. By spreading propaganda, unwelcome messages, false information, fake news, and rumours, as well as by posting harmful links, this social network also encourages deviant behavior. In this research, we introduce Deep Belief Network (DBN) with Triple DES, a hybrid approach to anomaly detection in unbalanced classification. The results show that the DBN-TDES model can typically detect anomalous user behaviors that other models in anomaly detection cannot.
The world of Internet as such is open and insecure by nature. Companies and organizations exploit enormous possibilities that internet offers to build computer systems to enable communication and data sharing capabilities within their corporate platforms. In doing so, they continuously strive towards providing a fast, efficient and at the same time secure working environment by protecting their organizational assets. Enterprises build their network infrastructure with intention to find reliable solutions to protect themselves from untrusted and cybercrime activities. In this sense, Virtual Private Networks (VPN) are primarily concerned about Data privacy. VPNs represent an extension of a private network made through added features like encapsulating the data packets with a header on both ends, along the lines of the communication as well as throughout setting communication tunnels using composite suite of protocols available. This paper offers a set of simulated secure data communication tunnels together with a comparison of results of the speed variables measured against the security through different encryption protocols between remote LAN's. These encryption protocols are ran onto distributed queries using various database functions. (C) 2015 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).
Cybercrime has been the focus of public attention during the last decade. However, within the criminological field, no prior research initiatives have been launched in an effort to better understand this phenomenon using computer network data. Addressing this challenge, we employ the classical routine-activities and lifestyle perspective to raise hypotheses regarding the trends and origin of computer-focused crime incidents (i.e. computer exploits, port scans, and Denial of Service (DoS) attacks) against a large university computer network. We first propose that computer-focused crimes against a university network are determined by the university users' daily activity patterns. In addition, we hypothesize that the social composition of the network users determines the origin of computer attacks against the university network. We use data recorded between the years 2007 and 2009 by an Intrusion Prevention System (IPS) to test these claims. Consistently with our theoretical expectations, two important findings emerge. First, computer attacks are more likely to occur during university official business hours. Second, an increase in the number of foreign network users substantially increases the number of computer-focused crimes originating from Internet Protocol (IP) addresses linked with these users' countries of origin. Future directions for subsequent studies are discussed.
This research proves the failure to address theoretically fundamental issues of robots' legal capacity and cyber security and as a result crudity of issues concerning criminal liability of robots for their actions. The proposals of a number of Spanish scientists on the possibility of non-proliferation of the sphere of criminal law on robots in connection with the existing possibility of criminal prosecution of legal entities in Spain have been worked out. In retrospect, the Spanish concepts of criminal responsibility of artificial intelligence were studied: their novels, shortcomings and problems of application in modern conditions were revealed.
Email is an essential communication tool for modern people and offers a variety of functions. After the outbreak of COVID-19, the importance of emails enhanced further as non-face-to-face work increased. However, with the spread and dissemination of emails, cybercrime that abused emails has also increased. The number of cases of stealing or damaging email users by impersonating public institutions such as the National Police Agency, the Prosecutor's Office, or the WHO. This study proposes an advanced algorithm of email classification using an SMTP response code to strengthen the level of email security. The proposed system is located on the side of the recipient's email server and operates upon receipt of the email. When an email is received, it automatically verifies whether the domain of the email sender is normally registered in DNS. Thereafter, MX, SPF, and PTR records are extracted and combined to determine the state of the sending server. When additional verification is required, a proposed algorithm automatically connects the communication session to the sender to request the SMTP response code. The proposed algorithm was applied to two organizations and succeeded in classifying received emails into various categories. This study contributes to the literature on email classification by presenting new ideas in the process of sender verification.
With the emergence of blockchain-based multi-party trading scenarios, such as finance, government work, and supply chain management. Information on the blockchain poses a serious threat to users' privacy, and anonymous transactions become the most urgent need. At present, solutions to the realization of anonymous transactions can only achieve a certain degree of trader identity privacy and transaction content privacy, so we introduce zero knowledge proof to achieve complete privacy. At the same time, unconditional privacy provides conditions for cybercrime. Due to the great application potential of the blockchain in many fields, supporting privacy protection and supervision simultaneously in the blockchain is a bottleneck, and existing works can not solve the problem of coexistence of privacy protection and supervision. This paper takes the lead in studying the privacy and supervision in multi-party anonymous transactions, and proposes a distributed anonymous payment scheme with supervision (DAPS) based on zk-SNARK, signature, commitment and elliptic curve cryptography, which enables users to be anonymous under supervision in transactions. The advantages of DAPS are twofold: enhanced privacy and additional supervision. We formally discussed the security of the whole system framework provided by the zero-knowledge proof, and verified its feasibility and practicability in the open source blockchain framework BCOS.
"To controlling the information flows between the competing companies (CC) in the cloud computing (Multi-tenancy) or in the social network, the Chinese wall security policy (CWSP) is very interesting solution. Its rule is the building of the walls between the CC. The CWSPs idea, is the building of impenetrable wall among subject, and this to controlling the information flow between the CC, caused by the subjects. However, the preview proposed models [1-6]; they have a set of errors and they can't fix the composite information flows (CIF) problem between the CC (a malicious Trojan horses problem). In this article; we will propose a new approach based on the access query type of the subject to the objects and the philosophy of the CWSP. In our model we have two types walls placement, the first are built around the subject, and the second around the object; which, we can't find inside each once wall two competing objects data."
Cryptocurrency abuse has become a critical problem. Due to the anonymous nature of cryptocurrency, criminals commonly adopt cryptocurrency for trading drugs and deceiving people without revealing their identities. Despite its significance and severity, only few works have studied how cryptocurrency has been abused in the real world, and they only provide some limited measurement results. Thus, to provide a more in-depth understanding on the cryptocurrency abuse cases, we present a large-scale analysis on various Bitcoin abuse types using 200,507 realworld reports collected by victims from 214 countries. We scrutinize observable abuse trends, which are closely related to real-world incidents, to understand the causality of the abuses. Furthermore, we investigate the semantics of various cryptocurrency abuse types to show that several abuse types overlap in meaning and to provide valuable insight into the public dataset. In addition, we delve into abuse channels to identify which widely-known platforms can be maliciously deployed by abusers following the COVID-19 pandemic outbreak. Consequently, we demonstrate the polarization property of Bitcoin addresses practically utilized on transactions, and confirm the possible usage of public report data for providing clues to track cyber threats. We expect that this research on Bitcoin abuse can empirically reach victims more effectively than cybercrime, which is subject to professional investigation.
Online Social Networks (OSNs) such as Twitter and Facebook have become a significant testing ground for Artificial Intelligence developers who build programs, known as socialbots, that imitate actual users by automating their social-network activities such as forming social links and posting content. Particularly, Twitter users have shown difficulties in distinguishing these socialbots from the human users in their social graphs. Frequently, legitimate users engage in conversations with socialbots. More impressively, socialbots are effective in acquiring human users as followers and exercising influence within them. While the success of socialbots is certainly a remarkable achievement for AI practitioners, their proliferation in the Twitter-sphere opens many possibilities for cybercrime. The proliferation of socialbots in the Twitter-sphere motivates us to assess the characteristics or strategies that make socialbots most likely to succeed. In this direction, we created 120 socialbot accounts in Twitter, which have a profile, follow other users, and generate tweets either by reposting messages that others have posted or by creating their own synthetic tweets. Then, we employ a 2 k factorial design experiment in order to quantify the infiltration effectiveness of different socialbot strategies. Our analysis is the first of a kind, and reveals what strategies make socialbots successful in the Twitter-sphere.
Internet of Things (IoT) is envisaged as a transformative approach that provides any service over the Internet by enabling communications through the interconnection of heterogeneous devices. The potential threats and possible attacks against IoT devices have grown drastically and the cybercrime statistics from the South African Banking Risk Information Centre (SABRIC) shows that South Africans lose in excess of R2.2 billion due to the security vulnerabilities annually. Unfortunately, the security requirements are not yet well addressed as it is still in its infant stage. Therefore, before there is a widespread adoption of IoT, there needs to be stringent security and reliability in the IoT space. The proposed research aims to close this gap through the implementation of a user-centric and automated IoT testbed in context with a setting that is close to the real-world South African environment to address the specific IoT vulnerabilities in the country. Because of the limited amount of work performed so far in this field of study, this research will not only contribute in filling the missing gap in the area of IoT security vulnerabilities, but will also contribute towards a better understanding of the subject matter and uptake of the IoT technology.
The metric space model of similarity has become a standard formal paradigm of generic similarity search engine implementations. However, the constraints of identity and symmetry prevent from expressing the subjectivity and dependence on the context perceived by humans. In this paper, we study the suitability of the Distance density model of similarity for searching. First, we use the Local Outlier Factor (LOF) to estimate a data density in search collections and evaluate plenty of queries using the standard geometric model and its extension respecting the densities. We let 200 people assess the search effectiveness of the two alternatives using the web interface. Encouraged by the positive effects of the Distance density model, we propose an alternative way to estimate the data densities to avoid the quadratic LOF computation complexity with respect to the dataset size. The sketches with unbalanced bits are clarified to be in correlation with LOFs, which opens a possibility for an efficient implementation of large-scale similarity search systems based on the Distance density model.
Understanding the processes in education, such as the student learning behavior within a specific course, is a key to continuous course improvement. In online learning systems, students' learning can be tracked and examined based on data collected by the systems themselves. However, it is non-trivial to decide how to extract the desired students' behavior from the limited data in traditional classroom courses. Software development courses are a domain where student behavior analysis would be especially useful, as continuous teaching improvement in this fast progressing domain is necessary. In this paper, we propose to use process mining for improvement-motivated process analysis of a software development course (web development in particular). To this end, we analyze Git logs of students' projects to understand their development processes. Process mining has been chosen as it can help us to find a descriptive model of this process. The main contribution of this paper is the detailed methodology of process mining usage for students' project development analysis, considering various commit characteristics, which are crucial in understanding student coding-behavior patterns. The process mining analysis proved to be very useful, indicating multiple directions for the course improvement, which we also include in this work as a secondary contribution. The third contribution of this work is the summary and discussion of the process mining advantages and current gaps in process mining research for this task. The data we used are made publicly available to other researchers.
In this paper, we describe a system for the continuous collection of data for the needs of network security management. When a cybersecurity incident occurs in the network, the contextual information on the involved assets facilitates estimating the severity and impact of the incident and selecting an appropriate incident response. We propose a system based on the combination of active and passive network measurements and the correlation of the data with third-party systems. The system enumerates devices and services in the network and their vulnerabilities via fingerprinting of operating systems and applications. Further, the system pairs the hosts in the network with contacts on responsible administrators and highlights critical infrastructure and its dependencies. The system concentrates all the information required for common incident handling procedures and aims to speed up incident response, reduce the time spent on the manual investigation, and prevent errors caused by negligence or lack of information.
Motion capture technologies digitize human movements by tracking 3D positions of specific skeleton joints in time. Such spatio-temporal data have an enormous application potential in many fields, ranging from computer animation, through security and sports to medicine, but their computerized processing is a difficult problem. The recorded data can be imprecise, voluminous, and the same movement action can be performed by various subjects in a number of alternatives that can vary in speed, timing or a position in space. This requires employing completely different data-processing paradigms compared to the traditional domains such as attributes, text or images. The objective of this tutorial is to explain fundamental principles and technologies designed for similarity comparison, searching, subsequence matching, classification and action detection in the motion capture data. Specifically, we emphasize the importance of similarity needed to express the degree of accordance between pairs of motion sequences and also discuss the machine-learning approaches able to automatically acquire content-descriptive movement features. We explain how the concept of similarity together with the learned features can be employed for searching similar occurrences of interested actions within a long motion sequence. Assuming a user-provided categorization of example motions, we discuss techniques able to recognize types of specific movement actions and detect such kinds of actions within continuous motion sequences. Selected operations will be demonstrated by on-line web applications.
"Mining opinion on social media microblogs presents opportunities to extract meaningful insight from the public from trending issues like the yahoo-yahoo which in Nigeria, is synonymous to cybercrime. In this study, content analysis of selected historical tweets from yahoo-yahoo hashtag was conducted for sentiment and topic modelling. A corpus of 5500 tweets was obtained and pre-processed using a pre-trained tweet tokenizer while Valence Aware Dictionary for Sentiment Reasoning (VADER), Liu Hu method, Latent Dirichlet Allocation (LDA), Latent Semantic Indexing (LSI) and Multidimensional Scaling (MDS) graphs were used for sentiment analysis, topic modelling and topic visualization. Results showed the corpus had 173 unique tweet clusters, 5327 duplicates tweets and a frequency of 9555 for yahoo. Further validation using the mean sentiment scores of ten volunteers returned R and R2 of 0.8038 and 0.6402; 0.5994 and 0.3463; 0.5999 and 0.3586 for Human and VADER; Human and Liu Hu; Liu Hu and VADER sentiment scores, respectively. While VADER outperforms Liu Hu in sentiment analysis, LDA and LSI returned similar results in the topic modelling. The study confirms VADER's performance on unstructured social media data containing non-English slangs, conjunctions, emoticons, etc. and proved that emojis are more representative of sentiments in tweets than the texts."
"Only recently have scholars of criminology begun to examine a wider spectrum of the effects of digital technologies beyond 'cybercrime' to include human rights, privacy, data extractivism and surveillance. Such accounts, however, remain anthropocentric and capitalocentric. They do not fully consider the environmental impacts caused by the manufacture, consumption, use and disposal of digital technologies under conditions of ecologically unequal exchange. The worst impacts of extractivism and pollution are borne by societies and ecosystems in the world's economic periphery and contribute to an acceleration of planetary ecocide. Three examples illustrate our argument: (1) deep-sea mining of metals and minerals; (2) the planned obsolescence of digital devices while limiting the right to repair; and (3) the disposal of e-waste. Acknowledging the urgent need to reorient the trajectory of technology innovation towards more-than-human futures, we advance some ideas from the field of design research-that is, the field of scholarly inquiry into design practices-on how to decouple technological progress from neoliberal economic growth. We venture outside criminology and offer a glimpse into how design researchers have recently begun a similar reflective engagement with post-anthropocentric critiques, which can inspire new directions for research across digital and green criminology."
At recent time, cybercrime has increased significantly in the world and also in Bangladesh due to consequence of massive digitalization efforts in every sector. These types of offense require systematic probing in order to be revealed properly at the court of law. This research work proposed a comprehensive digital forensics framework to facilitate forensically ready common standard digital forensics to the investigators. The framework recommended proactive and reactive forensics to enable live and post forensics issues. The purpose of the proposed framework is to allow systematic postmortem of digital forensics. Moreover, it allows further investigations suggested during examination or analysis phase. An expert system is introduced into the framework to improve efficiency through inference engine. This assistive system included digital forensics case database as a history keeper to store all documentations and results of the accomplished forensics examination. The inference engine is expected to generate a priority list from the historical data by a proposed algorithm. In addition, the expert system is expected to guide examiners during forensics. Furthermore, dissemination of forensics outcomes anticipated automatically to the authorized investigators by this system. However, the proposed framework is based on theoretical approach which is expected to be evaluated and implemented using assistive expert system in the future.
In recent years, understanding the people behind cybercrime from a hacker-centric perspective has drawn increased attention. Preliminary exploration in online hacker social dynamics has found that hackers extensively exchange information with others in online communities, including vulnerabilities, stolen data, etc. However, there is a lack of research that explores automated identification and characterization of expert hackers within online communities. In this research, we identify expert hackers and characterize their specialties by devising a scalable and generalizable framework leveraging two categories of features to analyze hacker forum content. The framework encompasses text analytics for key hacker identification and analysis. In the Text Analytics module, we employ an interaction coherence analysis (ICA) framework, to extract interactions among the users in hacker communities as topological feature. In Expert Identification & Analysis, we characterize each hacker with content features extracted with lexicon matching and structural features from the ICA component. Results reveal an interaction network and content-based clustering of key actors within the studied hacker community. Our project contributes to both social media analytics and cybersecurity research as we provide a complete analytical framework to analyze the key hackers from both an interaction network perspective and discussion content perspective. This framework can benefit cybersecurity researchers and practitioners by offering an inclusive angle for analyzing hacker social dynamics.
Incident handling, a fundamental activity of a cybersecurity incident response team, is a complex discipline that consumes a significant amount of personnel's time and costs. There are continuous efforts to facilitate incident handling and response in terms of providing procedural or decision support and processing relevant data. In this paper, we survey the approaches towards (semi-)automated incident handling and response backed by recommender systems that are successful in other domains. We discuss which phases and tiers of incident handling can be automated and to what level while evaluating the maturity of proposed approaches and tools. While we did not find a full-scale recommender system that would guide the user through incident handling and suggest which steps to take, many of them aim at particular problems. The discussed issues are not resolved yet but seem to get the attention of researchers and will likely be investigated in the future.
The current study provides an empirical testing of the victim-offender overlap in online platforms due to the scarcity of studies examining this overlapping victim-offending dynamic. Two types of cyber-interpersonal violence are examined: Cyber-harassment (including cyber-sexual harassment) and cyber-impersonation. Using Choi's (2008) integrated theory of Cyber-Routine Activities Theory, a sample of 272 college students at a Massachusetts university are examined. Three major findings are revealed: (1) Respondents who engage in risky online leisure activities are more likely to experience interpersonal violence in cyberspace, (2) poor online security management can contribute to the likelihood of being victimized by interpersonal violence on social networking sites (SNS), and (3) respondents who engage in risky social networking site activities are likely to commit cyber-interpersonal violence. For the two types of cyber-interpersonal violence examined in this study, it could also be predicted that females are more likely to have higher levels of victimization. Cybersecurity management and sex had no significant effects on cyber-interpersonal violence offending. The hope is that education on the potential hazards of the Internet and of cyber-interpersonal violence will induce more responsible online activity and engagement. Published by Elsevier Ltd.
"Law develops along with technological developments; hence both cannot be separated. In the current era, technology is developing very fast and often humans need to be protected from its uncontrollable use. This is where the law emerges to provide oversight and protection against violations created by technology. One of such technologies that violate legislation is deep fake technology. It is widely used in the creative industry which is fast emerging in Indonesia, and is closely related to the use of copyrights, which are facing a great threat from deepfake in the form of copyright violations. This study used normative juridical research methods to examine criminal acts in deepfakes. Several laws and legal studies were analyzed to identify factors regarding violations that occur when using deepfake technology in cyberspace. It was evident in the findings that deepfake is responsible for violation of the intellectual property rights, mainly the copyrights of the creator of works of art, and thus categorized as cybercrimes. The study treated deepfake as intellectual property violations, and also a cybercrime as there was the use of internet and computer technology and artificial intelligence to commit these crimes. The study implies that intellectual property rights are also like moral rights and economic rights."
"Mainstream media tend to frame media content from the perspectives of locals and seldom in the interest of diasporic communities. However, the emergence of social media has supported the existence of online diasporic communities. Deploying insights from the concepts of alternative media, digital counter public sphere and hidden transcript, this study explores the use of Facebook groups by Zimbabwean diaspora to form transnational online communities and connect with the homeland. This is a netnographic study of selected Facebook groups of Zimbabweans residing abroad. The study shows that since mainstream media in both home and host countries tend to exclude and marginalise interests of diasporic communities, Facebook has become an alternative counter public sphere for diasporic communities. Due to the affordances of social media, networking sites can be expedient platforms for creating diasporic networks, facilitating information circulation and enabling unrestricted and wide-ranging public debate; however, they can be volatile and susceptible to cybercrime and cyberbullying. The study demonstrates conundrums of online-based communities with regard to issues of trust, legitimacy, accountability and regulation. Research Article; Cultural Studies; 60 Arts & Humanities; 60.2.13 Media Communication; 60.3 Humanities; 60.3.6 Cultural Studies; 60.4 Language & Literature; 60.4.1 Language & Linguistics"
The Internet of Things (IoT) is permeating our daily lives, blurring the boundaries between the cyber world and the physical world. With an increased number of identified sources, obtaining potential digital evidence has become a significant aspect of investigations and incident responses. However, multi-level interconnections are bringing new challenges to identifying connected things in IoT ecosystems. This study focuses on IoT services that can connect or be connected to various things to reveal their connected relationship by examining forensic artifacts under six interconnected scenarios. We found that not all services retain traces of interconnection. Based on our findings from scenario-based experiments and existing IoT forensic frameworks, we propose an improved IoT forensic model to identify interconnectivity between things. This framework suggests additional phases for identifying the interconnectivity of things. The proposed framework is designed to investigate IoT-related cases by identifying all the connected things, ensuring relevant data collection, narrowing down areas of investigation, and integrating the data. Our proposed model discovers pieces of connected things, which helps reveal the whole interconnected IoT ecosystems where users are involved in. This approach can also be applied for a thorough understanding of events relating to things and services around a suspect in cybercrime response.
In the Republic of Croatia, electronic signature is subject to the regulation of a statute which regulate the implementation of European legislation on electronic identification and trust services for electronic transactions. The goal of the paper is to identify and estimate the new EU legislative framework for electronic identification and a new system of regulation and supervision for electronic service providers in the EU, which should stimulate electronic commerce and cross-border digital public services. This regulation places particular emphasis on the recognition of electronic identification means between Member States for cross-border authentication, personal data security and the supervision of qualified providers of trust services. Using the normative and institutional analysis of national legal source, as research method in the paper, has helped to identify the typical problems that arise in regulating electronic signature. We find that the results imply that, despite the optimism that using electronic signatures can have a significant impact on e-finance, cybercrime is a growing threat. Commercial operators only become aware that the electronic signature methods they use must be adequate and that certain criminals can even,break the two-degree authentication process. Professional users and consumers needs continuous education to keep pace with the fraudsters.
In a globally interconnected cyber space, an appropriate level of information security (IS) is one of the business requirements for the successful operation of business systems, particularly in the fields of high-tech and finance. These two fields are also the most exposed to abuse and different types of cybercrime. The development of information security policies (ISP) is one of the basic measures for providing adequate levels of IS. These must be appropriately promoted within organisations, which ought to provide relevant training courses for employees in order to raise their awareness and make them understand that they are contributing to safe and secure working processes and thus to the general success of the organisation by complying with prescribed procedures and implementing prescribed working methods consistently. The present research study, which focuses on IS and employees' compliance with ISP, aims to determine the state-of-affairs with respect to IS in Slovene insurance undertakings, the types of in-house actions that are or could be implemented to improve the aforementioned issues, the types of measures that are used to decrease the long-term costs of insurance undertakings' operations. It presents analysed data in the field of insurance, measures for improving the level of IS with the best cost-benefit ratio and individual improvements.
Many criminals exploit the convenience of anonymity in the cyber world to conduct illegal activities. E-mail is the most commonly used medium for such activities. Extracting knowledge and information from e-mail text has become an important step for cybercrime investigation and evidence collection. Yet, it is one of the most challenging and time-consuming tasks due to special characteristics of e-mail dataset. In this paper, we focus on the problem of mining the writing styles from a collection of e-mails written by multiple anonymous authors. The general idea is to first cluster the anonymous e-mail by the stylometric features and then extract the writeprint, i.e., the unique writing style, from each cluster. We emphasize that the presented problem together with our proposed solution is different from the traditional problem of authorship identification, which assumes training data is available for building a classifier. Our proposed method is particularly useful in the initial stage of investigation, in which the investigator usually have very little information of the case and the true authors of suspicious e-mail collection. Experiments on a real-life dataset suggest that clustering by writing style is a promising approach for grouping e-mails written by the same author. (C) 2010 Elsevier Ltd. All rights reserved.
"Recently, hackers intend to reproduce malicious links utilizing several ways to mislead users. They try to control victims' machines or get their data remotely by gaining access to private information they use via cyberspace. QR codes are twodimensional barcodes with the capacity to encode various data types and can be viewed by digital devices, such as smartphones. However, there is no approved protocol in QR code generation; therefore, QR codes might be exposed to several questionable attacks. QR code attacks might be perpetrated using barcodes, and there are some security countermeasures. Some of these solutions are restricted to malicious link detection techniques with knowledge of cryptographic methods. Therefore, this study aims to detect malicious links embedded in 1D (linear) and 2D (QR) codes. A cybercrime attack was proposed based on barcode counterfeiting that can be used to perform online attacks. A dataset of 100000 malicious and benign URLs was created via several resources, and their lexical features were obtained. Analyses were conducted to illustrate how different features and users deal with online barcode content. Several artificial intelligence models were implemented. A decision tree classifier was identified as the most suitable model for identifying malicious URLs. Our outcomes suggested that a secure artificial intelligence barcode scanner (BarAI) is recommended to detect malicious barcode links with an accuracy of 90.243%."
Crisis and disruption are often unpredictable and can create opportunities for crime. During such times, policing may also need to meet additional challenges to handle the disruption. The use of social media by officials can be essential for crisis mitigation and crime reduction. In this paper, we study the use of Twitter for crime mitigation and reduction by UK police (and associated) agencies in the early stages of the Covid-19 pandemic. Our findings suggest that whilst most of the tweets from our sample concerned issues that were not specifically about crime, especially during the first stages of the pandemic, there was a significant increase in tweets about fraud, cybercrime and domestic abuse. There was also an increase in retweeting activity as opposed to the creation of original messages. Moreover, in terms of the impact of tweets, as measured by the rate at which they are retweeted, followers were more likely to 'spread the word' when the tweet was content-rich (discussed a crime specific matter and contained media), and account holders were themselves more active on Twitter. Considering the changing world we live in, criminal opportunity is likely to evolve. To help mitigate this, policy makers and researchers should consider more systematic approaches to developing social media communication strategies for the purpose of crime mitigation and reduction during disruption and change more generally. We suggest a framework for so doing.
Information systems accelerate the advancement of society. However, malicious manipulation of information would bring great harm. Recently, criminal groups are increasingly involved in cybercrime, especially in the Local Area Network (LAN). Several methods are being used to analyze network traffic in LAN such as extracting the transition patterns in traffic flows, however, research on visualization of network traffic, thus detecting and classifying various abnormal events, is insufficient. In this research, we propose visual analytics for generating feature maps of network events based on protocol information. We extract protocol information of ARP, TCP, and UDP from network traffic and generate each type of feature maps. Then for each event, we merge these three types into one image by putting them into different channels, to represent features. We simulate and visualize eight types of network events in LAN which are the normal, arp scan, tcp scan, scan of tcp port 23, scan of tcp port 80, udp scan, scan of udp port 137 and scan of udp port 1900. Then for this multiclass classification problem, we adopt a deep convolutional neural network (CNN) to differentiate between these network events, with these eight types as labels and generated feature maps as inputs. We evaluated the scheme using precision, recall, and F-measure in two LANs, at last, achieving an average F-measure of 0.76.
File type identification (FTI) has become a major discipline for anti-virus developers, firewall designers and for forensic cybercrime investigators. Over the past few years, research has seen the introduction of several classifiers and features. One of these advances is the so-called n-grams analysis, which is an interpretation of statistical counting in classified fragments. Recently, n-grams based approaches were already successfully combined with computational intelligence classifiers. However, the academic body of literature is scant when it comes to a comprehensive explanation of machine learning based approaches such as neural networks (NN) or support vector machines (SVM). For example, how the input parameters, including learning rate, different values of n for n-grams, etc. influence the results. In addition, very few studies have compared the scalability of NN vs. SVM approaches. Therefore, a systematic research in comparing different approaches is needed to address these questions. Hence, this paper investigates this type of comparison, by focusing on the n-gram analysis as a feature for the two different classifiers: SVMs and NNs. This paper details our experiments with two NNs and four SVMs, using linear kernels and RBF kernels on RealDC datasets. In general, we found that SVM-based approaches performed better than the NN, but their scalability is still a challenge. (c) 2021 The Authors. Published by Elsevier Ltd.
Research has shown that psychosocial and behavioral factors are associated with engagement in a range of deviant behaviors across offline settings. To date, however, very little research has explored the impact of these factors in online contexts. This article addresses this gap by examining the psychosocial and behavioral factors associated with common types of adolescent cyberdeviance. This is accomplished through an empirical study of 327 adolescents enrolled in a high school located in a large Australian city. The study assesses various aspects of psychosocial and behavioral functioning using the Strengths and Difficulties Questionnaire (total difficulties, internalizing problems, externalizing problems, prosocial behavior), as well as numerous types of cyberdeviance relevant to young people, including cyberfraud, cyberhate, cyberviolence, sexting, digital piracy, hacking, and cyberbullying. A series of multivariate logistic regression analyses were conducted to demonstrate the association between psychosocial and behavioral difficulties and various types of cyberdeviance, independent of gender, school grade, socioeconomic status, and engagement in offline delinquency. Results indicate that total difficulties, internalizing problems, and externalizing problems were significantly associated with greater likelihood of engagement in most types of cyberdeviance examined in this study, whereas prosocial behavior was associated with a lower likelihood of engagement in digital piracy only. A discussion of the findings highlights the importance of understanding these factors in a digital context, as well as demonstrating the need to account for them when designing targeted interventions.
Despite the recent proliferation of legal online gambling in the Unites States, offshore gambling sites still remain prevalent, causing various problems in the U.S. Although numerous law violations occur in this domain, prior research has reported limited information about offshore gambling, mostly focusing on offshore gamblers' characteristics and motivations. Using routine activities theory, this study attempted to understand environmental and theoretical factors that affect the use of offshore sites by focusing on offshore gambling-generating contexts that involve offshore sites and online casino reviews. Major findings show that the online visibility of offshore sites may be a key predictor of the use of the sites by U.S. players. In addition, online casino reviews providing a blacklist of online gambling sites served as informal guardians, helping players avoid unreliable offshore gambling sites that pose a risk to their customers. Policy implications were suggested based on the findings and provided insights toward effective online gambling regulatory efforts.
Cybercriminals have been using the Internet to accomplish illegitimate activities and to execute catastrophic attacks. Computer-Mediated Communication such as online chat provides an anonymous channel for predators to exploit victims. In order to prosecute criminals in a court of law, an investigator often needs to extract evidence from a large volume of chat messages. Most of the existing search tools are keyword-based, and the search terms are provided by an investigator. The quality of the retrieved results depends on the search terms provided. Due to the large volume of chat messages and the large number of participants in public chat rooms, the process is often time-consuming and error-prone. This paper presents a topic search model to analyze archives of chat logs for segregating crime-relevant logs from others. Specifically, we propose an extension of the Latent Dirichlet Allocation-based model to extract topics, compute the contribution of authors in these topics, and study the transitions of these topics over time. In addition, we present a special model for characterizing authors-topics over time. This is crucial for investigation because it provides a view of the activity in which authors are involved in certain topics. Experiments on two real-life datasets suggest that the proposed approach can discover hidden criminal topics and the distribution of authors to these topics.
"A Distributed Denial of Service (DDoS) attack is a type of cybercrime that renders a target service unavailable by overwhelming it with traffic from several sources (attack nodes). In this paper, we focus on DDoS attacks on a computer network by spreading bots throughout the network. A mathematical differential equation model is proposed to represent the dynamism of nodes at different compartments of the model. The model considers two levels of security, with the assumption that the recovered nodes do not return to the same security level. In previous models, the recovered nodes are returned to be suspect on the same security level, which is an unrealistic assumption. Moreover, it is assumed that the attacker can use the infected target nodes to attack again. With such epidemic-like assumptions of infection, different cases are presented and discussed, and the stability of the model is analyzed as well; reversing the symmetry transformation of attacking nodes population is also proven. The proposed model has many parameters in order to precisely describe the infection movement and propagation. Numerical simulation methods are used to solve the developed system of equations using MATLAB, with the intention of finding the best counteraction to control DDoS spread throughout a network."
Technological progress leads to the transformation of social environment, generating a digital society that exists in the context of new opportunities and new threats. The article is devoted to the problem of expansion of radical political ideas among the most active in the Internet space audience of young people (18-30 years). It is considered the features of digital communication channels, the main advantages of the Internet as a space of freedom, characteristics of the political behavior of young people in digital society. Considering the new technological conditions, as well as the peculiarities of the perception of information by the youth audience, we can talk about favorable conditions for the expansion of radical political ideas among the most active users of the digital reality. The main threats today are cybercrime, illegal trafficking in drugs and weapons, as well as the deliberate reduction of the authority of authorities at different levels, the promotion of terrorism. So, namely the youth audience is the most important target group exposed to the information impact during the hybrid wars, which is a new type of conflicts, characterized the digital epoch. It is proposed the measures to counter the expansion of radical political ideas among young people.
One of the pressing issues in cyber crime is to carry out efficient investigation in child pornography cases. The work in this paper presents an in-depth understanding and deployment of statistical analysis methodologies in the field of digital forensic, with the linkage of the MD5 hash value in conducting forensic investigations. More precisely, it highlights the challenges that forensic investigators are facing during an examination, such as the amount or the content of data. Furthermore, the paper presents the margins of forensic software when dealing with large quantities of data, and how the use of statistical analysis assists in reducing the investigation time and improving effectiveness and efficiency.
The network flow monitoring has evolved to collect information beyond the network and transport layers, most importantly the application layer information. This information is used to improve network security and performance by enabling more precise performance analysis and intrusion detection. In this paper, we contribute to this effort by extending flow monitoring with information from the SSH protocol. Firstly, we analyze the SSH protocol to determine which information can be obtained from the connection establishment phase. Based on the analysis, we create an extension to our flow monitoring infrastructure that allows obtaining the selected information. Lastly, we analyze the SSH connections observed in the university campus network and discuss the benefits of performing the detailed SSH protocol analysis. We argue that with a precise recognition of login attempt results it is possible to improve the detection of successful bruteforce password attacks. Moreover, we publish an anonymized version of our dataset including the SSH specific information.
Secure multi-party computation (SMC) allows multiple parties to jointly and securely compute a function while preserving the privacy of the involved parties. In this regard, homomorphic cryptosystems allow users to perform addition or multiplication operations on encrypted values without having to decrypt the input values. In this paper, we propose both a cryptographic and a non-cryptographic privacy-preserving protocols that allow one party to collaboratively compute a private polynomial function with at least two other parties using semantically secure cryptosystems. In this respect, the function to be evaluated is kept private to the party in question. In addition, we assume that there is no fully trusted party and that all parties are semi-honest. Moreover, we assume that there is no collusion between the different parties. We present the two protocols in question and report on the underlying performance through the presentation of our implementation results. The proposed protocols are relevant in many applications such as evidence sharing in multi-party forensic investigations, situation awareness through the secure sharing in distributed monitoring of plan execution, etc.
Several critical infrastructures are integrating information technology into their operations, and as a result, the cyber attack surface extends over a broad range of these infrastructures. Cyber attacks have been a serious problem for industries since the early 2000s, causing significant interruptions to their ability to produce goods or offer services to their clients. The thriving cybercrime economy encompasses money laundering, black markets, and attacks on cyber-physical systems that result in service disruptions. Furthermore, extensive data breaches have compromised the personally identifiable information of millions of people. This paper aims to summarize some of the major cyber attacks that have occurred in the past 20 years against critical infrastructures. These data are gathered in order to analyze the types of cyber attacks, their consequences, vulnerabilities, as well as the victims and attackers. Cybersecurity standards and tools are tabulated in this paper in order to address this issue. This paper also provides an estimate of the number of major cyber attacks that will occur on critical infrastructure in the future. This estimate predicts a significant increase in such incidents worldwide over the next five years. Based on the study's findings, it is estimated that over the next 5 years, 1100 major cyber attacks will occur on critical infrastructures worldwide, each causing more than USD 1 million in damages.
Although spam email messages have been the primary source of cybercrime since the early Internet era, there is no quick fix to this problem. Governments have established anti-spam legislation, but surprisingly, there has been no measurement of policy impact. This study aims to fill the gap by utilizing a quasi-experimental setting in South Korea, where the anti-spam policy was substantially amended in November 2014. A significant change was in the default setting, switching from an opt-out to an opt-in scheme, which required that commercial email senders obtain recipients' prior consent. Also, the law notably escalated the deterrent penalties for perpetrators. To empirically examine the policy effectiveness, we use a large-scale data set of 5.61 billion spam emails originating from over 38,000 spammers in 226 countries during twenty months in 2014-2015. Our findings suggest that the amended policy adopting the opt-in scheme decreased the volume of spam originating from Korea by 16.1%. The expected economic gain from the increased productivity of recipients is 7.649 million USD per year. This study contributes to the literature by highlighting that a well-designed policy can lower cybersecurity incidents that threaten organizations, operations, and individuals. Our finding also provides important implications for policymakers and managers in designing effective policies with data-driven evidence.
"National security policies increasingly threaten the rules that govern trade and investment flows. This problem is deeper and far more intractable than recent high-profile controversies, such as disputes over the Trump Administration's steel and aluminum tariffs, suggest. Governments worldwide have adopted national security policies that address an increasingly wide array of risks and vulnerabilities, including climate change; pandemic disease; cybercrime; terrorism; and threats to infrastructure, industry, and the media. These policies are also increasingly likely to conflict with trade and investment rules. In other words, while today's high-profile controversies center on alleged abuses of national security in economic law, it is the potential for good-faith but novel national security claims that poses a more significant and permanent threat to the system. This Article is the first to map the new national security challenge and consider its implications for reforming the economic order. It demonstrates that the twenty-first-century expansion of national security policy undermines existing models for separating security measures from ordinary economic regulation. What is needed, it argues, is a new model for reintegrating the economic order with the national security state. To that end, this Article identifies reforms that allow for some oversight of increasingly novel national security claims while preserving flexibility for governments to redefine their security policies in response to twenty-first-century threats."
Domain names play a critical role in cybercrime, because they identify hosts that serve malicious content (such as malware, Trojan binaries, or malicious scripts), operate as command-and-control servers, or carry out some other role in the malicious network infrastructure. To defend against Internet attacks and scams, operators widely use blacklisting to detect and block malicious domain names and IP addresses. Existing blacklists are typically generated by crawling suspicious domains, manually or automatically analyzing malware, and collecting information from honeypots and intrusion detection systems. Unfortunately, such blacklists are difficult to maintain and are often slow to respond to new attacks. Security experts set up and join mailing lists to discuss and share intelligence information, which provides a better chance to identify emerging malicious activities. In this paper, we design GOSSIP, a novel approach to automatically detect malicious domains based on the analysis of discussions in technical mailing lists (particularly on security-related topics) by using natural language processing and machine learning techniques. We identify a set of effective features extracted from email threads, users participating in the discussions, and content keywords, to infer malicious domains from mailing lists, without the need to actually crawl the suspect websites. Our result shows that GOSSIP achieves high detection accuracy. Moreover, the detection from our system is often days or weeks earlier than existing public blacklists.
This paper uses a survey of social networking users to empirically explore their perceptions of security notices - independently verified artefacts informing internet site users that security measures are taken by the site owner. We investigate such factors as purchase experience, purchase intention, risk propensity, usage of various social network categories and user victimisation. The results suggest a strong positive link between purchase intention and paying attention to security notices/features on social networks. We find that higher use of narrow-purpose social networking services has a negative association with paying attention to security notices. We also show that users with higher risk propensity pay less attention to security notices/features. Finally, we find no association between purchase experience, user victimisation and perception of security notices/features. Our results provide new, and possibly more refined, evidence of the factors that influence the attention paid to security notices/features by social media users. The results have important implications for theory development, policy and practice. (C) 2015 Elsevier Ltd. All rights reserved.
Cloud storage services allow users to store their data online and remotely access, maintain, manage, and back up their data from anywhere through the Internet. Although this storage is helpful, it challenges digital forensic investigators and practitioners in collecting, identifying, acquiring, and preserving evidential data. This research proposes an investigation scheme for analyzing data remnants and determining probative artefacts in a cloud environment. Using the Box cloud as a case study, we collect the data remnants available on end-user device storage following the accessing, uploading, and storing of data in the cloud storage. The data remnants are collected from several sources, such as client software files, Prefetch, directory listings, registries, browsers, network PCAP, and memory and link files. Results indicate that the collected data remnants are helpful in determining a sufficient number of artefacts about investigated cybercrimes.
Cybersecurity is now more important than ever, and so is education in this field. However, the cybersecurity domain encompasses an extensive set of concepts, which can be taught in different ways and contexts. To understand the state of the art of cybersecurity education and related research, we examine papers from the ACM SIGCSE and ACM ITiCSE conferences. From 2010 to 2019, a total of 1,748 papers were published at these conferences, and 71 of them focus on cybersecurity education. The papers discuss courses, tools, exercises, and teaching approaches. For each paper, we map the covered topics, teaching context, evaluation methods, impact, and the community of authors. We discovered that the technical topic areas are evenly covered (the most prominent being secure programming, network security, and offensive security), and human aspects, such as privacy and social engineering, are present as well. The interventions described in SIGCSE and ITiCSE papers predominantly focus on tertiary education in the USA. The subsequent evaluation mostly consists of collecting students' subjective perceptions via questionnaires. However, less than a third of the papers provide supplementary materials for other educators, and none of the authors published their dataset. Our results provide orientation in the area, a synthesis of trends, and implications for further research. Therefore, they are relevant for instructors, researchers, and anyone new in the field of cybersecurity education. The information we collected and synthesized from individual papers are organized in a publicly available dataset.
"This is a comparative study to empirically investigate the performances of three different procedures for calculating authorship attribution likelihood ratios (LR). The procedures to be compared are: 1) a procedure based on multivariate kernel density (MVKD) with lexical features; 2) a procedure based on word N-grams; and 3) a procedure based on character N-grams. Furthermore, the best-performing LRs of these three procedures are fused into combined single LRs using a logistic-regression fusion, in order to investigate the extent of the improvement/deterioration that the fusion brings about. This study uses chatlog messages, which were presented as evidence to prosecute paedophiles, for testing. The numbers of word tokens used to model the authorship attribution of each message group are 500 and 1000 words. This was done to examine the effect of sample size on the performance of a system. The performance of a system is assessed with regard to its validity (= accuracy) and reliability (= precision) using the log-likelihood-ratio cost (C-llr) and 95% credible intervals (CI), respectively. While describing the different characteristics of these three procedures in their outcomes, this study demonstrates that the MVKD procedure was the best-performing procedure out of the three in terms of C-llr. This study also demonstrates that a logistic-regression fusion is useful for combining the LRs obtained from the three procedures in question, resulting in a good improvement in performance."
An increasing number of small and medium-sized enterprises (SMEs) use the Internet to support and grow businesses. The application of new technologies comes with inherent risks of ever-changing cyberspace and increasing cybercrime. Previous research has shown that the human factor remains the core element in the cybersecurity chain. Therefore, it is paramount that employees receive effective training to acquire a security mindset. This study puts forward previous research that resulted in a portable escape room game to raise cybersecurity awareness. The purpose of the study is to elaborate the transformation of the physical game into a virtual learning experience to increase flexibility in times such as the Covid-19 lockdown. As main method, we applied the design science framework of Hevner et al. As main result, the research elaborates the design of the developed artifact-a virtual prototype of the escape room game addressing the cybersecurity challenges of SMEs. For the evaluation of the prototype, empirical data was collected in qualitative and quantitative form. As main conclusions, we have observed that a physical escape room can be transformed into a virtual setting with little means without sacrificing player immersion. A limitation was identified in teaching targeted social engineering attacks.
The development of sixth-generation (6G) wireless communication technology is expected to provide super high-speed data transmission, and advanced network performance than the current fifth-generation (5G) and be fully functional by the 2030s. This development will have a significant impact and add improvements to digital extended reality (XR), autonomous systems, vehicular ad hoc networks (VANETs), artificial intelligence (AI), underwater communications, blockchain technology, pervasive biomedical informatics and smart cities built on the digital infrastructure backbone of the Internet of Things (IoT). The ubiquitous nature of this large-scale 6G-enabled IoT that offers faster connectivity capabilities and integrates both terrestrial and non-terrestrial networks will not only create new data security and privacy issues but also provide a treasure trove of digital evidence useful for digital forensic examiners investigating security incidents and cybercrime. However, for digital forensic examiners, evidence collection, preservation and analysis will become a priority in the successful deployment of 6G IoT networks. In this study, we define key applications of 6G network technology to the Internet of Things and its existing architectures. The survey introduces potential digital forensic challenges and related issues affecting digital forensic investigations specific to 6G IoT networks. Finally, we highlight and discuss forensic readiness and future research directions for identified challenges within the 6G IoT network environments.This article is categorized under:Digital and Multimedia Science > IoT Forensics
With the rapid advancement of technologies in the era of Industry 4.0, the inter-connected nature of operations and systems is introducing a rapidly changing landscape of digitized and connected systems. Cybercrime is considered as possibly the greatest threat to connected systems worldwide, and therefore there exists a large drive in engineering to include cybersecurity in the design, development and maintenance of smart cyber-physical systems. Traditionally, the cybersecurity space was considered the responsibility of Information Technology (IT) professionals, where the greater IT infrastructure was required to keep these engineering systems safe. However, through the evolution of engineering and control systems, the IT infrastructure has started to become more integrated with these systems, improving the efficiency of the systems, but also making them more susceptible to cyber-attacks. These changes mean that securing these systems cannot remain the sole responsibility of the IT professionals, as systems must be designed with cybersecurity in mind. Considering that engineers are designing and developing more integrated systems, there exists a knowledge gap in the field of cybersecurity engineering and engineers' understanding of their cybersecurity responsibilities. This study aimed to determine the level of security that is currently considered in standard electrical engineering projects in a typical academic environment. This baseline serves as a motivation to develop a practical approach to assist engineering students in considering cybersecurity when developing engineering systems and products.
Digital evidence continues to be an integral component in cybercrime investigative and judicial processes. However, increasing volume digital content and files makes it challenging for forensic examiners to process evidence in a timely way. In this paper, we use machine learning to predict stealthy watermarks in various file types. We use a black box approach which is different from current steganographic and cryptographic methods to find patterns of candidate file locations for hidden data. The results in this paper demonstrate that it is possible to use machine learning to build singleton models of the same file type as well as hybrid models to predict stealthy watermarks in files. In our experiments, the DOCX singleton models predicted stealthy watermarks with predictive accuracies ranging from 40% to 100%. The PPTX singleton model predicted stealthy watermarks with predictive accuracies ranging from 32.5% to 100%. Similarly, the JPEG singleton model predicted stealthy watermarks with predictive accuracies ranging from 37.5% to 65%. We also generated four types of hybrid models: both HYBID3 and JPEG_PPTX models predicted stealthy watermarks with predictive accuracies ranging from 47.5% to 92.5% while HYBRID_OOXML model predicted stealthy watermarks with predictive accuracies ranging from 32.5% to 100%. In addition, JPEG_DOCX model predicted stealthy watermarks in files with predictive accuracies ranging from 47.5% to 90%.
"The article focuses on the main trends in the rapid development of digitization from ordinary consciousness of the society to the digital world, penetration of digitization elements in our everyday life. The author identifies main directions of the current development of digitization and society with the aim to secure its better regulation by criminal law. There have been four main trends distinguished: the identity trend; decentralized cooperation; data-based new economy, and physical-digital convergence. The research of the issue in question has identified a subjective build-up of digitization models that proves that the humankind on the whole and an individual, in particular, is responsible for actions undertaken by a robotic machine and an automated program. However, when building a horizontal model of control, an individual can withdraw from the process of making decisions but retain the right to receive benefits from such interaction. At the same time if such activity is based on the digital economy, there are some risks related to crime, including the ones related to the use of artificial intelligence by criminal groups."
Nowadays, Wireless Vehicular Ad-Hoc Network (VANET) has become a valuable asset for transportation systems. However, this advanced technology is characterized by highly distributed and networked environment, which makes VANET communications vulnerable to malicious jamming attacks. Although Big Data Analytics has been used to solve this critical security issue by supporting the development of anti-jamming applications, as the amount of vehicular data is growing exponentially, the anti-jamming applications face many challenges (i., reactions in real-time) due to the lack of specific solutions that can keep up with the fast advancement of VANET. In this paper, we propose a new vehicular data prioritization model based on coresets to accelerate the Big Data Analytics in VANET. Our experimental evaluation shows that our solution can significantly increase the efficiency for clustering in jamming detection while keeping and improving the clustering quality. Also, the proposed solution can enable the real-time detection and be integrated to anti-jamming applications.
Investigating cybersecurity incidents requires in-depth knowledge from the analyst. Moreover, the whole process is demanding due to the vast data volumes that need to be analyzed. While various techniques exist nowadays to help with particular tasks of the analysis, the process as a whole still requires a lot of manual activities and expert skills. We propose an approach that allows the analysis of disk snapshots more efficiently and with lower demands on expert knowledge. Following a user-centered design methodology, we implemented an analytical tool to guide analysts during security incident investigations. The viability of the solution was validated by an evaluation conducted with members of different security teams.
Network segmentation is a powerful tool for network defense. In contemporary complex, dynamic, and multilayer networks, network segmentation suffers from lack of visibility into processes in the network, which results in less strict segment definition and loosen network security. Moreover, the dynamics of the networks makes the manual identification of network segments nearly impossible. In this paper, we inspect the possibilities of the behavioraware network segmentation using IP flows and machine learning approaches that would enable to identify segments automatically, even in a complex network. We evaluate the suitability of clustering algorithms for identification of behavior-consistent segments in a network. We show that the clustering algorithms can identify relevant behavior-consistent clusters that overlap with those identified manually by experts. Apart from the segment identification, we investigate the other essential task of network segmentation process: assignment of an unknown host to an existing segment. We evaluate the performance of four different classification mechanisms on a real-world dataset. We show that it is possible to assign an unknown host to an appropriate network segment with up to 92% precision. Moreover, we release the whole dataset and experiment steps available for public use.
Cybercrime reports showed an increase in the number of attacks targeting financial institutions. Indeed, banks were the target of 30% of the total number of cyber-attacks. One of the recommended methods for driving the security challenges is to implement an Information Security Governance Framework (ISGF), a comprehensive practice that starts from the top management and ends with the smallest function in a bank. Although such initiatives are effective, they typically take years to achieve and require loads of resources, especially for larger banks or if there are multiple ISGFs available for the bank to choose. These implementation challenges showed the necessity of having a method for evaluating the adequacy of an ISGF for a bank. The research performed during the preparation of this article did not reveal any available structured evaluation method for an ISGF before its implementation. This chapter introduces a novel method for scoring an ISGF to assess its adequacy for a bank without implementing it. The suggested approach is based on ISGF decomposition and transformation into a survey that will be answered by security experts. The survey results were loaded into a Deep Learning Algorithm that produced a scoring model that could predict the adequacy of an ISGF for a bank with an accuracy of 75%.
Cyberterrorism has become a hotly debated research issue in the past decades because of the convergence of mobile computing powers and the fledging multimedia communication computing capabilities. Cyberterrorism is the exploitation of computer network tools to incur malfunction or shut down critical infrastructures with several keyboard punches, which is dramatically different from traditional terrorism. Due to the ubiquitous multimedia communication tools, they have radically transformed the ways concerning data transmission. Unfortunately, it also incurs unprecedented opportunities for committing cyber crimes that we were not able to foresee two decades ago. Undoubtedly, the mushrooming proliferation of mobile phones spectacularly triggers the information security leakage while most people heavily rely on mobile phones for daily communication. As cybercrime or cyberterrorism surges, digital forensics (DF) of mobile communication devices still enormously lags behind than computer forensics. Hence, in this research paper, we provide a hypothetical case review concerning the DF of a potential cyberterrorist attack that was triggered by a mobile multimedia smart phone utilizing a popular web 2.0 application program via ubiquitous Wi-Fi access points. The corresponding DF of the mobile device was conducted in a step-by-step manner as well as the crime scene reconstruction based on the digital evidence collected, analyzed, and preserved.
"Web phishing is a form of cybercrime aimed at tricking people into visiting malicious URLs to exfiltrate sensitive data. Since the structure of a malicious URL evolves over time, phishing detection mechanisms that can adapt to such variations are paramount. Furthermore, web phishing detection is an unbalanced classification task, as legitimate URLs outnumber malicious ones in real-life cases. Deep learning (DL) has emerged as a promising technique to minimize concept drift to enhance web phishing detection. Deep reinforcement learning (DRL) combines DL with reinforcement learning (RL); that is, a sequential decision-making paradigm in which the problem to be addressed is expressed as a Markov decision process (MDP). Recent studies have proposed an ad hoc MDP formulation to tackle unbalanced classification tasks called the imbalanced classification Markov decision process (ICMDP). In this paper, we exploit the ICMDP to present a double deep Q-Network (DDQN)-based classifier to address the unbalanced web phishing classification problem. The proposed algorithm is evaluated on a Mendeley web phishing dataset, from which three different data imbalance scenarios are generated. Despite a significant training time, it results in better geometric mean, index of balanced accuracy, F1 score, and area under the ROC curve than other DL-based classifiers combined with data-level sampling techniques in all test cases."
Nowadays, the volume of the multimedia heterogeneous evidence presented for digital forensic analysis has significantly increased, thus requiring the application of big data technologies, cloud-based forensics services, as well as Machine Learning (ML) techniques. In digital forensics domain, ML algorithms have been applied for cybercrime investigation such as child abuse investigations, malware classification, and image forensics. This paper addresses this issues and deals with forensic analysis of digital images and videos. In particular, this work aims at proposing a multimedia classification tool with a parallel software architecture for a fast inspection, which is easy to use (to be used by officers during a search), requires limited hardware resources and it is built on an open-source software to limit its costs. Moreover, this tool must be able to quickly inspect multiple devices at a time. When positives are found in a device, such device will be seized for a deeper analysis later in the lab. It will not be seized otherwise, reducing the inconvenience for the suspect as well as the time required for the next analysis phase. As a case study, we focus on the identification of child pornography images. Experimental results show that the proposed architecture is capable of guaranteeing a high recall, a fast process and high performances in real scenarios.
Phishing is one of the most frequently occurring forms of cybercrime that Internet users face and represents a violation of cybersecurity principles. Phishing is a fraudulent attack that is performed over the Internet with the purpose of obtaining and using without authorization the sensitive information of Internet users, such as usernames, passwords, credit card details, and bank account information. Some widely used phishing attempts involve using email spoofing or instant messaging, aiming to convince a victim to visit the spoofed websites, which will result in obtaining the victim's information. In this work, we identify and analyze the most important features needed to detect the spoofed websites in virtue of two new feature selection techniques. The first proposed feature selection technique uses underlying feature selection methods that vote on each feature, and if such methods agree on a specific feature, that feature is selected. The second feature selection technique also uses underlying feature selection methods that vote on each feature, and if the majority vote on a specific feature, the feature is selected. We also propose a phishing detection technique based on both AdaBoost and LightGBM ensemble methods to detect the spoofed websites. The proposed method achieves a very high accuracy compared to that of the existing methods.
The increasing of cybercrime means an increase in the number of digital evidences produced in these criminal activities. Also, to pay attention to the security and integrity of digital data, the construction of digital evidence storage must pay attention to the need for convenience in adding storage and the need for investigators to avoid geographical problems in carrying out their duties. An ideal implementation of the problem is the development of network-based digital evidence storage using the Software-Defined Storage to build a clustered storage system. In this study, we build a clustered storage system using Ceph that will be functioned as digital evidence storage. Ceph is a Software-Defined Storage which is designed to provide excellent performance, reliability and scalability. On the other hand, since digital evidence is fragile, then the security of storage system must also be considered so that the stored digital evidence in the storage system can be accepted in court. The security includes aspects of confidentiality, integrity and availability aspects. We were able to build a storage system using Ceph and as a result, our system storage has met the aforementioned scalability, confidentiality, integrity and availability aspects. The storage system is expected to contribute in terms of digital evidence storage and preservations.
Background: The proliferation of electronic banking has revolutionised the delivery of financial services across the globe. E-banking services offer substantial benefits, such as reduced costs to transact, convenience and flexibility. Despite the considerable penetration of e-banking in South Africa, it remains unclear whether customers are willing to continue using these services, particularly given the safety concerns, because of the high incidence of cybercrime. Objectives: Owing to the paucity of research on customers' attitude towards continued usage intentions of e-banking services, the aim of this study is to investigate the formation of attitude towards e-banking continued usage intentions among rural banking customers. Method: The study was underpinned by a positivist paradigm, and a descriptive design was employed. Primary data were collected by means of self-administered questionnaires, which yielded 139 valid responses. Regression analysis was utilised during the analysis for hypothesis testing. Results: The results revealed that e-banking attitude is mainly driven by perceived ease of use and perceived usefulness. Furthermore, the results showed that e-banking attitude strongly predicts continued usage intentions. Conclusion: The empirical evidence presented in this study adds value to the existing research on e-banking, particularly in the context of rural banking customers, an area which is largely under-researched in South Africa.
While the internet and online social networks have positively enriched societal communications and economic opportunities, these technological advancements have changed-and continue to change-the very nature of crime, serving to breed a new sophisticated and technically capable criminal. Furthermore, the borderless nature of the phenomenon of cybercrime and the transnational dimensions of human trafficking, drugs importation and the illegal movement of firearms, cash and stolen goods means that criminals can plan their crimes from jurisdictions across the world, making law enforcement particularly challenging, the very reason why LEAs must maximise the potential of OSINT and seek new and innovative ways to prevent crime. Hence, it is essential for all practitioners, policy-makers and policing professionals to understand what OSINT is and what it is not, how it can be used and the limitations or conditions on it, as well as understanding more about the scale, scope and complexity of the threats from criminals whose methods of operating are becoming increasingly sophisticated. The purpose of this chapter is to explain the role and function of OSINT within the context of policing and existing intelligence collection disciplines, as well as to define OSINT from an LEA perspective and describe its position within the intelligence profession of policing.
Voice over IP (VoIP) technologies such as Skype are becoming increasingly popular and widely used in different organisations, and therefore identifying the usage of this service at the network level becomes very important. Reasons for this include applying Quality of Service (QoS), network planning, prohibiting its use in some networks and lawful interception of communications. Researchers have addressed VoIP traffic classification from different viewpoints, such as classifier accuracy, building time, classification time and online classification. This previous research tested their models using the same version of a VoIP product they used for training the model, giving generalizability only to that version of the product. This means that as new VoIP versions are released, these classifiers become obsolete. In this paper, we address if this approach is applicable to detecting new, untrained, versions of Skype. We suggest that using cost-sensitive classifiers can help to improve the accuracy of detecting untrained versions, by testing compared to other algorithms. Our experiment demonstrates promising preliminary results to detect Skype version 4, by building a cost sensitive classifier on Skype version 3, achieving an F-measure score of 0.57. This is a drastic improvement from not using cost sensitivity, which scores an F-measure of 0. This approach may be enhanced to improve the detection results and extended to improve detection for other applications that change protocols from version to version.
"The use of online review sites has grown significantly, allowing for communities to share information on products or services. These online review sites are marketed as being independent and trustworthy, but have been criticised for not ensuring the integrity of the reviews. One major concern is that of review fraud; where a person (such as a marketer) is paid to write favourable reviews for one product or poor reviews for a competitor. In this research we show a method for determining if two reviews share an author, which can be used to identify if a review is legitimate. Our results indicate a high quality of the method, with an f(1)-score of over 0.66 in testing data with 40 authors, with most authors having only one or two documents. This type of analysis can be used to investigate cases of potential hotel review fraud."
Background Crackdowns on urban sites with concentrated criminal activity are sometimes followed by geographical relocation of crime. Is this also the case in cyberspace, where illegal websites and online networks can be wiped clean, but also quickly rebuilt and replaced on new servers and URLs? Methods I address this question in three steps. First, I measure MDMA trade in a large digital market for drugs, before and after the arrest of a leading MDMA seller in the same market. Second, I count the number of available digital drug markets and vendor shops in the period February 2014 June 2018, to see if websites closed by police were replaced by new ones. Third, I track the digital movement and trading activities of individual drug sellers, before and after law enforcement shut down two large markets. Results After police arrested a leading MDMA seller, other MDMA sellers filled most - but not all - of the gap. A major law enforcement crackdown reduced the number of available markets, but new ones were created, and market counts eventually surpassed the previous peak. When law enforcement shut down two big markets, many of the sellers relocated to other e-commerce sites and continued high-earning operations there. Conclusion Arrests and market closures redirect digital drug trade to other sellers and markets. Hot spot policing in cyberspace might produce temporary results, but is arguably ineffective in the long run, as actors use information and communication technology's unique capacities to reorganize.
Internet of Things (IoT) is increasingly permeating peoples' lives, gradually revolutionizing our way of life. Due to the tight connection between people and IoT, now civil and criminal investigations or internal probes must take IoT into account. From the forensic perspective, the IoT environment contains a rich set of artifacts that could benefit investigations, while the forensic investigation in IoT paradigm may have to alter to accommodate characteristics of IoT. Therefore, in this article, we analyze the impact of IoT on digital forensics and systematize the research efforts made by previous researchers from 2010 to 2018. We sketch the landscape of IoT forensics and examine the state of IoT forensics under a 3-D framework. The 3-D framework consists of a temporal dimension, a spatial dimension, and a technical dimension. The temporal dimension walks through the standard digital forensic process while the spatial dimension explores where to identify sources of evidence in IoT environment. These two dimensions attempt to provide principles and guidelines for standardizing digital investigations in the context of IoT. The technical dimension guides a way to the exploration of tools and techniques to ensure the enforcement of digital forensics in the ever-evolving IoT environment. Put together, we present a holistic overview of digital forensics in IoT. We also highlight open issues and outline promising suggestions to inspire future study.
"There is a widespread perception that computer crime sentencing is too harsh. But this criticism has occurred in the absence of comprehensive, multi-year data on how computer crimes are actually sentenced and how those sentences compare to other, purportedly similar crimes, such as trespass, burglary, or fraud. This article uses an analysis of real-world sentencing data to examine how the computer crimes are actually sentenced. We combined court filings and U.S. Sentencing Commission data files to build a custom data set of 1095 Computer Fraud and Abuse Act (CFAA) sentences from 2005 through 1998. Our results show that CFAA sentences are sentenced differently from trespass, burglary, or non-CFAA fraud crimes; that sentences in which the defendant exceeded authorized access have declined over the years; and that the sophisticated means and special skills enhancements have been less routinely applied than has been assumed. These results have policy implications for how CFAA crimes are sentenced."
Cybersecurity encompasses a broad range of practices, tools and concepts related closely to those of information and operational technology (OT) security. Cybersecurity is distinctive in its inclusion of the offensive use of information technology to attack adversaries. Use of the term cybersecurity as a key challenge and a synonym for information security or IT security confuses customers and security practitioners, and obscures critical differences between these disciplines. Recommendation for security leaders is that they should use the term cybersecurity to designate only security practices related to the defensive actions involving or relying upon information technology and/or OT environments and systems. Within this paper, we are aiming to explain cybersecurity and describe the relationships among cybersecurity, information security, OT security, IT security, and other related disciplines and practices, e. g. cyber defence, related to their implementation aligned with the planned or existing cybersecurity strategy at the national level. In the case study given example of The National Cybersecurity Strategy of the Republic of Croatia and Action plan is presented and elaborated. The Strategy's primary objective is to recognize organizational problems in its implementation and broaden the understanding of the importance of this issue in the society.
Cyberattacks are on the increase in severity, complexity and frequency, negatively affecting the citizens, government, and businesses. Adversely, the security and Defence role-players in developing countries, such as South Africa, are short of the required capacity and capability to adequately defend and protect the national cyberspace against these fast moving and persistent threats and attacks. Be that as it may, the South African cyberspace still requires national attention and protection by the mandated role-players, such as the Defence force and its industry partners. Thus, within the cyber domain, the various Defence force role-players can no longer rely on traditional solutions to detect, defend, and respond to the forever changing cyber threats and cyberattacks. In order to reduce cybersecurity risks and strengthen cyber resilience of the nation, strategic cybersecurity information sharing in the Defence environment is becoming a necessity. Thus, the contribution from this paper is a systematic discussion and demonstration of a conceptual cyber threat intelligence sharing model and platform that could stimulate and enable different stakeholders within the Defence environment to seamlessly and collaboratively aggregate, analyse, and timely share contextual and actionable cyber-threat intelligence that could lead to a resilient cybersecurity posture and better protection of the national cyberspace.
Smart Grids play an important role in modern society and for the sustainability of its wellbeing. However, the undoubted advantages come at the cost of higher complexity, especially at the level of information and communication technologies that enhance the physical grid infrastructure. As such, software quality requirements, such as reliability, resilience, safety, security, privacy, and performance assume a more functional facet. In this paper, we focus on software reliability as one of the key qualities of a Smart Grid infrastructure, which is however not yet well defined and understood. We formulate relevant definitions of software reliability in the Smart Grid context, categorize information necessary to quantify the identified reliability views, and explore existing literature and online resources to assess what datasets, necessary for reliability quantification, are available to make the reliability assessment possible.
The digitalization of our society is only possible in the presence of secure and reliable software systems governing ongoing critical processes, so-called critical information infrastructures. The understanding of mutual interdependencies of events and processes is crucial for cybersecurity and software reliability. One of the promising ways to tackle these challenges is process mining, which is a set of techniques that aims to mine essential knowledge from processes, thus providing more perspectives and temporal context to data interpretation and process understanding. However, it is unclear how process mining can help and can be practically used in the context of cybersecurity and reliability. Therefore, in this work, we investigate the potential of process mining to aid in cybersecurity and software reliability to analyze and support research efforts in these areas. Concretely, we collect existing process mining applications, discuss current trends and promising research directions that can be used to tackle the current cybersecurity and software reliability challenges. To this end, we conduct a systematic literature review covering 35 relevant research approaches to examine how the process mining is currently used for these tasks and what are the research gaps and promising research directions in the area. This work is an extension of our previous work, which focused solely on the cybersecurity area, based on the observation of relative closeness and similar goals of those two fields, in which some approaches tend to overlap.
Countries across the world are deeply concerned about the cybercrimes. Experts and practitioners always emphasize on collaborative efforts to investigate and control illegal activities in virtual environment. Countries differ significantly on socio-economic continuum. Due to cultural dissimilarities, criminal activities as well as their causes and consequences also vary from region to region. This study was an effort to identify contextual factors stimulating illegitimate activities in cyber environment and to suggest remedial measures in this regard. With the help of focus group sessions comprising ten participants from diverse background, a discussion was held to explore the causes and consequences of cybercrimes. The consolidated results are presented duly agreed by the panelists.
Prior to the innovation of information communication technologies (ICT), social interactions evolved within small cultural boundaries such as geo spatial locations. The recent developments of communication technologies have considerably transcended the temporal and spatial limitations of traditional communications. These social technologies have created a revolution in user-generated information, online human networks, and rich human behavior-related data. However, the misuse of social technologies such as social media (SM) platforms, has introduced a new form of aggression and violence that occurs exclusively online. A new means of demonstrating aggressive behavior in SM websites are highlighted in this paper. The motivations for the construction of prediction models to fight aggressive behavior in SM are also outlined. We comprehensively review cyberbullying prediction models and identify the main issues related to the construction of cyberbullying prediction models in SM. This paper provides insights on the overall process for cyberbullying detection and most importantly overviews the methodology. Though data collection and feature engineering process has been elaborated, yet most of the emphasis is on feature selection algorithms and then using various machine learning algorithms for prediction of cyberbullying behaviors. Finally, the issues and challenges have been highlighted as well, which present new research directions for researchers to explore.
Voice over IP (VoIP) is increasingly replacing fixed line telephone systems globally due to lower cost, call quality improvements over digital lines and ease of availability. At the same time, criminals have also transitioned to using this environment, creating challenges for law enforcement, since interception of VoIP traffic is more difficult than a traditional telephony environment. One key problem for proprietary VoIP algorithms like Skype is being able to reliably identify and characterize network traffic. In this paper, the latest Skype version and its components are analyzed, in terms of network traffic behavior for logins, calls establishment, call answering and the change status phases. Network conditions tested included blocking different port numbers, inbound connections and outbound connections. The results provide a clearer view of the difficulties in characterizing Skype traffic in forensic contexts. We also found different changes from previous investigations into older versions of Skype.
"The purpose of this article is to formulate proposals to improve the criminal law of the Russian Federation on liability for digital crimes on the basis of a comparative legal study of the UK legislation in this area. The work provides a detailed comparative legal analysis of the UK legislation in the field of combating digital crimes. Based on the study, we proposed some mechanisms for ensuring the security of relations in the digital field. The cross-border nature of these attacks among the main tasks includes unification of legal norms governing the IT sphere, creation of a single mechanism to hold accountable for attacks in the IT sphere worldwide, regardless of geopolitical boundaries. A separate direction of the internal criminal policy of all countries shall be the creation of effective mechanisms for applying the provisions of legislation in the digital sphere; any legislation, even the most progressive, is useless and only declarative in nature without the necessary mechanism for its application. Evidence of the person's guilt is required in order to prosecute. The informational nature of infringements necessitates expanding the boundaries of the powers of law enforcement agencies, which inextricably leads to the problem of finding a balance between observing the freedoms of citizens in the information space and ensuring the universal information security. In the UK, as in all countries of the world, the answer to this question has not yet been found."
Cybercrime has been on the rise in recent years. Cybercriminals target Internet users and commit illegal acts. Malicious web pages are an easy tool to obtain sensitive personal information from Internet users, install malware on their computers, and expose them to cyberspace. More advanced techniques are used by the attackers to inject the malicious code into the normal legitimate webpages or create a web page with illegal content and then spread the URL through various possible media to perform the intended action. Internet users who click the link will be in their nest. Identifying such URLs or web pages can be a daunting task for internet users. So, the researchers using different machine/deep learning methods with different set features that are derived from URL, content, and server-based information of the web page to detect or classify the malicious web pages. Most attacks, including drive-by downloads, phishing, and injections, conceals the maliciously intended code or link in the content of legitimate or purportedly legitimate webpage. Many current research work uses the URL based features due to its risk-free processing. Only less attention is given to the content-based features. That hides the vital area of features to better classification. To address this issue, this paper focus on web content-based features for malicious web page classification. The result shows, the proposed method performs better in terms of accuracy.
The effect of digitization has led to an increased dependency on the internet. At the same time, cyber-attacks are on the rise due to this increased digitization. In cybercrime cases, digital evidence is of utmost importance. The forensic investigation process always begins after the incident occurred, by that time intelligent attackers got enough time to destroy the traces. This paper proposes a prior evidence capture protocol, that will help in the simultaneous collection of evidence when the crime has occurred. This collected evidence is in the form of device fingerprint which will uniquely identify the fingeprintee client device. In the future, if the dispute arises these prior captured device fingerprints can be used as legal evidence and help in the process of forensic investigation. The proposed protocol uses the concept of a trusted time stamping server (TTSS) to prove the integrity and nonrepudiation of the collected evidence. The timestamps are attached by the trusted third party TTSS with all collected evidence, these timestamps cannot be changed by local client devices. The paper also provides security validation of the proposed protocol by using Burrows Abadi Needham (BAN) logic. The formal verification is also done by using the AVISPA tool. The results of AVISPA shows that the proposed protocol is safe under OFMC and Cl-AtSe model. (C) 2020 The Authors. Published by Elsevier B.V.
Increasing involvement of Nigerian youths in cybercrime and fraud, ritual activities, prostitution, human and drug trafficking, kidnapping, robbery and hired killings reveal the growing materialism of a significant number of Nigerian youths, including uncountable numbers of professed Nigerian Christian youths. There is the need to address materialism amongst Nigerian youths with special reference to Nigerian Christian youths. Paul???s moral instructions to Timothy are still relevant for Nigerian Christian youths to emulate. Therefore, this study employs a redaction critical method of biblical exegesis to interpret and apply 1 Timothy 6:6???14 to the problem of materialism amongst Nigerian Christian youths. This study finds that materialism amongst Nigerian Christian youths is consolidating negative implications to their spiritual lives, families, to the mission mandate of the church and to Nigeria as a nation. This study argues that Paul has shown a model of how a Christian youth ought to be guided with regard to material possessions. Contribution: This study explored the issue of materialism amongst countless Nigerian Christian youths that is consolidating negative implications to their spiritual lives, families, to the church and to Nigeria as a nation. The study recommends that Paul???s instructions to Timothy about godliness and love of money are still relevant for Nigeria Christian youths to emulate.
Background. Cybersecurity controls are deployed to manage risks posed by malicious behaviours or systems. What is not often considered or articulated is how cybersecurity controls may impact legitimate users (often those whose use of a managed system needs to be protected, and preserved). This oversight characterises the `blunt' nature of many cybersecurity controls. Aim. Here we present a framework produced from a synthesis of methods from cybercrime opportunity reduction and behaviour change, and a consideration of existing risk management guidelines. Method. We illustrate the framework and its principles with a range of examples and a potential application focusing on online abuse and social media controls, relating in turn to issues inherent in cyberbullying and tech-abuse. Results. The framework describes a capacity to improve the precision of cybersecurity controls by examining shared determinants of negative and positive behaviours in a system. This identifies opportunities for risk owners to better protect legitimate users while simultaneously acting to prevent malicious activity in a managed system. Conclusions. We describe capabilities for a novel approach to managing sociotechnical cyber-risk which can be integrated into typical risk management processes. This includes consideration of user activities as a system asset to protect, and a consideration of how to engage with other stakeholders to identify behaviours to preserve in a system.
Authorship analysis (AA) is the study of unveiling the hidden properties of authors from textual data. It extracts an author's identity and sociolinguistic characteristics based on the reflected writing styles in the text. The process is essential for various areas, such as cybercrime investigation, psycholinguistics, political socialization, etc. However, most of the previous techniques critically depend on the manual feature engineering process. Consequently, the choice of feature set has been shown to be scenario-or dataset-dependent. In this paper, to mimic the human sentence composition process using a neural network approach, we propose to incorporate different categories of linguistic features into distributed representation of words in order to learn simultaneously the writing style representations based on unlabeled texts for AA. In particular, the proposed models allow topical, lexical, syntactical, and character-level feature vectors of each document to be extracted as stylometrics. We evaluate the performance of our approach on the problems of authorship characterization, authorship identification and authorship verification with the Twitter, blog, review, novel, and essay datasets. The experiments suggest that our proposed text representation outperforms the static stylometrics, dynamic n-grams, latent Dirichlet allocation, latent semantic analysis, distributed memory model of paragraph vectors, distributed bag of words version of paragraph vector, word2vec representations, and other baselines.
"In the digital age, the interaction between privacy; data protection and advanced technological developments such as big data analytics has become pertinent to Europol's effectiveness in providing accurate crime analyses. For the purposes of preventing and combating crime falling within the scope of its objectives, it is imperative for Europol to employ the fullest and most up-to-date information and technical capabilities possible whilst respecting fundamental human rights. The present article addresses precisely the paradox of on one side protecting fundamental human rights against external terrorist and/or cybercrime intrusions, and on the other providing a privacy-conscious approach to data collection and analytics, so that Europol can even more effectively support and strengthen action in protecting society against internal threats in a proportionate, responsible and legitimate manner. The advantage proposed in this very context of large quantities of data informing strategic analysis at Europol is a purpose-oriented data protection impact assessment. Namely, the evolution from traditional instruments in the fight against organised crime and terrorism to more technologically advanced ones equally requires an alteration of the conventional notions of privacy and investigative and information-sharing methods. (C) 2017 Daniel Drewer and Vesela Miladinova. Published by Elsevier Ltd. All rights reserved."
Recent pose-estimation methods enable digitization of human motion by extracting 3D skeleton sequences from ordinary video recordings. Such spatio-temporal skeleton representation offers attractive possibilities for a wide range of applications but, at the same time, requires effective and efficient content-based access to make the extracted data reusable. In this paper, we focus on content-based retrieval of pre-segmented skeleton sequences of human actions to identify the most similar ones to a query action. We mainly deal with the extraction of content-preserving action features, which are learned using the triplet-loss approach in an unsupervised way. Such features are (1) effective as they achieve a similar retrieval quality as the features learned in a supervised way, and (2) of a fixed size which enables the application of indexing structures for efficient retrieval.
A Smart Grid (SG) is a Cyber-Physical System (CPS) idealized with the ultimate goal to reach optimal energy efficiency, a key enabler for the Smart City concept. SG reliability needs to be ensured by an in-depth testing process, involving software/hardware components, network communication, together with quality concerns such as security of the overall infrastructure. While there are many models for modelling/designing SGs (e.g., SGAM), there is no specific testing process model customized for the SGs needs. As such, the aim of this paper is to identify a set of software testing process models developed over the years and investigate their adaptability to SG testing requirements. To reach the goal, we categorize SG requirements from related works with the help of a concept map. Afterwards, the most popular testing process models from literature are discussed in the context of the identified requirements. From a list of 17 main testing models, six testing models (TPI Next, TMMi, Test SPICE, TIM, ISO/IEC/IEEE 29119, and MTPF) are considered as potentially adaptable to the SG domain.
Transaction processing is a critical aspect of modern software systems. Such criticality increased over the years with the emergence of microservices, calling for appropriate management of transactions across separated application domains, ensuring the whole system can recover and operate in a possible degraded state. The Saga pattern emerged as a way to define compensating actions in the context of long-lived transactions. In this work, we discuss the relation between traditional transaction processing models and the Saga pattern targeting specifically the distributed environment of reactive microservices applications. In this context, we provide a comparison of the current state of transaction support in four Java-based enterprise application frameworks for microservices support: Axon, Eventuate Event Sourcing (ES), Eventuate Tram, and MicroProfile Long Running Actions (LRA).
This work proposes an end-to-end model for network attack detection and network attack classification using deep learning-based recurrent models. The proposed model extracts the features of hidden layers of recurrent models and further employs a kernel-based principal component analysis (KPCA) feature selection approach to identify optimal features. Finally, the optimal features of recurrent models are fused together and classification is done using an ensemble meta-classifier. Experimental analysis and results of the proposed method on more than one benchmark network intrusion dataset show that the proposed method performed better than the existing methods and other most commonly used machine learning and deep learning models. In particular, the proposed method showed maximum accuracy 99% in network attacks detection and 97% network attacks classification using the SDN-IoT dataset. Similar performances were obtained by the proposed model on other network intrusion datasets such as KDD-Cup-1999, UNSW-NB15, WSN-DS, and CICIDS-2017.
Dynamic development of IT technology poses new challenges related to the cross-border collection of electronic evidence from the cloud. Many times investigators need to secure data stored on foreign servers directly and then look for solutions on how to turn the data into a legitimate source of evidence. To study the situation and propose solutions, I conducted a survey among Polish representatives of public prosecutors' offices and courts. This paper presents information from digital evidence collection practices across multiple jurisdictions. I stated that representatives from the prosecution and the judiciary in Poland are aware of the issues associated with cross-border acquisition and preservation of cloud-based evidence. In their view, many of the problems are time-consuming and ineffective international cooperation, the voluntary nature of cooperation between foreign cloud service providers, lack of harmonized procedures and guidelines, the diversity of legal systems, and the lack of knowledge held by law enforcement officials and the judiciary. This work should be the beginning of an open discussion with practitioners about existing challenges and an invitation for further research with a larger sample of prosecutors and judges. There are no such studies in literature. The paper shows that it is possible to improve current procedures for the cross-border collection of cloud-based digital evidence.
In adversarial classification, the interaction between classifiers and adversaries can be modeled as a game between two players. It is natural to model this interaction as a dynamic game of incomplete information, since the classifier does not know the exact intentions of the different types of adversaries (senders). For these games, equilibrium strategies can be approximated and used as input for classification models. In this paper we show how to model such interactions between players, as well as give directions on how to approximate their mixed strategies. We propose perceptron-like machine learning approximations as well as novel Adversary-Aware Online Support Vector Machines. Results in a real-world adversarial environment show that our approach is competitive with benchmark online learning algorithms, and provides important insights into the complex relations among players.
There are many definitions of software Technical Debt (TD) that were proposed over time. While many techniques to measure TD emerged in recent times, there is still not a clear understanding about how different techniques compare when applied to software projects. The goal of this paper is to shed some light on this aspect, by comparing three techniques about TD identification that were proposed over time: (i) the Maintainability Index (MI), (ii) SIG TD models, and (iii) SQALE analysis. Considering 20 open source Python libraries, we compare the TD measurements time series in terms of trends and evolution according to different sets of releases (major, minor, and micro), to see if the perception of practitioners about TD evolution could be impacted. While all methods report generally growing trends of TD over time, there are different patterns. SQALE reports more periods of steady states compared to MI and SIG TD. MI is the method that reports more repayments of TD compared to the other methods. SIG TD and MI are the models that show more similarity in the way TD evolves, while SQALE and MI are less similar. The implications are that each method gives slightly a different perception about TD evolution.
Smart cities aim at integrating various IoT (Internet of Things) technologies by providing many opportunities for the development, governance, and management of user services. One of the ways to support this idea is to use cloud and edge computing techniques to reduce costs, manage resource consumption, enhance performance, and connect the IoT devices more effectively. However, the selection of services remains a significant research question since there are currently different strategies towards cloud computing, including services for central remote computing (traditional cloud model) as well as distributed local computing (edge computing). In this paper, we offer an integrated view of these two directions and the selection among the edge technologies based on MCDA (Multiple Criteria Decision Analysis) algorithms. To this end, we propose a foglet as a middleware that aims at achieving satisfactory levels of customer services by using fuzzy similarity and TOPSIS (Technique for Order of Preference by Similarity to Ideal Solution) to facilitate the rating and selection of services in the fog-to-cloud environment. Then, we describe the selection process with a numerical example, and conclude our work with an outline of future perspectives.
Rapid developments in Intelligent Transportation Systems (ITSs) have emerged as a new research field for building sustainable smart cities. VANET (vehicular ad hoc network) is one of the emergent transportation technologies that has a great impact on ensuring mainly traffic management and road safety in urban areas by efficiently using data sharing among vehicles. To further increase the security and safety of passengers and drivers, ITSs are continually striving to make the fusion of emergent network technologies to provide more reliable and efficient services. Relating VANET to UAV (unmanned aerial vehicle) is an example of this fusion, where UAVs act as an assistant to vehicles aiming to extend the network connectivity while efficiently avoiding obstacles (e.g., Buildings) and providing high data delivery ratios. However, VANET and UAV are still critical security subjects that must be addressed. Advanced Machine Learning (e.g., Deep Learning) techniques have recently been used to protect VANET and UAV communications against various cyber attacks that deteriorate the integrity, confidentiality, and availability of vehicular data. Thus, in this paper, we focus on reviewing related work on machine learning techniques for intrusion detection systems in VANET- and UAV-aided networks. We also highlight the main open research challenges in literature and provide hints for improving security in ITSs. (C) 2021 The Authors. Published by Elsevier B.V.
Specialty literature and solutions in the market have been focusing in the last decade on collecting and aggregating significant amounts of data about transactions (and user behavior) and on refining the algorithms used to identify fraud. At the same time, legislation in the European Union has been adopted in the same direction (e.g., PSD2) in order to impose obligations on stakeholders to identify fraud. However, on the one hand, the legislation provides a high-level description of this legal obligation, and on the other hand, the solutions in the market are diversifying in terms of data collected and, especially, attempts to aggregate data in order to generate more accurate results. This leads to an issue that has not been analyzed yet deeply in specialty literature or by legislators, respectively, the privacy concerns in case of profile building and aggregation of data for fraud identification purposes and responsibility of stakeholders in the identification of frauds in the context of their obligations under data protection legislation. This article comes as a building block in this direction of research, as it contains (i) an analysis of existing fraud detection methods and approaches, together with their impact from a data protection legislation perspective and (ii) an analysis of respondents' views toward privacy in case of fraud identification in transactions based on a questionnaire in this respect having 425 respondents. Consequently, this article assists in bridging the gap between data protection legislation and implementation of fraud detection obligations under the law, as it provides recommendations for compliance with the latter legal obligation while also complying with data protection aspects.
Cyberspace is a network of networks connecting billions of users round the globe with the help of networked gadgets that include computers, mobile phones, palmtops, iPods, etc. Internet, though offers great benefit to society, it also presents opportunities for criminals using new and highly sophisticated technology tools. It both poses and suffers from various security challenges as it is used to commit crime and is itself target of crime. It has unveiled many unique challenges like cyber espionage, cyber warfare, cyber terrorism that were not known previously in real space. The technology is constantly evolving which gives birth to a newer crime and a new generation of crimes has come on the horizon. It is in this backdrop that an attempt is made to analyze different facets of cybercrimes together with possible solutions offered by law and technology.
Social networking services (SNS) have increased in popularity over the last decade. They have become major platforms for e-commerce, personal branding, socialization and information. The success of social networking services like Facebook and Twitter as well as LinkedIn, LiveJournal and Foursquare and the variety of their usages leads their users to create a set of profiles on different SNS. Recently, social networking service aggregators have proposed centralizing the multiple social networking profiles of a given user in order to facilitate his interactions with social networking services. Such aggregators allow the messages received by a profile over multiple SNS to be retrieved, edited and posted with much less effort. Despite their obvious advantages, we highlight in this paper the risk of potential data leaks due to the inexperienced use of such tools. For this purpose, we provide a classification of online SNS and present their specificities with regard to the publicly exposed data of a user. Based on this classification, we investigate the possible insecure use of aggregators with an inappropriate set of SNS, which could lead to rendering sensitive data accessible to people it wasn't intended for. We present a decision tree approach for identifying a possible data leak based on the three following criteria: opinion, interest and location. We finally show the result of this approach on popular social networking aggregators.
Artificial intelligence (AI) advancements have revolutionized many critical domains by providing cost-effective, automated, and intelligent solutions. Recently, ChatGPT has achieved a momentous change and made substantial progress in natural language processing. As such, a chatbot-driven AI technology has the capabilities to interact and communicate with users and generate human-like responses. ChatGPT, on the other hand, has the potential to influence changes in the cybersecurity domain. ChatGPT can be utilized as a chatbot-driven security assistant for penetration testing to analyze, investigate, and develop security solutions. However, ChatGPT raises concerns about how the tool can be used for cybercrime and malicious activities. Attackers can use such a tool to cause substantial harm by exploiting vulnerabilities, writing malicious code, and circumventing security measures on a targeted system. This article investigates the implications of the ChatGPT model in the domain of cybersecurity. We present the state-of-the-art practical applications of ChatGPT in cybersecurity. In addition, we demonstrate in a case study how a ChatGPT can be used to design and develop False data injection attacks against critical infrastructure such as industrial control systems. Conversely, we show how such a tool can be used to help security analysts to analyze, design, and develop security solutions against cyberattacks. Finally, this article discusses the open challenges and future directions of ChatGPT in cybersecurity.
Over the past two decades, social media have become a crucial and omnipresent cultural and economic phenomenon, which has seen platforms come and go and advance technologically. In this study, we explore the further development of social media regarding interactive technologies, platform development, relationships to news media, the activities of institutional and organizational users, and effects of social media on the individual and the society over the next five to ten years by conducting an international, two-stage Delphi study. Our results show that enhanced interaction on platforms, including virtual and augmented reality, somatosensory sense, and touch- and movement-based navigation are expected. AIs will interact with other social media users. Inactive user profiles will outnumber active ones. Platform providers will diversify into the WWW, e-commerce, edu-tech, fintechs, the automobile industry, and HR. They will change to a freemium business model and put more effort into combating cybercrime. Social media will become the predominant news distributor, but fake news will still be problematic. Firms will spend greater amounts of their budgets on social media advertising, and schools, politicians, and the medical sector will increase their social media engagement. Social media use will increasingly lead to individuals' psychic issues. Society will benefit from economic growth and new jobs, increased political interest, democratic progress, and education due to social media. However, censorship and the energy consumption of platform operators might rise.
The cyber threat is highly dynamic and evolves in parallel with the innovation of systems and communications, which are outside the control of government authorities and respond exclusively to business logic and free initiative, often contingent on implementation of illegal activities. In particular, the threat posed by the criminal use of the Internet goes far beyond the cybercrime, in particular with the Tor network, where black markets are shifted with the shape of renown legal marketplaces as Ebay and Amazon. Hence even common crime can benefit of new modus operandi and new routes to deliver illegal goods or services, enforcing new investigation techniques to Law Enforcement Agencies (LEAs). This paper formerly analyses the goods/services categories of fourteen Tor marketplaces and the related vendors, while the last one provides a discussion on a novel investigative technique related to PGP Keys inter-relations. In particular, with the evolution/growth of the markets, the vendors are increasingly adopting open source tools and technologies, as PGP, which can be exploited to infer information such as the established relationships between users. This public information about the keys can be used to retrace social network of entities connected by PGP relationship and apply well-established graph analysis techniques. Finally, the paper analyses the strength and weaknesses of proposed methods, depicting future research directions.
Junk mail or spam mail has been regarded as a major problem in today's world. The spam mail can lead to cybercrime that impacts all individuals and organization. Many people and businesses seek for spam mail prevention technique in order to protect their own data and computer system. The spam mails normally contain advertise products or services contents and also conveys viruses, malwares, spywares and so forth. Many people thought spam mails do not cause any damage. In fact, the spam mails made a management cost increased and resources will be used ineffectively. Therefore, verifying and filtering spam mails need to be taken into consideration. The objective of this paper is to introduce the hybrid approach, which combines three techniques including stop-word removal, n-gram extraction and data classification, for filtering spam emails and simplifies system development. The proposed hybrid approach can be widely applied for all different languages due to being language independent technique. To examine the approach, CSDMC2010 spam mail corpus comprising of 198 common emails, 202 spam mails, and 10 selective emails were used in experimental study. The results showed that the proposed technique enabled to monitor whether the email is spam with 93.2% accuracy. Hence, this hybrid approach could provide benefits for all users and organization to decrease the computer risk.
Purpose - The purpose of the study is to conduct a scientometric analysis of cybersecurity literature indexed in the core collection of the Web of Science for a period of ten years (2011-2020). Design/methodology/approach - Cybersecurity is a focused topic of research across the globe. To identify the global research productivity in the field, the terms cybersecurity, cyber-security, web security, information security, computer security, etc. were used for retrieving the publications in the advanced search mode of the database Web of Science, limiting the time frame for 2011-2020. The results retrieved were downloaded in the Excel file for further analysis and interpretation. The harvested data was analysed by using scientometric techniques to measure the progress such as growth rate, doubling time and author collaborations. Besides, the Biblioshiny and VOSviewer software were used for mapping networks. Findings - The research output in the field of cybersecurity has shown an increasing trend during 2011-2020, and the maximum number of scholarly publications was published in 2020 (1,581), i.e. more than 715% of 2011 (221). A good number of countries (93) have contributed globally in cybersecurity research, and the highest share in research publications was reported by the USA (23.55%), followed by China (23.24%), South Korea (5.31%), UK (5.28%) and India (4.25%). The authorship patterns in cybersecurity publications show a collaborative trend, as most articles have been published by multiple authors. Total 5,532 (90.14%) articles have been published in co-authorship, whereas only 605 (9.86%) articles have been published by single authors. Keyword analysis shows that the most common keyword research by the authors is cybersecurity and its variants such as cyber security and cyber-security (1,698) followed by security (782), computer security (680) and information security (329). Research limitations/implications - The database studied for the work does not represent the total literary output available on the theme. There are plenty of other databases, such as Scopus, Compendex, INSPEC, IEEE Xplore, arXiv, contributing to the same theme as well. Practical implications - The findings of the study may help researchers, information technologists, library professionals and information specialists to identify the research progress, authorship patterns, collaborative networks and hot topics of research in the field of cybersecurity. Besides, it will assess the global response to the cybersecurity issue. Originality/value - The study is the scientometric analysis of the cybersecurity based on current literature and will highlight the progress and development of global research in the said field.
Emails are often used to illegal cybercrime today, so it is important to verify the identity of the email author. This paper proposes a general model for solving the problem of anonymous email author attribution, which can be used in email authorship identification and email authorship verification. The first situation is to find the author of an anonymous email among the many suspected targets. Another situation is to verify if an email was written by the sender. This paper extracts features from the email header and email body and analyzes the writing style and other behaviors of email authors. The behaviors of email authors are extracted through a statistical algorithm from email headers. Moreover, the author's writing style in the email body is extracted by a sequence-to-sequence bidirectional long short-term memory (BiLSTM) algorithm. This model combines multiple factors to solve the problem of anonymous email author attribution. The experiments proved that the accuracy and other indicators of proposed model are better than other methods. In email authorship verification experiment, our average accuracy, average recall and average F1-score reached 89.9 %. In email authorship identification experiment, our model's accuracy rate is 98.9% for 10 authors, 92.9% for 25 authors and 89.5% for 50 authors.
"The 21st century is called the digital century. The digital world and modern mobile digital technologies are everywhere today. This shift in technology development brings not only huge benefits but also huge risks. Digital crime is one of the fastest-growing criminal activities, with more than a million people becoming victims of it worldwide every day. The peculiarity of these crimes is that, due to the development of digital technologies, the content of crime has changed; and most of it is committed in a virtual environment. For many years, the international community and states have been attempting to counter the commission of crimes using computer technology. Conventions, agreements, recommendations, and laws are adopted. But at present, they do not fully reflect the situation in this area. No less attention is paid to the problems caused by using modern technologies in the scientific literature. But there is no unity of opinion, most of the writings use such mostly outdated terms as computer crime, cybercrime, etc. At the same time, an analysis of the state of crime in this field, as well as of legislation and scientific literature, shows that there is a need for clarification of the existing concept of computer crime, expanding it to the concept of the digital crime."
Electronic transactions rank the top on our daily transactions. Internet became invaluable for government, business, and personal use. This occurred in synchronization with the great increase in online attacks, particularly the development of newest forms from known attacks such as Tabnabbing. Thus, users' confidentiality and personal information must be protected using information security. Tabnabbing is a new form of phishing. The attacker needs nothing to steal credentials except users' preoccupation with other work and exploitation of human memory weakness. The impact of this malicious attempt begins with identity theft and ends with financial loss. That has encouraged some security specialists and researchers to tackle tabnabbing attack, but their studies are still in their infancy and not sufficient. The work done here focuses on developing an effective anti-tabnabbing extension for the Google Chrome browser to protect Internet users from been victims as well as raise their awareness. The system developed has a novel significance due to its effectiveness in detecting a tabnabbing attack and the combination of two famous approaches used to combat online attacks. The success of the system was examined by performance measurements such as confusion matrix and ROC. The system produces promising results.
"Using e-services in Saudi Arabia is growing. Using such services offers a wide range of benefits and makes people's life easier. However, the development and the deployment of these e-services on the Internet increase the likelihood of exposure to cyber-attacks. Attackers take advantage of vulnerabilities in these e-services. Vulnerabilities arise as a result of weaknesses in the programming, miss-configuration or lack of updates. It is unfortunate that only little effort is done to evaluate the security posture of Saudi Arabia's websites. In this paper, 150 Financial, Academic, Governmental and commercial organizations websites were assessed using open source tools. In addition, a comparison between governmental and commercial websites was done based on the numbers of vulnerabilities found. The results show that Saudi Arabia's websites suffer from high, medium and low impact vulnerabilities. For example; 17.5% of websites are vulnerable to SQL injection, 13.5% are vulnerable to Shell injection, and 61% are vulnerable to Clickjacking. Finally, the evaluation showed that commercial websites are more secure than governmental websites."
The social impact of technology has become one of the most challenging research issues. One of the most famous information technology is BitTorrent, a protocol for peer to peer file sharing. This paper discusses an ethical issue related to Peer to Peer file sharing using BitTorrent protocol of copyrighted materials. We choose an academic approach by choosing an ethical analysis tool that allows pros and cons opinions amongst different points of view. We studied this issue in Saudi community by distributing the questionnaires and analyzing its results. The result reveals that a large percentage of the participants were using Torrent to download movies and music illegally and half of them do not even realize that it is considered as illegal. Hence, it is suggested that in order to reduce the illegal use of a promising technology is to raise the awareness of the issue to the society.
With the rapid increase of published open datasets, it is crucial to support the open data progress in smart cities while considering the open data quality. In the Czech Republic, and its National Open Data Catalogue (NODC), the open datasets are usually evaluated based on their metadata only, while leaving the content and the adherence to the recommended data structure to the sole responsibility of the data providers. The interoperability of open datasets remains unknown. This paper therefore aims to propose a novel content-aware quality evaluation framework that assesses the quality of open datasets based on five data quality dimensions. With the proposed framework, we provide a fundamental view on the interoperability-oriented data quality of Czech open datasets, which are published in NODC. Our evaluations find that domain-specific open data quality assessments are able to detect data quality issues beyond traditional heuristics used for determining Czech open data quality, increase their interoperability, and thus increase their potential to bring value for the society. The findings of this research are beneficial not only for the case of the Czech Republic, but also can be applied in other countries that intend to enhance their open data quality evaluation processes.
For decades, the success of the similarity search has been based on a detailed quantification of pairwise similarity of objects. Currently, the search features have become much more precise but also bulkier, and the similarity computations more time-consuming. While the k nearest neighbours (kNN) search dominates the real-life applications, we claim that it is principally free of a need for precise similarity quantifications. Based on the well-known fact that a selection of the most similar alternative out of several options is a much easier task than deciding the absolute similarity scores, we propose the search based on an epistemologically simpler concept of relational similarity. Having arbitrary objects q, o(1) , o(2) from the search domain, the kNN search is solvable just by the ability to choose the more similar object to q out of o(1), o(2) - the decision can also contain a neutral option. We formalise such searching and discuss its advantages concerning similarity quantifications, namely its efficiency and robustness. We also propose a pioneering implementation of the relational similarity search for the Euclidean spaces and report its extreme filtering power in comparison with 3 contemporary techniques.
As the complexity of Internet services, transmission speed, and data volume increases, current IP flow monitoring and analysis approaches cease to be sufficient, especially within high-speed and large-scale networks. Although IP flows consist only of selected network traffic features, their processing faces high computational demands, analysis delays, and large storage requirements. To address these challenges, we propose to improve the IP flow monitoring workflow by stream-based collection and analysis of IP flows utilizing a distributed data stream processing. This approach requires changing the paradigm of IP flow data monitoring and analysis, which is the main goal of our research. We analyze distributed stream processing systems, for which we design a novel performance benchmark to determine their suitability for stream-based processing of IP flow data. We define a stream-based workflow of IP flow collection and analysis based on the benchmark results, which we also implement as a publicly available and open-source framework Stream4Flow. Furthermore, we propose new analytical methods that leverage the stream-based IP flow data processing approach and extend network monitoring and threat detection capabilities.
Effective triage is of utmost importance for cyber-security incident response, namely in handling ransomware or similar incidents in which the attacker may use self-propagating worms, infected files, or email attachments to spread malware. If a device is infected, it is vital to know which other devices can be infected too or are immediately threatened. The number and heterogeneity of devices in today's network complicate situational awareness of incident handlers, and, thus, we propose a recommender system that uses network monitoring data to prioritize devices in the network based on their similarity and proximity to an already infected device. The system enumerates devices in close proximity in terms of physical and logical network topology and sorts them by their similarity given by the similarity of their behavioral profile, fingerprint, or common history. The incident handlers can use the recommendation to promptly prevent malware from spreading or trace the attacker's lateral movement.
Hands-on computing education requires a realistic learning environment that enables students to gain and deepen their skills. Available learning environments, including virtual and physical laboratories, provide students with real-world computer systems but rarely adapt the learning environment to individual students of various proficiency and background. We design a unique and novel smart environment for the adaptive training of cybersecurity skills. The environment collects a variety of student data to assign a suitable learning path through the training. To enable such adaptiveness, we propose, develop, and deploy a new tutor model and a training format. We evaluate the learning environment using two different adaptive trainings attended by 114 students of various proficiency. The results show that students were assigned tasks with a more appropriate difficulty, which enabled them to successfully complete the training. Students reported that they enjoyed the training, felt the training difficulty was appropriately designed, and would attend more training sessions like these. Instructors can use the environment for teaching any topic involving real-world computer networks and systems because it is not tailored to particular training. We freely release the software along with exemplary training so that other instructors can adopt the innovations in their teaching practice.
The integration of renewable sources, communication and power networks with information and communication technologies is one of the main challenges in Smart Grids (SG) large-scale testing. For this reason, the coupling of simulators is commonly used to dynamically simulate several aspects of the SG infrastructure, in the so-called co-simulations. In this paper, we provide a scoping review of research of co-simulations in the context of Smart Grids: i) research areas and research problems addressed by co-simulations, ii) specific co-simulation aspects focus of research, iii) typical coupling of simulators in co-simulation studies. Based on the results, we discuss research directions of future SG co-simulation research in each of the identified areas.
There are different requirements on cybersecurity of industrial control systems and information technology systems. This fact exacerbates the global issue of hiring cybersecurity employees with relevant skills. In this paper, we present KYPO4INDUSTRY training facility and a course syllabus for beginner and intermediate computer science students to learn cybersecurity in a simulated industrial environment. The training facility is built using opensource hardware and software and provides reconfigurable modules of industrial control systems. The course uses a flipped classroom format with hands-on projects: the students create educational games that replicate real cyber attacks. Throughout the semester, they learn to understand the risks and gain capabilities to respond to cyber attacks that target industrial control systems. Our described experience from the design of the testbed and its usage can help any educator interested in teaching cybersecurity of cyber-physical systems.
The increasing adoption of Internet-of-Things (IoT) devices present new challenges to digital forensic investigators and law enforcement agencies when investigation into cybercrime on these new platforms are required. However, there has been no formal study to document actual challenges faced by investigators and whether existing tools help them in their work. Prior issues such as the correlation and consistency problem in digital forensic evidence have also become a pressing concern in light of numerous evidence sources from IoT devices. Motivated by these observations, we conduct a user study with 39 digital forensic investigators from both public and private sectors to document the challenges they faced in traditional and IoT digital forensics. We also created a tool, STITCHER, that addresses the technical challenges faced by investigators when handling loT digital forensics investigation. We simulated an IoT crime that mimics sophisticated cybercriminals and invited our user study participants to utilize STITCHER to investigate the crime. The efficacy of STITCHER is confirmed by our study results where 96.2% of users indicated that STITCHER assisted them in handling the crime, and 61.5% of users who used STITCHER with its full features solved the crime completely. (C) 2020 Elsevier Ltd. All rights reserved.
An increased use of data driven applications and integrated systems have caused an accelerating expansion in data volumes and increase in the number of digital records, over the past few decades. Exponentially growing data volumes being processed by large-scale distributed data-intensive applications have placed an increasing pressure on the underlying storage services for timely and efficient storage and retrieval of the data. The use of cloud storage is among the best strategies to efficiently store growing volumes of data. However, outsourcing data to public cloud storage leads to the challenge of data confidentiality preservation. Data Confidentiality is among the top challenges associated with cloud storage which have contributed substantially as an inhibitor for cloud computing adoption all over the world and is considered a serious concern, especially in case of big data, where securing data in a timely and accurate manner is an arduous task. Our study aims to contribute in anti-cybercrime by protecting the confidentiality of sensitive growing data. We enunciate an optimized confiden-tially preserving framework on distributed cloud storage, that works for growing data with time-efficiency and minimum memory usage. Our framework uses a merger of Genetic Algorithm (GA), parallel data distribution, and privacy-aware selective encryption techniques. The experiments and comparative analysis depict that our proposed framework outperforms others under consideration, in terms of execution time, memory usage and network throughput respectively.
Cybercrime has led to the loss of billions of dollars, the malfunctioning of computer systems, the destruction of critical information, the compromising of network integrity and confidentiality, etc. In view of these crimes committed on a daily basis, the security of the computer systems has become imperative to minimize and possibly avoid the impact of cybercrimes. In this paper, we review recent advances in the use of cyber security benchmark datasets for the evaluation of machine learning and data mining-based intrusion detection systems. It was found that the state-of-the-art cyber security benchmark datasets KDD and UNM are no longer reliable, because their datasets cannot meet the expectations of current advances in computer technology. As a result, a new ADFA Linux (ADFA-LD) cyber security benchmark dataset for the evaluation of machine learning and data mining-based intrusion detection systems was proposed in 2013 to meet the current significant advances in computer technology. ADFA-LD requires improvement in terms of full descriptions of its attributes. This review can be used by the research community as a basis for abandoning the previous state-of-the-art cyber security benchmark datasets and starting to use the newly introduced benchmark dataset for effective and robust evaluation of machine learning and data mining-based intrusion detection system. (C) 2015 The Authors. Published by Elsevier B.V.
Sequential pattern mining, which is one of the core tasks in data mining, allows to gain insight into datasets with complex sequential data. As the task is computationally intensive, there are many different approaches that are suitable for various types of data. We explore the possibility of optimising the analysis of sequences based on the characteristic (quickly obtainable) properties of the analysed data. In this paper, we propose five such characteristics and explore the efficiency of three algorithms that are representatives of the three main approaches to sequential pattern mining. We discovered that it is possible to save up to 21% of the search time compared to the best-performing representative. We trained a decision tree model with 87% accuracy of choosing the best algorithm for selected data based on these characteristics.
We present a new approach to tackle the problem of lattice-type metamorphic robots reconfiguration. We base our approach on a reduction to satisfiability modulo theory (SMT). Unlike the current state-of-the-art solutions, we consider the spatial limitations of the modules themselves and produce collision-free plans. We give an in-depth description of the reduction and discuss several optimizations for our technique. We also show an experimental evaluation of our approach and list possible future improvements to our technique.
Software engineers widely acknowledge the inclusion of security requirements in the early stages of the development process. However, the need to prepare the software for the failure of the implemented security controls and subsequent investigation of the incident is often not discussed. Forensic-ready software systems represent an evolution of secure systems being designed for the eventual digital forensic investigation. However, their exact properties remain largely unexplored, beyond preliminary high-level conceptualizations of requirements and capabilities. Further obstacles hindering the adoption of forensic-ready software systems are the different priorities and goals of involved parties and a gap in the digital forensics expertise of software engineers. In this paper, we conduct an empirical qualitative study identifying the problems and needs of forensic readiness while framing the notion of an ideal forensic-ready software system and how it should treat potential evidence. To this end, we conducted semisupervised interviews with digital forensics experts on their idea, experience, and suggestions. The results provide insights into the needs of the experts to facilitate the definition of correct requirements towards forensic-ready software systems to support the anticipated investigations properly.
Unquestionably, WhatsApp is one of the most pervasive instant message application programs in contemporary digital era. It could be utilized on mobile smart phones or desktop computing devices including Windows-based personal computers and MacBook with Mac OS X. On account of the tremendous amount of information security incidences occurring concerning the usage of WhatsApp program on a MacBook, this research work constructed several cases to validate the recoverability of digital evidences on the computing device. The recoverability of WhatsApp user ID, username, and the contents of instant message dialogues were discussed based on the physical memory acquisition of the MacBook from digital forensics point of view. In addition, by the inherent characteristics and the executing mechanism of the WhatsApp program, the manufacture and the mode of the smartphone was capable of being disclosed by the digital traces accordingly. All the aforementioned disclosed digital traces could be probative digital evidences in a court of law. Therefore, this research work could be substantively applied and extended to other mushrooming cybercrime investigations regarding instant message incidents in the public sector or the noncompliance of computing resource usages in the private sector. The procedures of physical memory acquisition should be scientifically premeditated and systematically conducted due to the volatility of the physical memory of a computing device with rigorous procedures.
Blockchains have emerged over time as a reliable and secure way to record transactions in an immutable manner in a wide range of application domains. However, current related solutions are not yet capable of appropriately checking the authenticity of data when their volumes are huge. They are not also capable of updating Blockchain data blocks and synchronizing them within reasonable timeframes. This is the case within the specific context of Blockchain vehicular networks, where these solutions are commonly cumbersome when attempting to add new vehicles to the network. In order to address these problems, we propose in this paper a new Blockchain-based solution that intelligently implement selective communication and collaborative endorsement approaches to reduce communications between vehicles. Our solution represents the vehicles of the Blockchain as intelligent software agents with a Belief-Desire-Intention (BDI) architecture. Furthermore, we propose an approach based on multi-endorsement levels to exchange data of varying sensitive categories. This approach, which is based on endorsing scores, is also used to shorten the admission of new vehicles into the Blockchain. We run simulations using the Hyperledger Fabric Blockchain tool. Results show the efficiency of our solution in reducing the processing times of transactions within two different scenarios.
The proliferation of malware in recent times have accounted for the increase in computer crimes and prompted for a more aggressive research into improved investigative strategies, to keep up with the menace. Recent techniques and tools that have been developed and adopted to keep up in an arms race with malware authors who have resorted to the use of evasive techniques to avoid analysis during investigation is an on-going concern. Exploring dynamic analysis is unarguably, a positive step to supporting static evidence with malware dynamic behaviour logs. In view of this, analysing this huge generated reports raises concerns about speed, accuracy and performance. This research proposes an Automated Malware Investigative Framework Model, a component based approach that is designed to support investigation by integrating both malware analysis and data mining clustering techniques as part of an effort to solve the problem of overly generated reports. Thus, grouping analysed suspicious samples that exhibit similar behavioural features to make investigation easy and more intuitive. The focus of this paper however, is on implementing sub-components of the framework that directly deals with the problem at hand.
Recent studies have investigated the link between online advertising and rogue websites promoting film piracy, in several different countries around Asia-Pacific. The key findings are that high risk advertising dominates sites which are geared towards Hollywood TV and movies in English, yet mainstream advertising tends to be more prevalent on sites in local languages with local content. In this first systematic analysis of music piracy, the prevalence of high risk and mainstream advertising on music rogue sites was estimated. Advertising was measured from 3,210 web pages identified as infringing by Google, according to the DMCA, and advertisements were downloaded with a New Zealand IP address, to replicate what New Zealand users would have seen. New Zealand was selected because the founder of the Megaupload file hosting website - previously among the Top 100 websites worldwide - has recently been the subject of a police investigation into criminal copyright infringement. The results show that 93% of advertisements were high risk, and only 7% were mainstream. Disturbingly, 97.24% of the high risk ads were for malware - much higher than for movie websites - suggesting that Kiwis are at extreme risk for malware infection if they visit these sites. Further research on the malware samples downloaded needs to be undertaken to determine if rogue music websites are a vector for banking malware.
We present a dataset of 13446 shell commands from 175 participants who attended cybersecurity training and solved assignments in the Linux terminal. Each acquired data record contains a command with its arguments and metadata, such as a timestamp, working directory, and host identification in the emulated training infrastructure. The commands were captured in Bash, ZSH, and Metasploit shells. The data are stored as JSON records, enabling vast possibilities for their further use in research and development. These include educational data mining, learning analytics, student modeling, and evaluating machine learning models for intrusion detection. The data were collected from 27 cybersecurity training sessions using an open-source logging toolset and two open-source interactive learning environments. Researchers and developers may use the dataset or deploy the learning environments with the logging toolset to generate their own data in the same format. Moreover, we provide a set of common analytical queries to facilitate the exploratory analysis of the dataset. (C) 2021 The Author(s). Published by Elsevier Inc.
Smart Grids (SG) represent one of the key critical infrastructures. Over time, several ontologies were defined in the SG domain to model aspects such as devices and sensors integration, and prosumers' communication needs. In this paper, we review the state of the art regarding semantic web reasoning in the domain of SGs. We compare five main ontologies in terms of descriptive statistics (e.g., number of axioms), load time and reasoners runtime performance. Results show that not all the ontologies in the SG domain are readily available, and that some of them might be more appropriate for deployment in devices with limited computational resources.
At this time the economy around the world was down due to the impact of the covid-19 virus which caused many people to lose their jobs and find it difficult to find work. The Economic Crisis has made people put emotion ahead of logic, in meeting their needs by accepting offers to borrow money from illegal online lending which provides very high administration and interest expenses. One of the factors that also causes people to fall into online lending is a lifestyle that is too high, namely the consumptive nature of shopping excessively not according to their needs or what is called hedonism, which makes them the target of online lenders. The existence of an easy disbursement system, attractive offers, and coupled a lack of financial literacy has made many people ensnared. This gives interest to the author to analyze the fraud committed by illegal online lending actors. The results of the study showed indications of fraud that caused losses. The losses experienced are not only financial but also non-financial. Financial losses experienced due to high admin fees, high interest, the imposition of unlimited fines, and the existence of cybercrime. Non-financial losses experienced were defamation, dissemination of personal data, threats, sexual harassment, and intimidation from illegal lenders. With these losses, illegal online lending users can become stressed, panicked, and can even cause someone to commit suicide [1].
Many companies are making the move to a cloud based environment for data storage and management. Having their data in the cloud has many benefits in that it may help the company move forward and innovate. The embracing of cloud-based services by corporates, businesses, and people, has helped to usher in a paradigm shift in the people-data-service relationship. While most steps in the evolution of technology center around development and productivity, given the changing scenario of threats and cybercrime, security in the cloud needs to be thoroughly analyzed, researched, and mitigated, even if it cannot be completely eliminated or avoided. However, cloud-based environments are not completely safe from attacks. Criminals are always looking for ways to make money through malicious activity. Cyber Security is already one of the fastest growing fields in the modern world and the number of incidents that occur on a daily basis are continuing evidence of its necessity. Systems security of all types have been addressed in similar ways but cloud-base environments offer quite a few unique threats that force professionals to become creative when preparing mitigation techniques. This paper introduce a cloud computing security analysis survey where we list out some of the grave security threats that the Workload Distribution and Resource Pooling Architecture in Cloud Systems model faces, and some mitigation techniques to encounter them.
In recent years, cybercrime activities have grown significantly, compromising device security and jeopardizing the normal activities of enterprises. The profits obtained through intimidation and the limitations for tracking down the illegal transactions have created a lucrative business based on the hijacking of users' files. In this context, ransomware takes advantage of cryptography to compromise the user information or deny access to the operating system. Then, the attacker extorts the victim to pay a ransom in order to regain access, recover the data, or keep the information private. Nowadays, the adoption of Situational Awareness (SA) and cognitive approaches can facilitate the rapid identification of ransomware threats. SA allows knowing what is happening in compromised devices and network communications through monitoring, aggregation, correlation, and analysis tasks. The current literature provides some parameters that are monitored and analyzed in order to prevent these kinds of attacks at an early stage. However, there is no complete list of them. To the best of our knowledge, this paper is the first proposal that summarizes the parameters evaluated in this research field and considers the SA concept. Furthermore, there are several articles that tackle ransomware problems. However, there are few surveys that summarize the current situation in the area, not only regarding its evolution but also its issues and future challenges. This survey also provides a classification of ransomware articles based on detection and prevention approaches.
In most cases, one of the major goals of behavioral malware analysis is to extract from malware samples the intelligence critical to identify the nature of malware. Given the increasing complexity of operating system and its services that creates a significant volume of noise in the background, malware intelligence gathering process is required to minimise non malware related information while not missing malware related events. Another highly demanded attribute in behavioral analysis is the analysis scalability that enables automated analysis on sizeable sample volumes. It is also desirable to perform the analysis in unobstructed manner providing resiliency to rootkits and analysis-specific evasion techniques. This paper introduces an efficient behavioral malware analysis method, Execution Tracking, that creates critical malware intelligence with minimum volume of unnecessary information and maximum accuracy, which helps acquiring baseline information for further deep analysis, automating analysis process for a high malware volume lab, and producing strategies to mitigate the threats. This method is demonstrated by a reference implementation, Malware Expert.
Platform as a Service (PaaS) cloud domain brings great benefits of an elastic platform with many prefabricated services, but at the same time challenges software architects who need to navigate a rich set of services, variability of PaaS cloud environment and quality conflicts in existing design tactics, which makes it almost impossible to foresee the impact of architectural design decisions on the overall application quality without time-consuming implementation of application prototypes. To ease the architecture design of PaaS cloud applications, this paper proposes a design-time quality evaluation approach for PaaS cloud applications based on automatically generated prototypes, which are deployed to the cloud and repeatedly evaluated in the context of multiple quality attributes and environment configurations. In this paper, all steps of the approach are described and demonstrated on an example of a real-world complex IoT system for collection and processing of Smart Home sensor data. The approach has been implemented and the automated prototype generation and evaluation tool, referred to as PaaSArch Cloud Prototyper, is presented together with the approach. (C) 2020 Elsevier Inc. All rights reserved.
Fingerprinting a host's operating system is a very common yet precarious task in network, asset, and vulnerability management. Estimating the operating system via network traffic analysis may leverage TCP/IP header parameters or complex analysis of hosts' behavior using machine learning. However, the existing approaches are becoming obsolete as network traffic evolves which makes the problem still open. This paper discusses various approaches to passive OS fingerprinting and their evolution in the past twenty years. We illustrate their usage, compare their results in an experiment, and list challenges faced by the current fingerprinting approaches. The hosts' differences in network stack settings were initially the most important information source for OS fingerprinting, which is now complemented by hosts' behavioral analysis and combined approaches backed by machine learning. The most impactful reasons for this evolution were the Internet-wide network traffic encryption and the general adoption of privacy-preserving concepts in application protocols. Other changes, such as the increasing proliferation of web applications on handheld devices, raised the need to identify these devices in the networks, for which we may use the techniques of OS fingerprinting.
The design and development of secure systems is an important and challenging task. However, such systems should also be prepared for eventual disputes or occurrences of a security incident. To solve this, forensicready software systems are, by-design, prepared to assist in the forensic investigation and to provide on-point data with high evidentiary value. However, software engineering support for the systematic development of such software systems is rather sparse. This paper tackles the problem by introducing novel modelling notation, called BPMN for Forensic-Ready Software Systems (BPMN4FRSS), including its syntax and semantics. The notation aims to capture the forensic-ready controls and enable reasoning over them, primarily focusing on potential digital evidence. Importantly, it is made to support forensic readiness oriented risk management decisions. The approach is then demonstrated in a scenario where the controls, which mitigate security and business risks, are properly represented.
Filtering is a fundamental strategy of metric similarity indexes to minimise the number of computed distances. Given a triple of objects for which distances of two pairs are known, the lower and upper bounds on the third distance can be set as the difference and the sum of these two already known distances, due to the triangle inequality rule of the metric space. For efficiency reasons, the tightness of bounds is crucial, but as angles within triangles of distances can be arbitrary, the worst case with zero and straight angles must also be considered for correctness. However, in data of real-life applications, the distribution of possible angles is skewed and extremes are very unlikely to occur. In this paper, we enhance the existing definition of bounds on the unknown distance with information about possible angles within triangles. We show that two lower bounds and one upper bound on each distance exist in case of limited angles. We analyse their filtering power and confirm high improvements of efficiency by experiments on several real-life datasets.
This research addresses adding the encryption technic and a digital signature technic to web-based electronic archive in order to prevent cybercrime problem such as robbery, modification and unauthorized access. The Secure Electronic Archive Application intends to combine given services such as confidentiality, integrity, authentication, and non repudiation. Encryption is used to give a confidentiality service with 128 bit AES algorithm. In this research encryption implements to give a confidential service with 128 bit AES algorithm. AES 128 bit algorithm has not been exposed to related key attack. The digital signature is used to provide integrity, authentication, and non repudiation with SHA 256 bit and RSA 1024 bit. The Application will be secured by entering the archive name, symmetric key, private key, and passphrase. Implemented in web programming, Analysis of the application will be explained by a use case diagram and the testing would be web testing. Such as Functional testing, Usability Testing, Interface Testing, Compatibility Testing, Performance Test, and Security Test. Encryption implementation in this research can prevent archive thievery which is shown on implementation and proved on web testing. As the result of Functional Testing, the archive can not be read after it has been encrypted. Therefore, the aplication can give information about archive modification and authentication with showing a notification after the sign has been done.
Authentication mechanisms are at the forefront of defending the world from various types of cybercrime. Steganography can serve as an authentication solution through the use of a digital signature embedded in a carrier object to ensure the integrity of the object and simultaneously lighten the burden of metadata management. Nevertheless, despite being generally imperceptible to human sensory systems, any degree of steganographic distortion might be inadmissible in fidelity-sensitive situations such as forensic science, legal proceedings, medical diagnosis and military reconnaissance. This has led to the development of reversible steganography. A fundamental element of reversible steganography is predictive analytics, for which powerful neural network models have been effectively deployed. Another core element is reversible steganographic coding. Contemporary coding is based primarily on heuristics, which offers a shortcut towards sufficient, but not necessarily optimal, capacity-distortion performance. While attempts have been made to realise automatic coding with neural networks, perfect reversibility is unattainable via such learning machinery. Instead of relying on heuristics and machine learning, we aim to derive optimal coding by means of mathematical optimisation. In this study, we formulate reversible steganographic coding as a nonlinear discrete optimisation problem with a logarithmic capacity constraint and a quadratic distortion objective. Linearisation techniques are developed to enable iterative mixed-integer linear programming. Experimental results validate the near-optimality of the proposed optimisation algorithm when benchmarked against a brute-force method.
Due to the interoperability difficulty and development bottleneck of various services in a city, the effective design of services has become a critical concern in smart city ecosystems. Based on the interconnection concept of Management by Competencies, this paper proposes an DISDA architecture to facilitate the digital service design in a smart city ecosystem. The DISDA architecture not only can guide the users to design the services with the defined processes but also can measure the maturity of the existing services and determine possible service improvements. Based on the proposed service design architecture, we conduct a case study to validate the usability and applicability of the proposed service design architecture.
Catastrophic events, such as cyclones, floods, droughts, terrorism, or cybercrime, are astronomically on the increase the world over. These events disrupt businesses' smooth continuity leading to reputational digital data and financial losses among others. Zimbabwe's districts of Chimanimani and Chipinge in March 2019 experienced a catastrophic Cyclone Idai that highly disrupted various important business activities and the associated supply chains. This study, therefore, focuses on the impact of business continuity and organizational performance on mitigating the disruptive effects on major supply chains during a disaster. Ordinary Least Squares (OLS) regression model was used to analyse the relationship between supply chain disruption and business continuity. The study had a population of 82 humanitarian organizations and the researchers successfully administered questionnaires to a sample of 65 humanitarian organizations that participated in relief operations during Cyclone Idai in Zimbabwe. The results show that business continuity has a negative and significant effect on supply chain disruption. At 5% level of significance, business continuity has a positive effect of about 8%. This means that a marginal change in business continuity will result in significant 8% influence on mitigation of supply chain disruption in humanitarian relief efforts. The study findings will be useful to practitioners such as supply chain managers in coming up with strategies in case of supply chain disruption threats due to unseen shocks.
Cybercrime has increased considerably in recent times by creating new methods of stealing, changing, and destroying data in daily lives. Portable Docu-ment Format (PDF) has been traditionally utilized as a popular way of spreading malware. The recent advances of machine learning (ML) and deep learning (DL) models are utilized to detect and classify malware. With this motivation, this study focuses on the design of mayfly optimization with a deep belief network for PDF malware detection and classification (MFODBN-MDC) technique. The major intention of the MFODBN-MDC technique is for identifying and classify-ing the presence of malware exist in the PDFs. The proposed MFODBN-MDC method derives a new MFO algorithm for the optimal selection of feature subsets. In addition, Adamax optimizer with the DBN model is used for PDF malware detection and classification. The design of the MFO algorithm to select features and Adamax based hyperparameter tuning for PDF malware detection and classi-fication demonstrates the novelty of the work. For demonstrating the improved outcomes of the MFODBN-MDC model, a wide range of simulations are exe-cuted, and the results are assessed in various aspects. The comparison study high-lighted the enhanced outcomes of the MFODBN-MDC model over the existing techniques with maximum precision, recall, and F1 score of 97.42%, 97.33%, and 97.33%, respectively.
This article explores the extent to which Covid-19 has impacted the trajectory of EU Cybersecurity Policy. The Covid-19 crisis has led to an unprecedent reliance on digital solutions, ranging from teleworking to virus-tracking systems, resulting in the proliferation of Covid-19 related cybercrime, critical information infrastructure attacks and dissemination of pandemic disinformation. Although the virus has been repeatedly portrayed as life altering and as having considerably increased the cybersecurity risks faced by States, businesses and citizens, the proposed solutions, however, have accelerated existing trends in the field rather than resulting in significant institutional change. In particular, there has been a reinforcement of the role as a coordinating actor, of the introduction of further coherence between sub-areas and instruments, and of the positioning of public-private partnerships at the heart of the policy. However, where the role of social media platforms in facilitating the spread of disinformation is concerned, a changing trust relationship has resulted in a discursive shift in which these platforms require greater oversight, a belief reinforced by the spread of Covid-19 disinformation. The article proposes, through the lenses of historical and discursive institutionalism, that the EU's response to Covid-19 in the field of cybersecurity can only be understood in light of these pre-existing trends, which are the result of an economic and security path dependence that emerged in the 1980s.
"Digital world has produced efficiencies, new innovative products, and close customer relationships globally by the effective use of mobile, IoT (Internet of Things), social media, analytics and cloud technology to generate models for better decisions. Blockchain is recently introduced and revolutionizing the digital world bringing a new perspective to security, resiliency and efficiency of systems While initially popularized by Bitcoin, Blockchain is much more than a foundation for crypto currency. It offers a secure way to exchange any kind of good, service, or transaction. Industrial growth increasingly depends on trusted partnerships; but increasing regulation, cybercrime and fraud are inhibiting expansion. To address these challenges, Blockchain will enable more agile value chains, faster product innovations, closer customer relationships, and quicker integration with the IoT and cloud technology. Further Blockchain provides a lower cost of trade with a trusted contract monitored without intervention from third parties who may not add direct value. It facilitates smart contracts, engagements, and agreements with inherent, robust cyber security features This paper is an effort to break the ground for presenting and demonstrating the use of Blockchain technology in multiple industrial applications. A healthcare industry application, Healthchain, is formalized and developed on the foundation of Blockchain using IBM Blockchain initiative. The concepts are transferable to a wide range of industries as finance, government and manufacturing where security, scalability and efficiency must meet."
In this article, we discuss the issues of GDPR's impact on cyber-security software and operations, namely automated information sharing. We illustrate the topic on an example of an intrusion detection alert sharing platform. First, we had to investigate the risks to privacy in the alert sharing platform and ensure its compliance with the GDPR's obligations. Second, fears and uncertainties emerged in the alert sharing community regarding the GDPR and its obligations and, thus, willingness to share the information was negatively impacted. We conducted DPIA to investigate risks related to information sharing in cyber security and dismiss doubts within the community. Although our results suggest that the risks are not high, we point out that the hype around GDPR caused substantial development of the sharing platform. The DPIA helped in a deeper understanding of risks and their management and is a solid argument for information sharing in cyber security under GDPR.
We present RoFICoM, a new retractable connection device that allows for mechanical, electric, and data communication connection between separable robotic modules. The device is intentionally designed to be used in lattice-type metamorphic robots, however, its applicability is much wider. The main novelty of our solution lies primarily in a new unique flat design and spatial compactness of the connector. With a flat connector, much more space is left for the body of a robotic module in the structure. Moreover, the connector is also fully self-contained device with well defined mechanical, electrical and data interfaces, hence it can be easily embedded in various robotic solutions. Our RoFICoM connector is easy to produce, it is open-hardware and free for non-commercial use. In the paper, we give construction details and report on a couple of experiments we performed to demonstrate key features of the connection achieved with two RoFICoM devices.
Pivoting is a technique used by cyber attackers to exploit the privileges of compromised hosts in order to reach their final target. Existing research on countering this menace is only effective for pivoting activities spanning within the internal network perimeter. When applying existing methods to include external traffic, the detection algorithm produces overwhelming entries, most of which unrelated to pivoting. We address this problem by identifying the major characteristics that are specific to potentially malicious pivoting. Our analysis combines human expertise with machine learning and is based on the inspection of real network traffic generated by a large organization. The final goal is the reduction of the unacceptable amounts of false positives generated by the state of the art methods. This paper paves the way for future researches aimed at countering the critical menace of illegitimate pivoting activities.
This paper is a digest of the thesis on predicting cyber attacks in a collaborative environment. While previous works mostly focused on predicting attacks as seen from a single observation point, we proposed taking advantage of collaboration and exchange of intrusion detection alerts among organizations and networks. Thus, we can observe the cyber attack on a large scale and predict the next action of an adversary and its target. The thesis follows the three levels of cyber situational awareness: perception, comprehension, and projection. In the perception phase, we discuss the improvements of intrusion detection systems that allow for sharing intrusion detection alerts and their correlation. In the comprehension phase, we employed data mining to discover frequent attack patterns. In the projection phase, we present the analytical framework for the predictive analysis of the alerts backed by data mining and contemporary data processing approaches. The results are shown from experimental evaluation in the security alert sharing platform SABU, where real-world alerts from Czech academic and commercial networks are shared. The thesis is accompanied by the implementation of the analytical framework and a dataset that provides a baseline for future work.
In this paper, we present AIDA, an analytical framework for processing intrusion detection alerts with a focus on alert correlation and predictive analytics. The framework contains components that filter, aggregate, and correlate the alerts, and predict future security events using the predictive rules distilled from historical records. The components are based on stream processing and use selected features of data mining (namely sequential rule mining) and complex event processing. The framework was deployed as an analytical component of an alert sharing platform, where alerts from intrusion detection systems, honeypots, and other data sources are exchanged among the community of peers. The deployment is briefly described and evaluated to illustrate the capabilities of the framework in practice. Further, the framework may be deployed locally for experimentations over datasets.
"Blacklists ( blocklists, denylists) of network entities (e.g., IP addresses, domain names) are popular approaches to preventing cyber attacks. However, the limited capacity of active network defense devices may not hold all the entries on a blacklist. In this paper, we evaluated two strategies to limit the size of a blacklist and their impact on the blacklist's accuracy. The first strategy is setting the maximal size of a blacklist; the second is setting an expiration time to blacklist items. Short-term attack predictions are typically more precise, and, thus, the recent blacklist entries should be more valuable than older ones. Our experiment shows that the blacklists reduced to half of the size via either strategy achieve only a 25 % drop in accuracy."
In the context of the global information age, cases concerning the provision of technical assistance to commit cybercrimes are growing in leaps and bounds and a brand-new crime-as-a-service industry is beginning to take shape. German criminal law addresses this issue in the context of joint commission theory and individual incrimination as complementary, whereas the Chinese model, by contrast, has made marked progress in the fight against cyber aiding by introducing new criminal provisions. The change of cyber-aiding indeed represents a significant challenge to current criminal legislation and consideration of its criminal countermeasures is indispensably significant. (C) 2016 Ting Zhang.
"Security is a significant issue for everyone due to new and creative ways to commit cybercrime. The Closed-Circuit Television (CCTV) systems are being installed in offices, houses, shopping malls, and on streets to protect lives. Operators monitor CCTV; however, it is difficult for a single person to monitor the actions of multiple people at one time. Consequently, there is a dire need for an automated monitoring system that detects a person with ammunition or any other harmful material Based on our research and findings of this study, we have designed a new Intelligent Ammunition Detection and Classification (IADC) system using Convolutional Neural Network (CNN). The proposed system is designed to identify persons carrying weapons and ammunition using CCTV cameras. When weapons are identified, the cameras sound an alarm. In the proposed IADC system, CNN was used to detect firearms and ammunition. The CNN model which is a Deep Learning technique consists of neural networks, most commonly applied to analyzing visual imagery has gained popularity for unstructured (images, videos) data classification. Additionally, this system generates an early warning through detection of ammunition before conditions become critical. Hence the faster and earlier the prediction, the lower the response time, loses and potential victims. The proposed IADC system provides better results than earlier published models like VGGNet, OverFeat-1, OverFeat-2, and OverFeat-3."
Malware is 'malicious software' programs that carry out many of the cyberattacks on the Internet, including cybercrime, fraud, scams and nation-state cyberwar. These malicious software programs come in a wide range of different classifications such as viruses, Trojans, worms, spyware, botnet malware, ransomware, Rootkit, etc. Ransomware is class of malware that holds the victim's data hostage by encrypting the data on a user's computer to make it unavailable to the user and only decrypt it after the user pays a ransom in the form of a sum of money. To avoid detection, different variants of ransomware utilise one or more techniques in their attack flow including Machine Learning (ML) algorithms. There is, therefore, a need to understand the techniques used ransomware development and their deployment strategy in order to understand their attack flow better to develop appropriate countermeasures. In this paper, we propose DNAact-Ran, A Digital DNA Sequencing Engine for Ransomware Detection Using Machine Learning. DNAact-Ran utilises Digital DNA sequencing design constraints and k-mer frequency vector. To measure the efficacy of the proposed approach, we evaluated DNAact-Run on 582 ransomware and 942 goodware instances to measure the performance of precision, recall, f-measure and accuracy. Compared to other methods, the evaluation results show that DNAact-Run can predict and detect ransomware accurately and effectively.
There is an alarming increase in the number of cybercrime incidents through anonymous e-mails. The problem of e-mail authorship attribution is to identify the most plausible author of an anonymous e-mail from a group of potential suspects. Most previous contributions employed a traditional classification approach, such as decision tree and Support Vector Machine (SVM), to identify the author and studied the effects of different writing style features on the classification accuracy. However, little attention has been given on ensuring the quality of the evidence. In this paper, we introduce an innovative data mining method to capture the write-print of every suspect and model it as combinations of features that occurred frequently in the suspect's e-mails. This notion is called frequent pattern, which has proven to be effective in many data mining applications, but it is the first time to be applied to the problem of authorship attribution. Unlike the traditional approach, the extracted write-print by our method is unique among the suspects and, therefore, provides convincing and credible evidence for presenting it in a court of law. Experiments on real-life e-mails suggest that the proposed method can effectively identify the author and the results are supported by a strong evidence. (c) 2008 Digital Forensic Research Workshop. Published by Elsevier Ltd. All rights reserved.
This Full Paper in the Innovative Practice category presents and evaluates a technical innovation for hands-on classes. When learning cybersecurity, operating systems, or networking, students perform practical tasks using a broad range of command-line tools. Collecting and analyzing data about the command usage can reveal valuable insights into how students progress and where they make mistakes. However, few learning environments support recording and inspecting command-line inputs, and setting up an efficient infrastructure for this purpose is challenging. To aid engineering and computing educators, we share the design and implementation of an open-source toolset for logging commands that students execute on Linux machines. Compared to basic solutions, such as shell history files, the toolset's novelty and added value are threefold. First, its configuration is automated so that it can be easily used in classes on different topics. Second, it collects metadata about the command execution, such as a timestamp, hostname, and IP address. Third, all data are instantly forwarded to central storage in a unified, semi-structured format. This enables automated processing of the data, both in real-time and post hoc, to enhance the instructors' understanding of student actions. The toolset works independently of the teaching content, the training network's topology, or the number of students working in parallel. We demonstrated the toolset's value in two learning environments at four training sessions. Over two semesters, 50 students played educational cybersecurity games using a Linux command-line interface. Each training session lasted approximately two hours, during which we recorded 4439 shell commands. The semiautomated data analysis revealed different solution patterns, used tools, and misconceptions of students. Our insights from creating the toolset and applying it in teaching practice are relevant for instructors, researchers, and developers of learning environments. We provide the software and data resulting from this work so that others can use them in their hands-on classes.
Cyber situation awareness has been recognized as a vital requirement for effective cyber defense. Cyber situation awareness allows cybersecurity operators to identify, understand, and anticipate incoming threats. Achieving and maintaining the cyber situation awareness is a challenging task given the continuous evolution of the computer networks, increasing volume and speeds of the data in a network, and rising number of threats to network security. Our work contributes to the continuous evolution of cyber situation awareness by the research of novel approaches to the perception and comprehension of a computer network. We concentrate our research efforts on the domain of IP flow network monitoring. We propose improvements to the IP flow monitoring techniques that enable the enhanced perception of a computer network. Further, we conduct detailed analyses of network traffic, which allows for an in-depth understanding of host behavior in a computer network. Last but not least, we propose a novel approach to IP flow network monitoring that enables real-time cyber situation awareness.
The resolution of lexical ambiguity in machine translation systems often involves the automated, on-line selection of the correct sense of polysemous target words in the context of a clause, phrase or sentence. However, the performance of machine translation systems in emulating this aspect of human language processing has not been entirely successful, to the extent that resolution of entities and terms in natural language could be automated for open source intelligence analysis. Whilst some of these systems confine themselves to processing domain-specific knowledge (e.g., medical terminology), with some success, the popular general-purpose direct translation systems now freely available on the World Wide Web (WWW) are investigated for characteristic semantic processing errors in this study. A ubiquitous sentence (The quick brown fox jumps over the lazy dog), an equative metaphor, and a simile are translated into four romance and one Germanic language, with the translation then inverted back to English using the same translation system. It is found that in addition to expected differences in correctly mapping shades of meaning (e.g., quick is mapped to fast), some spatial meanings are incorrectly transformed, especially for verbs (e.g., jumps over becomes branches over or jumps on). The most serious error is the addition of extra semantic features to individual words, particularly features associated with nouns (e.g., the gender-neutral fox becomes the female vixen). The implications of these types of errors for the automatic translation of human language - with respect to semantic representation in open source intelligence - are discussed.
A substantial amount of pornographic material is unregulated and readily accessible to all, which triggers cybercrime. Hence, the automatic detection and filtering of obscene content have become vital. Due to the sensitive nature of the data, a false-positive misclassification can affect the dignity of a person. So, we choose a detection algorithm that trains the model with annotated obscene images to specifically extract obscene features. We introduced a YOLOv3 ensemble learning method that integrates the sandglass block and convolutional block attention module (CBAM) into feature extraction. A diversified dataset is generated by using Pix-2-Pix GAN-based augmentation followed by extensive manual annotations to train this algorithm. The augmentation technique using illumination shifts and background color variations enables our model to reduce the misclassifications. The proposed design is optimized by using CFC3 loss, a combination of contrastive feature loss, and YOLOv3 default loss. We abbreviated this model as Ensemble learning with Attention-based YOLOv3 combined with CFC3 loss (EAYv3-CF}}C-3). EAYv3-CFC3 achieved an accuracy of 98.85 +/- 1.00%, a precision of 98.70 +/- 1.00%, a JI score of 97.73 +/- 1.00%, an FPR of 0.013 +/- 1.00%, an FM value of 98.85 +/- 1.00%, and MCC value of 97.70 +/- 1.30% with a better performance as compared to the state-of-the-art techniques.
Information and Communication Technologies (ICTs) have a tremendous potential to enhance individual, business, and government efficiency and improve general quality of life. However, the role of ICTs has brought with it numerous legal and regulatory challenges as well as an exacerbating criminal enterprise through cybercrime. Despite this, the evidence on the role of ICTs in improving the quality of life (QoL) is mixed and the exact nature of this relationship is not yet well understood especially at the global level. Existing information systems research has examined generally the effects of ICTs on the QoL at individual country level but there remains a gap in the understanding of the association between specific ICTs such as mobile penetration and QoL. In this study, we examine the effects of mobile penetration and QoL on one hand, and the mediating effects of ICT regulation on the other. We rely on the capability approach and archival data from 114 countries whilst employing Partial Least Squares-Structural Equation Modelling as the data analysis method. Our results show support for only the association of ICT regulation and QoL and prove that the mediating effects of ICT regulation is not significant. Our findings suggest that to enhance the QoL, practitioners and policymakers should emphasize ICT regulation to protect the privacy and security of individuals online to achieve the needed benefits. The study provides recommendations for future research.
"Most commercial Fraud Detection components of Internet banking systems use some kind of hybrid setup usually comprising a Rule-Base and an Artificial Neural Network. Such rule bases have been criticised for a lack of innovation in their approach to Knowledge Acquisition and maintenance. Furthermore, the systems are brittle; they have no way of knowing when a previously unseen set of fraud patterns is beyond their current knowledge. This limitation may have far reaching consequences in an online banking system. This paper presents a viable alternative to brittleness in Knowledge Based Systems; a potential milestone in the rapid detection of unique and novel fraud patterns in Internet banking. The experiments conducted with real online banking transaction log files suggest that Prudent based fraud detection may be a worthy alternative in online banking."
The Metaverse is a multi-user virtual world that combines physical reality with digital virtual reality. The three basic technologies for building the Metaverse are immersive technologies, artificial intelligence, and blockchain. Companies are subsequently making significant investments into creating an artificially intelligent Metaverse, with the consequence that cybersecurity has become more crucial. As cybercrime increases exponentially, it is evident that a comprehensive study of Metaverse security based on artificial intelligence is lacking. A growing number of distributed denial-of-service attacks and theft of user identification information makes it necessary to conduct comprehensive and inclusive research in this field in order to identify the Metaverse's vulnerabilities and weaknesses. This article provides a summary of existing research on AI-based Metaverse cybersecurity and discusses relevant security challenges. Based on the results, the issue of user identification plays a very important role in the presented works, for which biometric methods are the most commonly used. While the use of biometric data is considered the safest method, due to their uniqueness, they are also susceptible to misuse. A cyber-situation management system based on artificial intelligence should be able to analyze data of any volume with the help of algorithms. To prepare researchers who will pursue this topic in the future, this article provides a comprehensive summary of research on cybersecurity in the Metaverse based on artificial intelligence.
In the current age of digital world, all users of Internet/Network as well as organizations are suffering from intrusions which results into data/information are theft/loss. In the present manuscript concept of intrusion detection system (IDS) were discussed along with its types and basic approaches. It is found that signature analysis, expert system, data mining etc. still using for IDS. Survey was given related to cybercrime incidents across various industry sectors. After analyzing the attacks on networks of organizations in different industry sectors it is found that still attacks like DDoS are not preventable. Comparison of data mining algorithms used for intrusion detection was also done. Various methods to implement the algorithm along with the advantages and disadvantages were also discussed in detail. Because of the disadvantages like over fitting, slow testing speed, unstable algorithms etc., intruders in the network are still active. To avert these shortcomings there is a need to develop real-time intrusion detection and prevention system through which data/information can be protected and saved in real-time basis before a severe loss is experienced. The real-time prevention is possible only if alerts are received instantly without delays. For this purpose, process mining could be used. This technique gives instant time alerts with real time analysis so as to prevent intrusions and data loss.
This article analyses the current state and directions of development of digital insurance as one of the branches of modern economy of the Russian Federation. Attention is paid to the increasing risks of cybercrime, the assessment of the amount of damage to such crimes is given, the protection of information data is considered. The most perspective branches of development of digital insurance are marked. At the same time, a view was expressed on the future of insurance as an autonomous industry. Risks are presented, including those related to the active creation of ecosystems by banks, which allow insurance operations to be carried out by banks' clients on their own, which, on the other hand, is a serious incentive for insurance development. The advantages of digital transformation and integration of insurance companies and services into the global digital ecosystem are described. Newchallenges that insurers may face in the transition to digital format are presented. Attention is paid not only to the need to make technological changes in insurance, but also to modernize relationships with customers. The potential of the Russian insurance market has been identified and tools for its implementation have been offered. There is a suggestion to expand the range of complex services provided. The main problems on the way of digital insurance development and possible steps for their solution have been identified.
"Information Technology and the Internet are ubiquitous tools for companies. Various companies are increasingly deploying information technologies to increase their process efficiency and to minimize costs. Information technologies have redefined the ways of conducting businesses. These technological revolutions have changed the nature of doing business on a global scale; simultaneously the threats surrounding these technologies are on rise. There have been various cases found in the aspects of information misuse, security attacks etc., which can be put together as cybercrime. There are different security tools like Authentication, Access Control, Anti-Virus, Firewalls, Intrusion Detection Systems and Security Information and Event Management (SIEM) to enable organizations to control and mitigate information misuse and any threat surrounding the systems. However, they are effective in detecting outside threats, and often lack the ability to detect the insider threats (attacks undertaken by employees of the company). Insider threats have become one of the major information security challenges for the organizations. Traditional information security measures are focused on the threats from the outside environment rather than the internal environments. A wide range of research has been undertaken to investigate approaches to detect the insider threats. The study has identified packet based and flow based network analysis as the two popular approaches in detecting internal threats. This paper presents a comprehensive analysis, literature review and limitations on network traffic analysis approaches."
Live Forensics is the process of collecting forensically sound evidence from a running computer system. In cyber forensics, live forensics is very important which is to be done in order to collect volatile information. Live Forensics is done if the computer is in the running mode at the scene of crime because these information will be lost forever once the system is switched off. Also, this is the preferred option for forensically analyzing mission critical dedicated servers. During Live Forensics, it should be ensured that only relevant data is acquired from the Suspect's hard disk. This is done to minimize the tampering made in the Original Evidence. Browser files contain important information related to Suspects' Internet activities and hence its analysis is indispensable in both offline and live forensic analysis. Here, a framework which can do both the acquisition and analysis of Browser Files is discussed. The acquisition tool in the framework is capable of forensically retrieving the browser files from the Suspect's machine. The analysis tool analyzes the browser files acquired to find forensically relevant information related to Internet Activities. Browser Forensics of commonly used web browsers is described in the paper. This framework enables the investigators to find out crucial hints regarding the crime. This may help in proving whether the Internet activities related to the reported cybercrime had happened in the Suspect's machine.
"Cryptographic hashes such as MD5 and SHA-1 are used for many data mining and security applications - they are used as an identifier for files and documents. However, if a single byte of a file is changed, then cryptographic hashes result in a completely different hash value. It would be very useful to work with hashes which identify that files were similar based on their hash values. The security field has proposed similarity digests, and the data mining community has proposed locality sensitive hashes. Some proposals include the Nilsimsa hash (a locality sensitive hash), Ssdeep and Sdhash (both Ssdeep and Sdhash are similarity digests). Here, we describe a new locality sensitive hashing scheme the TLSH. We provide algorithms for evaluating and comparing hash values and provide a reference to its open source code. We do an empirical evaluation of publically available similarity digest schemes. The empirical evaluation highlights significant problems with previously proposed schemes; the TLSH scheme does not suffer from the flaws identified."
In light of the advancements in computing technology, and an increase in the use of mobile devices, various forms of services have emerged in recent times. Particularly, audiovisual (AV) content-based services that stream broadcasts through personal channels or self-produced video content shared with friends has been gaining ground. It is evolving into a new platform category to share information or help people express themselves. Meanwhile, there have been instances across the globe where these services have been used to stream or share illegal content. So far, few systematic technical studies have been conducted to investigate crimes associated with online services. In the process of viewing, sharing, and distributing illegal videos, digital artifacts may be saved on digital devices such as a PC or smartphone. This paper explores the client-side caches generated as a consequence of 'viewing' visible content through AV content-based services, including Dailymotion, Instagram, LINE, Snapchat, Telegram, TikTok, and YouTube, in various platforms including Windows, Android and iOS. Through analyzing caching mechanisms, this study categorizes and characterizes various AV caches observed during repeated experiments. This work also proposes algorithms (in combination with a developed open-source tool) for obtaining playable (visible) contents, by identifying and reassembling fragmented AV pieces.
The links between cybersecurity and civil protection in the last decades became strong due to the rapid increase in the use of Informatics Technologies (IT) worldwide and the need to increase citizens' protection from various disasters and uncertainties. The objective of the present study is to explore the role of cyber security in enhancing civil protection in Greek reality. Methodologically, the study has been based on a survey with a sample of 345 executives of IT companies in Greece by using a structured questionnaire. The basic results of the research study showed that cyber security technologies positively affect civil protection, cybercrime reduction practices have a positive influence on civil protection, and there is a significant relationship between the government's role in cyber security and civil protection. Cybersecurity also may have a significant influence on the principles of emergency operations: prevention, mitigation, preparation, response, or emergency evacuation and recovery. The main conclusion from the study is that it is very important for public and private agencies in Greece, as well as in the rest of Europe, to embrace new emerging cybersecurity technologies to help enhance cyber security and civil protection. Potential vulnerabilities in cybersecurity pose substantial risks to the effectiveness and efficiency of such critical infrastructure and directly impact the functioning of states, economies, and societies.
"The increased use of the Internet and the ease of access to online communities like social media have provided an avenue for cybercrimes. Cyberbullying, which is a kind of cybercrime, is defined as an aggressive, intentional action against a defenseless person by using the Internet, social media, or other electronic contents. Researchers have found that many of the bullying cases have tragically ended in suicides; hence automatic detection of cyberbullying has become important. The aim of this study is to detect cyberbullying on social media messages written in Turkish. To our knowledge, this is the first study which makes cyberbully detection on Turkish texts. We prepare a dataset from Instagram and Twitter messages written in Turkish and then we applied machine learning techniques that are Support Vector Machines (SVM), decision tree (C4.5), Naive Bayes Multinomial, and k Nearest Neighbors (kNN) classifiers to detect cyberbullying. We also apply information gain and chi-square feature selection methods to improve the accuracy of classifiers. We observe that when both words and emoticons in the text messages are taken into account as features, cyberbully detection improves. Among the classifiers, Naive Bayes Multinomial is the most successful one in terms both classification accuracy and running time. When feature selection is applied classification accuracy improves up to 84% for the dataset used."
With the development of network technology, the Internet in China's economic construction and people's lives have achieved a wide range of applications, which makes people's work, study and life be undergone a qualitative change, the Internet has become the essential part of people's lives. However, with the rapid development of the Internet and information technology, and the network increase of the proportion of the population, there are network problems gradually revealed cybercrime highlights as a new type of crime, especially online economic crime can be described as intensified. Among them, network infringement of intellectual property-type crime, online banking crime network involved in a congregation-based economy the crime network securities crime, online shopping, and our revenue criminal crime of network economics harm the economic interests of the country and the people, and seriously affected the country economic order. We are moving in the right direction to make the country's economic order, it is necessary to understand and combat the crime of the network economy. Therefore, it becomes increasingly necessary for its research. The author hopes that the crime of network economic research discussed in this article, it is able to provide some ideas, looking for some governance measures in order to find effective prevention and containment the network economic crimes method, our network towards a better economic life, healthier goal.
Recently, malicious software are gaining exponential growth due to the innumerable obfuscations of extended x86 IA-32 (OPcodes) that are being employed to evade from traditional detection methods. In this paper, we design a novel distinguisher to separate malware from benign that combines Multivariate Logistic Regression model using kernel HS in Penalized Splines along with OPcode frequency feature selection technique for efficiently detecting obfuscated malware. The main advantage of our penalized splines based feature selection technique is its performance capability achieved through the efficient filtering and identification of the most important OPcodes used in the obfuscation of malware. This is demonstrated through our successful implementation and experimental results of our proposed model on large malware datasets. The presented approach is effective at identifying previously examined malware and non-malware to assist in reverse engineering.
Cybercrime investigations rely heavily on digital evidence to establish links between suspects and the criminal conduct they are allegedly involved in. As a result, digital evidence must be protected since it is complex, volatile, and susceptible to alteration. In the digital evidence method, the chain of custody (CoC) is essential. As a result of the CoC, it is possible to establish that the evidence was never tampered with. Due to the inherent uncertainty of digital evidence, the trustworthiness of the CoC cannot be judged at this time. It is the duty of forensic examiners to challenge this inclination and publicly admit the inherent ambiguity in whatever evidence they use to make their decisions. This article suggests a new paradigm for maintaining the integrity of digital evidence in order to overcome these challenges. To handle the uncertainty generated by error-prone technologies while dealing with CoC documents, the new paradigm used a fuzzy hash inside the blockchain data structure. Traditional hashing methods are only able to tell whether two inputs are precisely the same or not because they are sensitive to even the smallest input changes. Using fuzzy hash functions, we can figure out how dissimilar two images are by comparing their similarities. As an example of how this paradigm may be applied to computer systems and make digital investigations more successful, we utilize image forensics as the focus of an in-depth look at how it works.
Cybercriminals use different types of geographically distributed servers to run their operations such as C&C servers for managing their malware, exploit servers to distribute the malware, payment servers for monetization, and redirectors for anonymity. Identifying the server infrastructure used by a cybercrime operation is fundamental for defenders, as it enables take-downs that can disrupt the operation and is a critical step towards identifying the criminals behind it. In this paper, we propose a novel active probing approach for detecting malicious servers and compromised hosts that listen for (and react to) incoming network requests. Our approach sends probes to remote hosts and examines their responses, determining whether the remote hosts are malicious or not. It identifies different malicious server types as well as malware that listens for incoming traffic such as P2P bots. Compared with existing defenses, our active probing approach is fast, cheap, easy to deploy, and achieves Internet scale. We have implemented our active probing approach in a tool called CyberProbe. We have used CyberProbe to identify 151 malicious servers and 7,881 P2P bots through 24 localized and Internet-wide scans. Of those servers 75% are unknown to publicly available databases of malicious servers, indicating that CyberProbe can achieve up to 4 times better coverage than existing techniques. Our results reveal an important provider locality property: operations hosts an average of 3.2 servers on the same hosting provider to amortize the cost of setting up a relationship with the provider.
The rapid pace of technological developments in the area of information and communications technologies caused nations and peoples to be more reliant on cyber infrastructure to survive. Besides opportunities, the widespread use of information technology introduces new threats as well. Risks related to cyber security have started to threaten critical infrastructures, which are defined as assets that are essential for the functioning of a society and its economy. Cyber security has become one of the most serious national security concerns. In 2003 the United States was the first nation to prepare and publish a national cyber security strategy In the last ten years, 35 other nations have subsequently published their national cyber security strategy document. There are several aspects for national cyber security strategies. According to Luiijif and Healey (2012), there are five mandates of national cyber security: 1) Military cyber operations, 2) Counter cybercrime, 3) Intelligence/Counter intelligence, 4) Cyber security crisis management and critical infrastructure protection and 5) Internet governance and cyber diplomacy. In this study, the national cyber security strategies of France, Germany, The Netherlands, United Kingdom, United States and Turkey are examined and compared. Correlations between specific properties of the nation (economic power and political situation etc.) and focus and content of its cyber strategy were examined. The results of the study will provide guidance for nations that plan to prepare or update a national cyber security strategy.
The amount of data generated by the current interconnected world is immeasurable, and a large part of such data is publicly available, which means that it is accessible by any user, at any time, from anywhere in the Internet. In this respect, Open Source Intelligence (OSINT) is a type of intelligence that actually benefits from that open nature by collecting, processing and correlating points of the whole cyberspace to generate knowledge. In fact, recent advances in technology are causing OSINT to currently evolve at a dizzying rate, providing innovative data-driven and AI-powered applications for politics, economy or society, but also offering new lines of action against cyberthreats and cybercrime. The paper at hand describes the current state of OSINT and makes a comprehensive review of the paradigm, focusing on the services and techniques enhancing the cybersecurity field. On the one hand, we analyze the strong points of this methodology and propose numerous ways to apply it to cybersecurity. On the other hand, we cover the limitations when adopting it. Considering there is a lot left to explore in this ample field, we also enumerate some open challenges to be addressed in the future. Additionally, we study the role of OSINT in the public sphere of governments, which constitute an ideal landscape to exploit open data.
"The complexity and diversity of banking risks has been on the rise in recent years due to increased competition between banks and the increasing number of financial products and services. Innovative banking financial services aimed at serving the population of the digitalized environment have caused major changes in online banking risks. The complex changes and regulations, as well as the increase in the volume of digital data, represent only a few of the factors that determine the change. As a result the associated risks are numerous. Bank risks in the digital environment are generated by fraudulent actions made on data, failures of the online banking system, cybercrime, cyber-terrorism, cyber-activism etc. These cyber threats lead to a variety of risks affecting the online banking system, such as: financial risks (R11 - R13); operational risks (R14); security risks (R15); fraud risks (R16); money-laundering risks (R17); reputational risks (R18); technological risks (R19). All these risk categories will be the subject of this paper. The code (R11-R21) will be used for easier identification during the paper. In this context, the purpose of this paper is to identify bank risks from another perspective for establishing a framework of risk variables for the machine-learning model that will be developed in future research. In this paper, the literature review was the central source to confirm our results."
Background: Youth mental health interventions aimed at reducing substance use and delinquency in adolescents compete with other types of interventions for reimbursement from public funding. Within the youth mental health domain, delinquent acts impose high costs on society. These costs should be included in economic evaluations conducted from a societal perspective. Although the relevance of these costs is recognized, they are often left out because the unit costs of delinquent acts are unknown. Aims of the Study: This study aims to provide a method for estimating the unit costs per perpetrator of 14 delinquent acts common in the Netherlands and included in self reported delinquency questionnaires: robbery/theft with violence, simple theft/pickpocketing, receiving stolen goods, destruction/vandalism of private or public property, disorderly conduct/discrimination, arson, cybercrime, simple and aggravated assault. threat. forced sexual contact, unauthorised driving, driving under the influence. dealing in soft drugs, and dealing in hard drugs. Methods: Information on government expenditures and the incidence of crimes, number of perpetrators, and the percentage of solved and reported crimes was obtained from the national database on crime and justice of the Research and Documentation Centre of the Ministry of Justice and Security, Statistics Netherlands, and the Council for the Judiciary in the Netherlands. We applied a top-down micro costing approach to calculate the point estimate of the unit costs for each of the delinquent acts and, subsequently. estimated the mean (SD) unit costs for each of the delinquent acts by taking random draws from a triangular distribution while taking into account a 10% uncertainty associated with the associated point estimate. Results: The mean (SD) unit costs per delinquent act per perpetrator ranged between (sic)495 ((sic)1.30) for Driving under the influence and (sic)33,813 ((sic)78.30) for a Cybercrime. These unit costs may be considered as outliers as most unit costs ranged between (sic)2,600 and (sic)13,500 per delinquent act per perpetrator. Discussion: This study is the first to estimate the unit costs per delinquent act per perpetrator in the Netherlands. The results of this study enable the inclusion of government expenditures associated with crime and justice in economic evaluations conducted from a societal perspective. Implications for Health Care Provision and Use: Youth mental health interventions aimed at reducing substance use and delinquency in adolescents are increasingly subjected to economic evaluations. These evaluations are used to inform decisions concerning the allocation of scarce healthcare resources and should cover all the costs and benefits for society, including those associated with delinquent acts. Implications for Health Policies: The results of this study facilitate economic evaluations of youth mental health interventions aimed at reducing substance use and delinquency in adolescents, conducted from a societal perspective. Implications for Further Research: Based on health-economic evaluations conducted in the field of youth mental health and the results of the current study, we recommend including the estimated unit costs in guidelines for health-economic evaluations conducted from a societal perspective. Future research could aim at examining whether these unit costs require regular updating. The methodology applied in this study allows for this.
The article deals with the problem of cybercrimes committed by young people and minors. The authors analyse the social portrait of juveniles committing cybercrimes, especially their personality and social status, with attention to details facilitating re-socialization. We consider the socio-demographic characteristics of minors who have committed crimes and were convicted with application of various penalties. It is concluded that there is no urgent need to reduce the age of criminal responsibility of minors. It is necessary to take into account, first of all, the psychological age, which reflects the level of development of the intellect and will, as well as the sociological age, which is responsible for the degree of socialization of the individual, and not the calendar age. Based on the analysis of court statistics and the results of the first nationwide study of the digital competence of adolescents and parents in the article it is shown that such components of digital competence as knowledge, skills, motivation and responsibility in different spheres of online activity (content, communication, technosphere, consumption) can be considered as factors that influence deviant behaviour of minors.
Over the last decades cybersecurity has become a cornerstone of European digital development. Alongside with the diffusion of information and communication technologies and the deepening (as well as widening) of the European Union, the initial narrow and sectoral data security policies have expanded into a comprehensive cybersecurity framework addressing issues from resilient infrastructure and technological sovereignty, through tackling cybercrime, to cyber defence capabilities and responsible state behaviour in cyberspace. In this complex web of interrelated policies a relative newcomer at the European Union (EU) level is cyber diplomacy. Sometimes also called public diplomacy 2.0, it factors into the cross-border connectivity of cyberspace and reflects a shift in international relations where the lines between external and internal policies, military and civilian domains are blurred. However, the term cyber diplomacy is fluid and it is not well understood which topics should be under its umbrella, in particular in relation to cybersecurity, where it seems to be linked the most. This article aims to map existing and proposed instruments that make up the EU's arsenal in this broad context to answer the following questions: what is cyber diplomacy and how is it related to the EU cybersecurity? Is cyber diplomacy in the EU becoming something in its own right as a distinct set of tools to secure the EU policy objectives?
An enormous growing of Information Technology and its applications in diverse fields enables us to think for digital world wherein cyberspace is being emerged significantly. After land, sea, air and space, cyberspace is the fifth domain where cybercrime, espionage and cyber weapons are on rising day by day. As a result, cyber security is a multi-dimensional concept and a complex issue straddling many disciplines and fields. Development of Information Technology makes possible to automate the industrial control system (ICS) for its efficient and remote operations. The automation in turn makes the industrial control system vulnerable to cyber security threats. Keeping in mind cyber security as a multi-dimensional complex issue, in this paper we have proposed a password policy for industrial control networks (ICNs) to have highest level of security. Our proposed password policy provides a set of guidelines for all involved individuals and associated systems in ICN and ICS ((IC)(NS)-N-2) with aim ensure security aspects such as authentication, authorization, confidentiality, integrity, availability, non-repudiation and cyber security best practices in (IC)(NS)-N-2. The password policy covers password creation criteria, minimum required length of password and character sets for password composition. We have also discussed about the purpose, scope, policy exception, review and enforcement of the password policy in (IC)(NS)-N-2.
This study examined the Marlian's influence on social behavior among youths in Ado-Ekiti, Nigeria. The study adopted the descriptive non-experimental survey design. The target population includes youth is some selected areas in Ado Ekiti consisting Iyin Road, Ejigbo, Oke-Iyinmi, Adebayo way and Post-office area. The choice of these areas is due to their population density of the youths. Thus, the total population consisted a total of 1,605 respondents among which a sample of 300 was randomly selected. Data collection involves the use of semi-structured questionnaire which was validated for inter-item consistencies using the Cronbach's alpha coefficient of 0.7% and above. Data analysis involved the use of the descriptive statistics such as the simple frequency counts and percentage distribution. Findings from the study shows that (51%) of the respondent strongly agreed that character and attitude of youths are usually influenced by the media and celebrity culture. The study further indicated that (51%) of the respondents strongly agree that Naira Marley lifestyle influence the usage of drugs among youths. The study concluded that youth now see cybercrime as a career choice and criminality among youth has been encouraged through Marlians behaviours. This study recommended that the Nigerian artists and celebrities should see themselves as role models to the younger generation and try their best to imbibe good moral values that young people can emulate.
Filtering is a fundamental strategy of metric similarity indexes to minimise the number of computed distances. Given a triplet of objects for which distances of two pairs are known, the lower and upper bounds on the third distance can be determined using the triangle inequality property. Obviously, tightness of the bounds is crucial for efficiency reasons - the more precise the estimation, the more distance computations can be avoided, and the more efficient the search is. We show that it is not necessary to consider arbitrary angles in triangles formed by pairwise distances of three objects, as specific range of possible angles is data dependent. When considering realistic ranges of angles, the bounds on distances can be much more tight and filtering much more effective. We formalise the problem of the data dependent estimation of bounds on distances and deeply analyse limited angles in triangles of distances. We justify the potential of the data dependent metric filtering both, analytically and experimentally, executing many distance estimations on several real-life datasets. (c) 2021 Elsevier Ltd. All rights reserved.
We focus on the efficient search for the most similar bit strings to a given query in the Hamming space. The Hamming distance can be lower-bounded by the difference of the number of ones in the compared strings, i.e. of their weights. Recently, such property has been successfully used by the Hamming Weight Tree (HWT) indexing structure. We propose modifications of the bit strings that preserve pairwise Hamming distances but improve the tightness of these lower bounds, so the query evaluation with the HWT is several times faster. We also show that the unbalanced bit strings, recently reported to provide similar quality of search as the traditionally used balanced bit strings, can be more efficiently indexed with the HWT. Combined with the distance preserving modifications, the HWT query evaluation can be more than one order of magnitude faster than the HWT baseline.
Understanding online risk for adults with intellectual disabilities is important to improve digital inclusion in society. Perceptions of online risk can determine behaviours that obstruct or facilitate Internet access and use. This current study aimed to qualitatively investigate the psychological implications of online victimisation risks, including online negative comments and/or messages for adults with intellectual disabilities, as a novel area yet explored in-depth. Semi-structured interview data was collected remotely. Template analysis found there to be both negative and positive psychological implications experienced in response to online risks. Specifically, participants reported a wide range of negative emotions but also positive growth in the form of learning from the experience and increased confidence. The attribution of blame process in cybervictimisation can involve both blaming the perpetrator but also internalised victim-blaming which may be a consequence of the type of online risk (i.e. sexual risks). Implications for both practice and research are suggested.
States are not being held accountable for the vast majority of their harmful cyberoperations, largely because classifications created in physical space do not map well onto the cyber domain. Most injurious and invasive cyberoperations are not cybercrimes and do not constitute cyberwarfare, nor are states extending existing definitions of wrongful acts permitting countermeasures to cyberoperations (possibly to avoid creating precedent restricting their own activities). Absent an appropriate label, victim states have few effective and non-escalatory responsive options, and the harms associated with these incidents lie where they fall. This Article draws on tort law and international law principles to construct a comprehensive system of state accountability in cyberspace, where states are liable for their harmful acts and responsible for their wrongful ones. It identifies international cybertorts-acts that employ, infect, or undermine the internet, a computer system, or a network and thereby cause significant transboundary harm-as distinct from cybercrime and cyberwarfare. Not only does this term distinguish a specific kind of harmful act, it highlights how the principle of state liability for transboundary harms (which holds states accountable for the harmful consequences of both their lawful and unlawful activities) could usefully complement the existing law of state responsibility (which applies only to unlawful state acts). Imposing state liability for international cybertorts minimizes the likelihood that victim states will resort to escalatory responses, increases the chance that those harmed will be compensated, and preserves a bounded grey zone for state experimentation in cyberspace.
"Images are the things those are being used in day to day life so much that it has become an important aspect of everyone's life. Images have always been a popular thing to be instantly shared with each other around the globe. Today's world is a place where one can get easily influenced and images being perfect to influence as it easily conveys a lot using visual representation also it tends to be remembered more. Images can have positive influence via means of motivational quotes images also it can have negative influence via means of sharing nude images, images of killing etc. Retrieval of images with object-of-interest from a vast pool of social media images has been a research interest in the understanding of human emotions, situations leading to conflicts, in the field of cybercrime etc. The proposed system is a model for suspicious detection of an image; it is a system which will decide the image is a good or bad influence based on the past and present experiences. The proposed system makes use of more number of attributes than the existing system which makes prediction more accurate. It is a machine learning application where a large amount of data is analyzed and some meaningful information is extracted. The system makes use of the huge dataset of data given to it and that data being trained."
Cloud computing is one of the fastest growing technologies in the history of computing. It also changes the way of information technology to manage, access, provide and create services. Cloud computing also brings many benefits to end users and organizations. However, the rapid growth of cloud computing has made a new arena of crime called cybercrime. This poses new challenges in technical, legal and governance matters such as how to conduct appropriate digital investigations in the cloud environment and how to proactively collect data before incidents occur to save time, money and effort. This study aims to get the most ideal model/framework for forensics readiness on the cloud from previously published research. This paper discusses the literature study using a meta-analysis approach that applies social network analysis from 38 papers that discuss forensic readiness with its supporting factors. From the results of the study it is known that the ideal framework was proposed by Elyas 2014 with thirteen factors, which can be seen from the results of the calculation of social network analysis with the value betweenness centrality obtained value of 0.037 which shows the highest value of all the models previously studied. This value comes from a number of factors (after normalization) used by the model proposed by Elyas 2014 compared to other researchers.
While scales for the 'strength' of pornographic images have been developed and used in research and for judicial purposes, the potential emotive impact of text-based pornography has received little attention. In this paper we describe our approach to characterising dimensions of sexually explicit language and outline their use in strategies for neutralising such language. Our intended application of this work is contexts where we may wish to affect the content to which a student, trainee or other professional may be exposed. By controlling the quantity and degree of such content, we aim to minimise any detrimental effects (observer impact) that such content may have on ill-prepared individuals.
It is very important and difficult for remote monitoring and alarming in places such as Laboratory, Office, Residential Area and Hotels etc. This paper implemented a Remote Monitoring Alarming and Management System Based on Mobile Intelligent Terminal (RMAS-MIT), the remote monitoring and alarming system (RMAS) is composed of the front video collecting system, pre-processing system, data transmission system, data processing center. The data processing center will determine if there exists an abnormal event according to both the processed result from the collected video and the abnormal conditions set by the administrator. If an abnormal event appears, it will start the alarming work, for example, play alarm sound, send short message to the administrator's mobile, send email to the administrator etc. Aiming at the shortage of IPv4 address, this paper utilized both Port Map and Dynamic DNS technologies to implement the Dynamic Reverse Connection Trace (DRCT) to solve the remote management of RSMS-MIT located behind private networks. To implement the anywhere and anytime management of RSMS-MIT, this paper development the Mobile Intelligent Terminal based remote monitoring management system (RMAS-MIT).
Antifragility, which is an evolutionary understanding of resilience, has become a predominant concept in academic and industrial fields as the criticality of vital infrastructures (like healthcare and transportation) has become more flexible and varying due the impact of digitization and adverse circumstances, such as changing the prioritization of industrial services while accelerating IoT (Internet of Things) deployment during the COVID-19 pandemic. The crucial role of antifragility is to enable critical infrastructures to gain from disorder to foster their adaptability to real unexpected environmental changes. Thus, this paper aims to provide a comprehensive survey on the antifragility concept while clarifying the difference with the resilience concept. Moreover, it highlights how the COVID-19 crisis has revealed the fragility of critical infrastructures and unintentionally promoted the antifragility concept. To showcase the main concepts, we adopt the blockchain as an example of an antifragile system.
In the modern world, the use of ICT communication technologies has become an integral part of life. ICT infrastructure is the bearer of digital traces of both legal and illegal activities performed through it. However, for something to become digital evidence, it must be obtained by law and by a person authorised by law. Namely, the virtual infrastructure, especially the Internet and the new challenges brought to us by cloud architectue due to its physical positioning outside national borders, calls into question the legality of searching and collecting digital evidence outside national borders. This paper analyses the legal basis for collecting digital evidence in cyberspace internationally, such as the Council of Europe Convention on Cybercrime, the US Cloud Act, the Australian Decryption Act and the European GDPR. Although the Court of Justice of the European Union declared invalid the decision of the European Commission (EU) 2016/1250 on the adequacy of data protection provided through the EU-US Privacy Shield, experts must not stop looking for a solution to the apparent problem. The paper intends to support decision-makers in taking clear national positions regarding the above controversial legal norms and their mutual conflict. The paper compares the legal consequences of such collection, and the acceptability of such digital evidence, and such collection may also be associated with a breach of the privacy of a legal and private entity.
In recent years, security concerns, including document fraud and identity theft, terrorism, and cybercrime have led to the growth and development of biometric technologies, and in general, all activities related to biometric research have gained momentum. This research presents a more intelligent methodology for authenticating and verifying one's identity while reading and typing a text. Threats to e-commerce security are wreaking havoc on online trade via a variety of nefarious methods. Thus, this method can be implemented to furtively identify these attacks and impede them, using a variety of texts to study. We refer to the authentication as a behavioral biometric technology based on how people look when typing in which the visual information was implemented to verify the identity of 35 users while typing via using an eye tracker device. Hence, we have employed the two-layer perceptron neural network to model the learning phase in the object recognition process. The fusion of eye tracker device information and two-layer perceptron neural network coefficients was done at the decision level, and we have achieved an average of 98.97%, 98.76% and 98.86% for sensitivity, specificity and accuracy, respectively. By the combination of biometric technology products as well as modern computer technology, it is easy to perform monitoring, management, systems integration, automated management, and security applications.
"In 2016, law enforcement dismantled the infrastructure of the Avalanche bulletproof hosting service, the largest takedown of a cybercrime operation so far. The malware families supported by Avalanche use Domain Generation Algorithms (DGAs) to generate random domain names for controlling their botnets. The takedown proactively targets these presumably malicious domains; however, as coincidental collisions with legitimate domains are possible, investigators must first classify domains to prevent undesirable harm to website owners and botnet victims. The constraints of this real-world takedown (proactive decisions without access to malware activity, no bulk patterns and no active connections) mean that approaches from the state of the art cannot be applied. The problem of classifying thousands of registered DGA domain names therefore required an extensive, painstaking manual effort by law enforcement investigators. To significantly reduce this effort without compromising correctness, we develop a model that automates the classification. Through a synergetic approach, we achieve an accuracy of 97.6% with ground truth from the 2017 and 2018 Avalanche takedowns; for the 2019 takedown, this translates into a reduction of 76.9% in manual investigation effort. Furthermore, we interpret the model to provide investigators with insights into how benign and malicious domains differ in behavior, which features and data sources are most important, and how the model can be applied according to the practical requirements of a real-world takedown."
This paper presents a study on the application of supervised machine learning algorithms for the purpose of distinguishing and categorizing Virtual Private Network (VPN) and The Onion Router (TOR) traffic on the dark web. The dark web, characterized by its anonymity and inaccessibility, has become a popular platform for illicit activities such as drug trafficking, money laundering, and cybercrime. While VPNs and TOR can be used for legitimate purposes such as privacy protection and bypassing internet censorship, they can also be exploited by cybercriminals. The CIC-Darknet2020 dataset, which includes a comprehensive collection of network traffic captures from the dark web incorporating traffic features from both VPN and TOR technologies, is used for this study. We employ classification algorithms such as Random Forest, Support Vector Machine, Naive Bayes, and Decision Tree classifiers to construct our model. The performance of the model is evaluated using parameters such as execution time, accuracy, precision, F-measure, and recall, utilizing five-fold and ten-fold cross-validation and 66/34 and 80/20 percentage splits. Our results show that the Decision Tree (J48) classifier outperforms other classifiers, achieving 99.6% accuracy with an execution time of 15 seconds for ten-fold cross-validation. The findings of this study have implications for enhancing cybersecurity measures in identifying and mitigating threats associated with VPN and TOR traffic on the dark web.
The COVID-19 pandemic introduced the new norm that changed the way we work and live. During these unprecedented times, most of the organizations expected their employees to work from home. Remote working created new opportunities for hackers since more users were making use of digital platforms for online shopping, accessing Virtual Private Network (VPN), videoconferencing platforms, and software alike. Consequently, cybercrime increased due to the increase in the attack surface, and software vulnerabilities were exploited for launching cyberattacks. There is existing research that explores vulnerability disclosure on Twitter. However, there is a lack of study on opportunistic targeted attacks where specific vulnerabilities are exploited in a way that benefit adversaries the most in times such as COVID-19. The primary aim of this work is to study the effectiveness of vulnerability disclosure pattern on Twitter in COVID-19, and discuss how Twitter can be leveraged as Open-Source Intelligence (OSINT) during a pandemic where the global users can follow a coordinated approach to share security-related information and conduct awareness campaigns. The study identifies Twitter as an apt source for conducting cybersecurity awareness campaigns as 99.83% of the security vulnerabilities are found to be accurate. The information can help global cybersecurity agencies to proactively identify vulnerabilities, coordinate activities, and plan for mitigation strategies since releasing patches from the vendor might take time.
This paper seeks to examine the notion of piracy cultures and their effects upon the field of education and in particular education at a university level. Whilst there is no doubt that the advances of technological developments have brought about many advancements in society, none more so than in the realms of information and communication technologies (ICTs) it is also true that their capabilities can be put to, perhaps, more nefarious uses. The idea of cybercrime is well documented (e. g. Wall 2007, Brenner 2010) however there is increasing concern over the ways in which ICTs are facilitating a shift in perception towards a tacit acceptance or even in some cases a proud promulgation of lesser misdemeanors. One of the less talked about spin offs of piracy culture in its broadest sense is the loss of integrity of information which has led to an increase in appropriation of content and more specifically to plagiarism in the educational world. Piracy cultures are blamed for a surge in plagiarism and other 'offences'. Is this perception true? The authors argue that there is growing evidence that points to a need for a re-conceptualization of what is and what is not allowed in terms of data sharing. Our concept of education is augmented by the potentialities that information and communication technologies (ICTs) enable.
Machine learning techniques are being widely used to develop an intrusion detection system (IDS) for detecting and classifying cyberattacks at the network-level and the host-level in a timely and automatic manner. However, many challenges arise since malicious attacks are continually changing and are occurring in very large volumes requiring a scalable solution. There are different malware datasets available publicly for further research by cyber security community. However, no existing study has shown the detailed analysis of the performance of various machine learning algorithms on various publicly available datasets. Due to the dynamic nature of malware with continuously changing attacking methods, the malware datasets available publicly are to be updated systematically and benchmarked. In this paper, a deep neural network (DNN), a type of deep learning model, is explored to develop a flexible and effective IDS to detect and classify unforeseen and unpredictable cyberattacks. The continuous change in network behavior and rapid evolution of attacks makes it necessary to evaluate various datasets which are generated over the years through static and dynamic approaches. This type of study facilitates to identify the best algorithm which can effectively work in detecting future cyberattacks. A comprehensive evaluation of experiments of DNNs and other classical machine learning classifiers are shown on various publicly available benchmark malware datasets. The optimal network parameters and network topologies for DNNs are chosen through the following hyperparameter selection methods with KDDCup 99 dataset. All the experiments of DNNs are run till 1,000 epochs with the learning rate varying in the range [0.01-0.5]. The DNN model which performed well on KDDCup 99 is applied on other datasets, such as NSL-KDD, UNSW-NB15, Kyoto, WSN-DS, and CICIDS 2017, to conduct the benchmark. Our DNN model learns the abstract and high-dimensional feature representation of the IDS data by passing them into many hidden layers. Through a rigorous experimental testing, it is confirmed that DNNs perform well in comparison with the classical machine learning classifiers. Finally, we propose a highly scalable and hybrid DNNs framework called scale-hybrid-IDS-AlertNet which can be used in real-time to effectively monitor the network traffic and host-level events to proactively alert possible cyberattacks.
Many techniques have emerged to evaluate software Technical Debt (TD). However, differences in reporting TD are not yet studied widely, as they can give different perceptions about the evolution of TD in projects. The goal of this paper is to compare three TD identification techniques: i. Maintainability Index (MI), ii. SIG TD models and iii. SQALE analysis. Considering 17 large open source Python libraries, we compare TD measurements time series in terms of trends in different sets of releases (major, minor, micro). While all methods report generally growing trends of TD over time, MI, SIG TD, and SQALE all report different patterns of TD evolution.
The previous two decades have given birth to a new popular phenomenon - online social networking. Its range, diversity and constantly growing impact on our lives provides space for the creation of new and profitable, highly professionalized industry of cybercrime. It is based on phishing, social engineering, brute force password guessing or malware collecting passwords. The aim is to penetrate another's account and monetize it by blackmailing, searching for cryptocurrency wallets, obtaining passwords, influencing public opinion in favor of an idea, or at least sending spam. Today's hackers are well organized into massive, internally specialized criminal networks recruited from the best IT specialists, and as the latest results of the British investigative initiative Bellingcat show, they are also often the cornerstone of the ongoing hybrid war. On top of this, in February 2021, the mother of all breaches appeared on the darknet - the COMB database consisting of 3.2 billion of unique pairs of emails and cleartext passwords. This is a clear and present danger for every user of any social network and their providers have to act immediately to protect accounts of their users. This paper describes how a social network operator with 250,000 daily active users deals with the problem of account compromise by deploying anomaly detection responsive to sudden changes in the behavior of a user trying to log in to the account.
Relevance. The study is relevance due to the problem of cybercrime, including corruption-related crimes, which were caused by the rapid worldwide development of computer technology, as well as the widespread use of cyberspace networks and the digitization of information-sharing processes in society. The object of the study is the public relations that arise in the process of judicial expertise in the investigation of corruption-related crimes committed in cyberspace. Several research methods have been used in the writing of this research article. The dialectical method was the first and foremost method in the study of judicial expertise. The method of analysis, the synthesis method, and comparison method were used during the research process. Research results. The authors of this scientific article came to the conclusion that the specific nature of the use of forensic investigations in the investigation of corruption offenses committed in cyberspace is one of the main forms of use of specialized knowledge in criminal proceedings and the result of which is the conclusion of expert opinion, which is the source of evidence in criminal proceedings. In addition, it was noted that all the issues that are solved by the examination of telecommunication systems (equipment) and tools have a diagnostic nature, and a list of typical issues should be fixed in the appropriate instructions.
Ransomware can prevent a user from accessing a device and its files until a ransom is paid to the attacker, most frequently in Bitcoin. With over 500 known ransomware families, it has become one of the dominant cybercrime threats for law enforcement, security professionals, and the public. However, a more comprehensive, evidence-based picture on the global direct financial impact of ransomware attacks is still missing. In this article, we present a data-driven method for identifying and gathering information on Bitcoin transactions related to illicit activity based on footprints left on the public Bitcoin blockchain. We implement this method on-top-of the GraphSense open-source platform and apply it to empirically analyze transactions related to 35 ransomware families. We estimate the lower bound direct financial impact of each ransomware family and find that, from 2013 to mid-2017, the market for ransomware payments has a minimum worth of USD 12 768 536 (22 967.54 BTC). We also find that the market is highly skewed with only a few number of players responsible for the majority of the payments. Based on these research findings, policy-makers and law enforcement agencies can use the statistics provided to understand the size of the illicit market and make informed decisions on how best to address the threat.
ACID transactions have proven to be a very useful mechanism of ensuring reliability of applications. Guaranteeing transactional properties effectively and correctly is a challenging task by itself. Furthermore, investigating transaction issues in a distributed environment is at least equally complex and requires systematic data collection and analysis. In this paper, we present mechanisms and concepts of distributed tracing with focus on the OpenTracing API and showcase our integration of tracing capabilities into the Narayana transaction manager. We show that the performance impact of tracing does not drastically decrease user application performance while providing useful information for the analysis of running transactions.
The last decade has meant a whole revolution in the context of Internet of Things (IoT), becoming one of the most important areas of research nowadays. The Semantic Web emerged as a way to add semantics to data to enable advanced reasoning. Due to this, the integration of both areas represents a beneficial convergence for more semantic-aware devices and services. In this paper we present a review to analyze the state of the art of the confluence of the Semantic Web and IoT: extracting an overview of the ad-hoc implemented tools, their categorization, and the supported sub-domains on which they exert their influence.
Smart Grids represent an important instance of cyberphysical systems for the energy sector. Due to the many layers involved and the complexities of interrelations, co-simulations have emerged as a way to integrate results from different simulators. In this paper, we propose a study of the possibilities of simulating node failure scenarios with a modification of the Mosaik co-simulation platform to allow for dynamic topologies changes. We show how co-simulations can help in determining the impact of different failure patterns using a sample scenario of households and PV units.
There is an increasing public and policy awareness that tracking cookies are being used to support behavioral advertising, but the extent to which tracking is occurring is not clear. The extent of tracking could have implications for the enforceability of legislative responses to the sharing of personal data, including the Privacy Act 1988 (Cth). In this paper, we develop a methodology for determining the prevalence of tracking cookies, and report the results for a sample of the 501 most visited sites by Australians. We find that the use of tracking cookies is endemic, but that distinct clusters of tracking can be identified across categories including search, pornography and social networking. The implications of the work in relation to privacy are discussed.
Web search engines provide easy access to a wealth of open access information but depend for their optimal use upon careful selection of search terms in order to return a high proportion of relevant information. Generally, this implies a clear association between the meaning of the chosen search expression and the information needs of the searcher. In contrast, this paper considers a scenario in which the searcher may wish to retrieve information without making their information requirements transparent. We argue that careful choice of search strategy can also facilitate retrieval of relevant information in a covert fashion that conceals the searcher's intention by ensuring return of a majority of irrelevant search hits.
Manufacturing production heavily depends on the processes that need to be followed during manufacturing. As there might be many reasons behind possible deviations from these processes, the deviations can also cover ongoing insider attacks, e.g., intended to perform sabotage or espionage on these infrastructures. Insider attacks can cause tremendous damage to a manufacturing company because an insider knows how to act inconspicuously, making insider attacks very hard to detect. In this paper, we examine the potential of process-mining methods for insider-attack detection in the context of manufacturing, which is a new and promising application context for process-aware methods. To this end, we present five manufacturing-related scenarios of insider threats identified in cooperation with a manufacturing company, where the process mining could be most helpful in the detection of their respective attack events. We describe these scenarios and demonstrate the utilization of process mining in this context, creating ground for further future research.
New devices in smart grid such as smart meters and sensors have emerged to become a massive and complex network, where a large volume of data is flowing to the smart grid systems. Those data can be real-time, fast-moving, and originated from a vast variety of terminal devices. However, the big smart grid data also bring various data quality problems, which may cause the delayed, inaccurate analysis of results, even fatal errors in the smart grid system. This paper, therefore, identifies a comprehensive taxonomy of typical data quality problems in the smart grid. Based on the adaptation of established data quality research and frameworks, this paper proposes a new data quality management framework that classifies the typical data quality problems into related data quality dimensions, contexts, as well as countermeasures. Based on this framework, this paper not only provides a systematic overview of data quality in the smart grid domain, but also offers practical guidance to improve data quality in smart grids such as which data quality dimensions are critical and which data quality problems can be addressed in which context.
"Despite the efforts of anti-doping agencies to control doping in sport, the use of performance enhancing substances (PES) and methods remains an intractable issue. Potentially heightening the problem is the emergence of the darknet which provides an anonymous online method for obtaining PES. Despite numerous investigations of PES availability via the regular internet, there is currently little understanding of their availability on the darknet. The aim of this study was to conduct a descriptive analysis of the availability, quantity, cost, and forms of substances included in the 2021 WADA International Standard Prohibited List that are for sale within darknet markets within a 12-week period. A search of substances that are included on the prohibited list was conducted across various darknet trading locations. The search revealed multiple anabolic agents; peptide hormones, growth factors, related substances, and mimetics; hormone and metabolic modulators; and diuretic and masking agents available for purchase on the darknet. The search also revealed the costs, the form (pills, injectables, gels), and available quantities of PES. Given the breadth of products available, it was concluded that the procurement of PES via the darknet presents anti-doping agencies with a novel and challenging problem. Anti-doping agencies and law enforcement could develop strategies to monitor darknet activities and consult cybercrime experts as a part of their toolkit for enforcing anti-doping behaviours. Further research investigating darknet activity of athletes and support personnel is required."
With the growth of mobile computing techniques, mobile gambling scams have seen a rampant increase in the recent past. In mobile gambling scams, miscreants deliver scamming messages via mobile instant messaging, host scam gambling platforms on mobile apps, and adopt mobile payment channels. To date, there is little quantitative knowledge about how this trending cybercrime operates, despite causing daily fraud losses estimated at more than $522,262 USD. This paper presents the first empirical study based on groundtruth data of mobile gambling scams, associated with 1,461 scam incident reports and 1,487 gambling scam apps, spanning from January 1, 2020 to December 31, 2020. The qualitative and quantitative analysis of this ground-truth data allows us to characterize the operational pipeline and full fraud kill chain of mobile gambling scams. In particular, we study the social engineering tricks used by scammers and reveal their effectiveness. Our work provides a systematic analysis of 1,068 confirmed Android and 419 iOS scam apps, including their development frameworks, declared permissions, compatibility, and backend network infrastructure. Perhaps surprisingly, our study unveils that public online app generators have been abused to develop gambling scam apps. Our analysis reveals several payment channels (ab)used by gambling scam app and uncovers a new type of money mule-based payment channel with the average daily gambling deposit of $400,000 USD. Our findings enable a better understanding of the mobile gambling scam ecosystem, and suggest potential avenues to disrupt these scam activities.
In recent years, the proliferation of fake accounts on social media has become a significant concern for individuals, organizations, and society. Fake accounts play an important role in spreading fake news, rumors, spam, unethical harassment, and other mischievous motives on social media platforms. Detecting such accounts manually is time-consuming and challenging, especially with the increasing sophistication of the methods used to create them. Therefore, there is a need for automated approaches to detect these fake accounts. This article aims to reveal fake accounts and their role on social media platforms. This article has explored various methodologies to detect fake accounts from social platforms, which will help prevent cybercrime. We have also proposed a generalized deep learning (DL) model to detect such accounts on social media using multimodal data. The proposed architecture uses a combination of textual, visual, and network-based features to capture the various characteristics of fake accounts. Specifically, we use a deep neural network that combines convolutional neural networks (CNNs) for visual data, long short-term memory (LSTM) networks for textual data, and convolutional graph networks (GCNs) for network-based data. We evaluated our model on a publicly available dataset of Twitter accounts and achieved state-of-the-art performance in detecting fake accounts, with an F1 score of 0.96. We also conducted experiments to show the effectiveness of each feature and the combination of the three features.
Technology is rapidly advancing and every aspect of life is being digitalized. Since technology has made life much better and easier, so organizations, such as businesses, industries, companies and educational institutes, etc., are using it. Despite the many benefits of technology, several risks and serious threats, called cyberattacks, are associated with it. The method of neutralizing these cyberattacks is known as cybersecurity. Sometimes, there are uncertainties in recognizing a cyberattack and nullifying its effects using righteous cybersecurity. For that reason, this article introduces interval-valued complex intuitionistic fuzzy relations (IVCIFRs). For the first time in the theory of fuzzy sets, we investigated the relationships among different types of cybersecurity and the sources of cyberattacks. Moreover, the Hasse diagram for the interval-valued complex intuitionistic partial order set and relation is defined. The concepts of the Hasse diagram are used to inspect different cybersecurity techniques and practices. Then, using the properties of Hasse diagrams, the most beneficial technique is identified. Furthermore, the best possible selection of types of cybersecurity is made after putting some restrictions on the selection. Lastly, the advantages of the proposed methods are illuminated through comparison tests.
Nowadays, VANET (Vehicular Ad-hoc NETwork) has gained increasing attention from many researchers with its various applications, such as enhancing traffic safety by collecting and disseminating traffic event information. This increased interest in VANET has necessitated greater scrutiny of machine learning (ML) methods used for improving the security capabilities of intrusion detection systems (IDSs), such as the need to solve computationally intensive ML problems due to the increased vehicular data. Therefore, in this paper, we propose a hybrid ML model to enhance the performance of IDSs by dealing with the explosive growth in computing power and the need for detecting malicious incidents timely. The proposed approach mainly uses the advantages of Random Forest to detect known network intrusions. Besides, there is a post-detection phase to detect possible novel intruders by using the advantages of coresets and clustering algorithms. Our approach is evaluated over a very recent IDS dataset named CICIDS2017. The preliminary results show that the proposed hybrid model can increase the utility of IDSs. (C) 2021 The Authors. Published by Elsevier B.V.
Credit/debit cards are a ubiquitous form of payment at present. They offer a number of advantages over cash, including convenience, security, and fraud protection. In contrast, the inherent vulnerabilities of credit/debit cards and transaction methods have led many payment institutions to focus on strengthening the security of these electronic payment methods. Also, the increasing number of electronic payment transactions around the world have led to a corresponding increase in the amount of money lost due to fraud and cybercrime. This loss of money has a significant impact on businesses and consumers, and it necessitates the development of rigid and robust security designs for securing underlying electronic transaction methods. In this regard, this research introduces a novel geolocation-based multi-factor authentication method for improving the security of electronic payment transactions, especially ATM transactions. The proposed method leverages geolocation to verify the user's identity and prevent fraudulent transactions. In addition, this research also proposes a novel design approach for further controlling the ownership of transactions in a convenient way (e.g., allowing users to deactivate/reactivate authentication at any time, block the card in case it is stolen or lost, and set up a withdrawal limit). Overall, this approach does not require any major modifications to the existing banking infrastructure, which would be an ideal solution for securing ATM transactions around the world.
The world is facing a cybersecurity skills gap as cybercrime and cyberwarfare grow in importance. One often-discussed quality that is potentially relevant to cybersecurity recruitment and education is the so-called security mindset: a way of thinking characteristic of some security professionals that they believe to be especially advantageous in their work. Although some employers express a desire to hire people with a security mindset, and initiatives to cultivate the security mindset are being implemented, it has no common definition and little is known about its characteristics, its development, and its consequences. We interviewed 21 cybersecurity professionals who strongly identified as having a security mindset based on a minimal description drawn from existing literature. Thematic analysis of the interview data suggests that the security mindset can be conceptualized as consisting of three interconnected aspects-monitoring for potential security anomalies, investigating anomalies more deeply to identify security flaws, and evaluating the relevance of those flaws in a larger context. These three aspects develop in different ways and have different personal and professional consequences. Participants mostly spoke positively of the security mindset, but they also mentioned several disadvantages not mentioned by existing security-mindset literature, such as mental health pressures, workplace tensions, and negative effects on personal relationships. We discuss the implications of these findings for future study of the security mindset and suggest practical implications for cybersecurity management, education, and recruitment.
With society becoming increasingly digital, new opportunities are afforded to potential offenders to weaponize digital features and affordances to carry out their crimes. As a result, concerns persist over online forms of crime, particularly cybercrime involving sexual exploitation, and what can be done about them. Drawing on interview and focus group data collected from 70 sex crime investigators from police service organizations across Canada, we uncover police perspectives on online sex-based crime. We demonstrate that police perceive online crimes to not necessarily be new forms of crime, but rather altered by digital media in terms of methods and weapons being used. We focus on uncovering the features and affordances police identify as contributing to the increase in crime itself as well as the creation of greater opportunities for crime to occur. In addition, resulting from crime shifting into digital spaces, we uncover the challenges digital media has presented for police in terms of how they handle, respond to, and investigate online crime. We discuss these challenges and their impact on policing and provide solutions for combatting them moving forward. Overall, this article contributes to the current body of literature investigating online crime and policing in the digital age by drawing on the theoretical framework of affordances and offering police perspectives on online sex-based crimes.
The emergence of synthetic media such as deep fakes is considered to be a disruptive technology shaping the fight against cybercrime as well as enabling political disinformation. Deep faked material exploits humans' interpersonal trust and is usually applied where technical solutions of deep fake authentication are not in place, unknown, or unaffordable. Improving the individual's ability to recognise deep fakes where they are not perfectly produced requires training and the incorporation of deep fake-based attacks into social engineering resilience training. Individualised or tailored approaches as part of cybersecurity awareness campaigns are superior to a one-size-fits-all approach, and need to identify persons in particular need for improvement. Research conducted in phishing simulations reported that persons with educational and/or professional background in information technology frequently underperform in social engineering simulations. In this study, we propose a method and metric to detect overconfident individuals in regards to deep fake recognition. The proposed overconfidence score flags individuals overestimating their performance and thus posing a previously unconsidered cybersecurity risk. In this study, and in line with comparable research from phishing simulations, individuals with IT background were particularly prone to overconfidence. We argue that this data-driven approach to identifying persons at risk enables educators to provide a more targeted education, evoke insight into own judgement deficiencies, and help to avoid the self-selection bias typical for voluntary participation.
ln the last few years, the high acceptability of service computing delivered over the internet has exponentially created immense security challenges for the services providers. Cyber criminals are using advanced malware such as polymorphic botnets for participating in our everyday online activities and trying to access the desired information in terms of personal details, credit card numbers and banking credentials. Polymorphic botnet attack is one of the biggest attacks in the history of cybercrime and currently, millions of computers are infected by the botnet clients over the world. Botnet attack is an intelligent and highly coordinated distributed attack which consists of a large number of bots that generates big volumes of spamming e-mails and launching distributed denial of service (DDoS) attacks on the victim machines in a heterogeneous network environment. Therefore, it is necessary to detect the malicious bots and prevent their planned attacks in the cloud environment. A number of techniques have been developed for detecting the malicious bots in a network in the past literature. This paper recognize the ineffectiveness exhibited by the singnature based detection technique and networktraffic based detection such as NetFlow or traffic flow detection and Anomaly based detection. We proposed a real time malware detection methodology based on Domain Generation Algorithm. It increasesthe throughput in terms of early detection of malicious bots and high accuracy of identifying the suspicious behavior.
"The Internet and social networks are increasingly becoming a media of extremist propaganda. On homepages, in forums or chats, extremists spread their ideologies and world views, which are often contrary to the basic liberal democratic values of the European Union. It is not uncommon that violence is used against those of different faiths, those who think differently, and members of social minorities. This paper presents a set of instruments and tools developed to help investigators to better address hybrid security threats, i.e., threats that combine physical and cyber attacks. These tools have been designed and developed to support security authorities in identifying extremist propaganda on the Internet and classifying it in terms of its degree of danger. This concerns both extremist content on freely accessible Internet pages and content in closed chats. We illustrate the functionalities of the tools through an example related to radicalisation detection; the data used here are just a few tweets, emails propaganda, and darknet posts. This work was supported by the EU granted PREVISION (Prediction and Visual Intelligence for Security Intelligence) project."
The online romance scam is a cybercrime that has resulted in substantial impact on victims, both psychologically and financially. The display of a personal advertisement by scammers on online dating websites in the form of the user profile is often the first step in the scam strategy to attract potential victims. This study examined differences in the use of language in self- and desired partner-description found in both the scammer profiles and a set of general user profiles on online dating websites. Using a corpus-based methodology, 500 user profiles identified as having fraudulent characteristics were compared with the same number of user profiles downloaded from five popular dating portals. Words that are statistically different in their frequencies of occurrence in the two sets of data were generated using a corpus analysis software and then categorized based on semantic and contextual criteria. The analysis uncovered linguistic features that are distinctive of the scammer profiles, revealing strategies used by online scammers in constructing and engaging a certain type of victim, by intensively invoking love, romance and commitment, and conjuring hope of a future as a couple through pertinent use of lexical and function words. Implications of the study include its contribution to the understanding and identification of the linguistic indicators of romance fraud user profiles which are important in efforts to raise public awareness and for future development of computerized scam detection tools.
Purpose As emphasised by the theory of knowledge-based view, knowledge constitutes the basic element for a firm's competitive advantage. Consequently, a firm's knowledge at risk could have an adverse effect on its performance. In this regard, this paper aims to investigate potential knowledge risks present in an (ICT)-supported collaborative project and establishes inter- and multi-relationships among these risks. Design/methodology/approach In this paper, an integrated approach using the total interpretive structural modelling (TISM) technique and MICMAC analysis is implemented to determine the hierarchical inter-relationships among knowledge risks and classify them according to their driving and dependence power. Findings The result reveals seven knowledge risks. The analysis establishes cybercrime and espionage as high drivers of knowledge risks in an ICT-supported collaborative project. Further, a comprehensive model is developed showing the hierarchical structure and multi- and inter-relationships among the analysed risks. Practical implications From a practical viewpoint, the proposed model in this study will be of great importance to practitioners because it highlights the most prominent knowledge risks in an ICT-supported collaborative project. Additionally, it will provide a clue for effective knowledge risk management in a systematic approach. Originality/value To the best of the author's knowledge, this is one of the first studies to use both the TISM technique and MICMAC analysis to identify and classify knowledge risks in an ICT-supported collaborative project.
Due to the advancement of technology, cybercrime has increased considerably, making digital forensics essential for any organisation. One of the most critical challenges is to analyse and classify the information on devices, identifying the relevant and valuable data for a specific purpose. This phase of the forensic process is one of the most complex and time-consuming, and requires expert analysts to avoid overlooking data relevant to the investigation. Although tools exist today that can automate this process, they will depend on how tightly their parameters are tuned to the case study, and many lack support for complex scenarios where language barriers play an important role. Recent advances in machine learning allow the creation of new architectures to significantly increase the performance of information analysis and perform the intelligent search process automatically, reducing analysis time and identifying relationships between files based on initial parameters. In this paper, we present a bibliographic review of artificial intelligence algorithms that allow an exhaustive analysis of multimedia information contained in removable devices in a forensic process, using natural language processing and natural language understanding techniques for the automatic classification of documents in seized devices. Finally, some of the open challenges technology developers face when generating tools that use artificial intelligence techniques to analyse the information contained in documents on seized devices are reviewed.
"With the significant growth of internet usage, people increasingly share their personal information online. As a result, an enormous amount of personal information and financial transactions become vulnerable to cybercriminals. Phishing is an example of a highly effective form of cybercrime that enables criminals to deceive users and steal important data. Since the first reported phishing attack in 1990, it has been evolved into a more sophisticated attack vector. At present, phishing is considered one of the most frequent examples of fraud activity on the Internet. Phishing attacks can lead to severe losses for their victims including sensitive information, identity theft, companies, and government secrets. This article aims to evaluate these attacks by identifying the current state of phishing and reviewing existing phishing techniques. Studies have classified phishing attacks according to fundamental phishing mechanisms and countermeasures discarding the importance of the end-to-end lifecycle of phishing. This article proposes a new detailed anatomy of phishing which involves attack phases, attacker's types, vulnerabilities, threats, targets, attack mediums, and attacking techniques. Moreover, the proposed anatomy will help readers understand the process lifecycle of a phishing attack which in turn will increase the awareness of these phishing attacks and the techniques being used; also, it helps in developing a holistic anti-phishing system. Furthermore, some precautionary countermeasures are investigated, and new strategies are suggested."
A significant negative impact of spam e-mail is not limited only to the serious waste of resources, time, and efforts, but also increases communications overload and cybercrime. Perhaps the most damaging aspect of spam email is that it has become such a major tool for attacks of cross-site scripting, malware infection, phishing, and cross-site request forgery, etc. Due to the nature of the adaptive unsolicited spam, it has been weakening the effect of the previous discovery techniques. This article proposes a new Spam Detection System (SDS) framework, by using a series of six different variants of enhanced Grasshopper Optimization Algorithm (EGOAs), which are investigated and combined with a Multilayer Perceptron (MLP) for the purpose of advanced spam email detection. In this context, the combination of MLP and EGOAs produces Neural Network (NN) models, referred to (EGOAMLPs). The main idea of this research is to use EGOAs to train the MLP to classify the emails as spam and non-spam. These models are applied to SpamBase, SpamAssassin, and UK-2011 Webspam benchmark datasets. In this way, the effectiveness of our models on detecting diverse forms of spam email is evidenced. The results showed that the proposed MLP model trained by EGOAs achieves a higher performance compared to other optimization methods in terms of accuracy, detection rate, and false alarm rate.
Recently, it reported that the cases of cybercrime had increased by about 13% in 2017, especially in data fraud crime, due to the poor security system. One of the solutions to this problem is by implementing a biometric system as a security platform. Biometric is a method of personal ID authentication by using human recognition using unique human biometric values such as fingerprint, voice, face, or iris. This review paper focused on the iris biometric system since Iris biometric widely used as a system for maintaining data security, such as ATM, cellular phone, etc. Moreover, the biometric has very high sensitivity and accuracy for recognition than the other. In this paper, we will review some experiments of iris recognition that was done by researchers. As a result, we will present some comparisons among several studies, including in feature extraction technique, performance, and time computation required on the matching process. From the paper review, we can analyze that a good result of iris recognition depends on the selection of the right features and the needed time computation. The use of Haar Wavelet (texture analysis) with three different iris databases showed the best performance, such as MMU (ACC = 96.44%, time = 0.34s), UTIRIS (ACC = 96.21%, time = 0.68s), and IITD (ACC = 97.14%, time = 0.64s).
Digital forensics is a rapidly growing and emerging field, which is considered as a part of information/computer security. Nowadays, the expansion of cybercrime incidents and criminal acts are becoming oppressive and the proper education and training in the digital forensic area are becoming very important due to the increasing demand for the professionals in this field. The study presented in the paper is a contribution to the educational game design advancement by providing the participants empirical experience close to the real life environment. The positive correlation between the gaming and the test scores points out that gaming during learning positively stimulates the development of the essential skills, know-hows and abilities, such as problem solving, strategic thinking, memory, fantasy, interaction and adaptation to the learning focus, which are elements well captured by the standardized checks in particular subjects. The goal of the obtained research was also to find out whether the learning attributes acknowledged in the upgraded taxonomy can be applied within a game designing process, as their usability was studied only in the assessment of already existing and popular significant games. The in-built in game learnability properties are based on the lately enhanced taxonomy game used for assessment and evaluation of educational results. The course design of the game is described and the learnability properties are estimated through survey and the observation of the participants.
As cybercrime is on the rise, individuals would like to rest assured that their authentication information cannot be stolen, and then used to gain access to their privileged information. Smart cards can and have played a pivotal role in lowering the statistics on identity theft. This has been achieved by predominantly implementing biometrics matching algorithms inside smart card technology. The biometric matching inside a smart card is known as Match-on-Card/On-Card comparison. However compared to traditional biometric systems implemented on PCs' and servers, smart cards are resource constrained. In addition smart cards do not implement mathematical functions such as trigonometry since they are mainly meant for secure data storage and simple processing. The current state of the On-Card biometric comparison technology is limited in that data for On-Card comparison either has to be pre-calculated outside the card at runtime or fetched from a look-up table. This is because of the limited mathematical operations available inside a smart card. The pre-calculation of data outside the smart card compromises the security offered by the card and the look-up table limits the accuracy of the biometric comparison. The paper reviews the techniques and challenges of implementing fingerprint On-Card comparison algorithms in a smart card environment. Approaches of overcoming the On-Card comparison challenges are also discussed.
Quality improvement of practical cybersecurity training is challenging due to the process-oriented nature of this learning domain. Event logs provide only a sparse preview of trainees' behavior in a form that is difficult to analyze. Process mining has great potential in converting events into behavioral graphs that could provide better cognitive features for understanding users' behavior than the raw data. However, practical usability for learning analytics is affected by many aspects. This paper aims to provide an experience report summarizing key features and obstacles in integrating process discovery into cyber ranges. We describe our lessons learned from applying process mining techniques to data captured in a cyber range, which we have been developing and operating for almost ten years. We discuss lessons learned from the whole workflow that covers data preprocessing, data mapping, and the utilization of process models for the post-training analysis of Capture the Flag games. Tactics addressing scalability are explicitly discussed because scalability has proven to be a challenging task. Interactive data mapping and Capture the Flag specific features are used to address this issue.
In our digital era, insider attacks are among the serious underresearched areas of the cybersecurity landscape. A significant type of insider attack is facilitated by employees without malicious intent. They are called unintentional perpetrators. We proposed mitigating these threats using a simulation-game platform to detect the potential attack vectors. This paper introduces and implements a scenario that demonstrates the usability of this approach in a case study. This work also helps to understand players' behavior when they are not told upfront that they will be a target of social engineering attacks. Furthermore, we provide relevant acquired observations for future research.
This demo paper presents a dashboard for network security management, a web application that visualizes data gathered by various sensors in the network and allows the user to achieve cyber situational awareness and provides decision support in the incident handling process. The dashboard and its underlying database use modern graph-based approaches to data modelling, storing, and querying. The dashboard speeds up routine tasks in incident handling, such as getting a context of a situation and quickly assessing the spread and impact of vulnerabilities. The implementation uses modern graph-based approaches to data storage and visualization.
Analyzing network traffic is one of the fundamental tasks in both network operations and security incident analysis. Despite the immense efforts in workflow automation, an ample portion of the work still relies on manual data exploration and analytical insights by domain specialists. Current state-of-the-art network analysis tools provide high flexibility at the expense of usability and have a steep learning curve. Recent-often web-based-analytical tools emphasize interactive visualizations and provide simple user interfaces but only limited analytical support. This paper describes the tool that supports the analytical work of network and security operators. We introduce typical user tasks and requirements. We also present the filtering funnel metaphor for exploring packet capture (PCAP) files through visualizations of linked filter steps. We have created PCAPFunnel, a novel tool that improves the user experience and speeds up packet capture data analysis. The tool provides an overview of the communication, intuitive data filtering, and details of individual network nodes and connections between them. The qualitative usability study with nine domain experts confirmed the usability and usefulness of our approach for the initial data exploration in a wide range of tasks and usage scenarios, from educational purposes to exploratory network data analysis.
Nowadays, a growing need for the forensic investigation of cybercrimes emerges. It is because of the rising threat of those crimes in cyberspace. Nevertheless, such investigations are highly time-consuming with delicate supportive processes. The problem of systematic preparation on the potential forensic investigation during the software development process, called forensic readiness, has only been explored since recently. Therefore there are still many open issues and challenges, with missing methods and guidelines that would support software engineers in building software systems that are forensic ready. One of the essential open challenges is the understanding of the forensic-readiness requirements that shall be embedded in the software system early during its design, and then also the techniques to ensure and verify forensic readiness and its specific needs along the way. In this paper, we propose a research roadmap towards verification of forensic readiness in software design and development, to help the progress of this new research domain. The results of the research based on the roadmap can then support software engineers in designing critical, forensic-ready systems, together with possible perspective methods of capturing and verifying the specific requirements constituting forensic readiness.
This article presents an overview of the growth of internet piracy in the global marketplace. The ethical perceptions (or lack of) of the younger generation is addressed, in terms of their willingness to consume counterfeit goods on the web. Firms face the task of educating the consumer that downloading music, software, movies and the like, without compensation, is unethical. This awareness is critical for decreasing the demand for counterfeit goods in the virtual marketplace, where a consumer can exhibit a rogue behaviour with a limited fear of prosecution. We address the pyramid of internet piracy, which encompasses sophisticated suppliers/facilitators, such as the Warez group. Recent sting operations, such as Operation Buccaneer, are also depicted to highlight successful tactical manoeuvres of enforcement agencies. An overview of the Digital Millennium Copyright Act and the No Electronic Theft Act is included to debate the controversy surrounding this legislation. A discussion of enterprise enforcement mechanisms and novel anti-piracy technology for cyberspace is provided to reveal some of the tools used to fight the pirates, such as innovations in digital watermarking and NEC's recently announced video content identification technology. Enterprise information systems and its interdependence on the internet are also demanding new technologies that enable internet investigators to rapidly search, verify and potentially remove pirated content using web services. The quality of service of web services designed to efficiently detect pirated content is a growing consideration for new anti-piracy technology.
Socialization has changed the behavior of physical objects in IoT (Internet of Things) by introducing the SIoT (Social Internet of Things) paradigm to emulate the social behavior of humans in a connected cyber-physical world. To create reliable social networking in IoT, trust management has been used to build social relationships autonomously and facilitate trustworthy information sharing among physical objects. Although trust management is being highlighted as a promising security mechanism, the understanding of its potential across various SIoT application domains is so far limited. Thus, in this paper, we examine the social aspects of trust management in the Internet of Things and their role in supporting the evolution of IoT towards SIoT. To this end, we examine existing work in various SIoT application domains in distinct environments to understand how trust management socializes smart objects among each domain, with the Underwater Acoustic Sensor Networks, Vehicular Ad-hoc Networks, and Flying Ad-hoc Networks being identified as the most popular domains, together with the Internet of Medical Things as a specific SIoT domain with raising interest by the scientific community and need of trustworthy interactions between patients and things. Finally, we highlight the future research directions that can be considered to improve the applicability of trust management in SIoT and reinforce the socialization of smart IoT objects in the digital ecosystems of the future.
The energy domain is part of Critical Infrastructures (CI), in which Smart Grids (SG) play the role of major enabler of concepts such as Smart Cities. The capability of reasoning through ontologies is of paramount importance to allow a better integration of sensors and devices part of the smart energy domain. In this paper, we provide a comprehensive evaluation of semantic reasoning in the energy domain, by evaluating the performance of ten ontologies in terms of different metrics such as load time and run-time performance, discussing how such ontologies can be applicable in case of deployment on devices with constrained resources.
"Nowadays, the web provides all of the nation's daily necessities, and time spent online is rising quickly. The Internet is being used more widely than ever. As a result, cyberattacks and cybercrime are becoming more prevalent. Various machine learning techniques will be used to rec-ognize network attacks and defend against cyber security threats. Developing intrusion detection systems can improve cybersecurity and identify anomalies on a computer server. An efficient intru-sion detection and prevention system will be created using machine learning techniques. Each intru-sion detection categorization system evaluated in this study has its unique uses. The Firefly Optimization (FFO) technique was used to detect the intrusions before the categorization proce-dure was carried out using a machine learning classifier. It considered how the anomalies in net-works were categorized in this research. The outcomes of the detection techniques will be validated using the Knowledge Discovery Dataset (KDD-CUP 99). The proposed method involves Probabilistic Neural Network for the categorization. The implementation will assess many perfor-mance metrics for various cyber-attack types, including specificity, recall, F1-score, accuracy, pre-cision, and sensitivity. The proposed technique achieves a high accuracy of 98.99%.& COPY; 2023 THE AUTHORS. Published by Elsevier BV on behalf of Faculty of Engineering, Alexandria University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/ licenses/by-nc-nd/4.0/)."
Phishing is a fraudulent technique used by attackers known as phishers for obtaining credentials (username and passwords) of a specific or group of users on internet. Phishing came into focus in 1996, and from then it is emerging out as one of the biggest cybercrime attacks on internet. The solutions for handling phishing attacks include: detecting the activity and filtering it from normal activity (Phished Email and Website detection), preventing it by the use of excellent user interfaces and login authentication schemes and user training so that users can prevent themselves from being falling into it. Researchers have focused on all the three techniques for solving phishing attacks. However the previous techniques handle the problem of phishing to an extent but are incomplete, and complex to implement or use in actuality. The research work in this paper will be focused on the critical review of previous schemes proposed, with a novel scheme for preventing phishing attacks with the use of four characters secret information display during login authentication. The novelty of the scheme lies in its low complexity, better user understandability and real time implementation ability. This paper also describes the analyzed results of a real time experiment done to evaluate the scheme. Our results show great improvement with a total of only 13.5%, 1% and 3% user's phished in rigorous phishing attempts, of various kinds for a period of one month.
Both the academy and industry believe that Intelligent Transportation System (ITS) would be achievable in one decade since modern vehicle and communication technologies advanced apace. Vehicular Communication System (VCS) introduces information technology to the ITS and aims to improve road safety and traffic efficiency. In recent year, security and privacy schemes in VCS are becoming important. However, recovery mechanisms to eliminate the negative effect of security and privacy attacks are still an important topic for research. Therefore, the certificate revocation scheme is considered as a feasible technique to prevent the system from potential attacks. The major challenge of the certificate revocation scheme is to achieve low-cost operation since the communication resources must be capable of carrying various applications apart from the security and privacy purposes. In this paper, we propose an efficient certificate revocation scheme in VCS. The Blockchain concept is introduced to simplify the network structure and distributed maintenance of the Certificate Revocation List (CRL). The proposed scheme embeds part of the certificate revocation functions within the security and privacy applications, aiming to reduce the communication overhead and shorten the processing time cost. Extensive simulations and analysis show the effectiveness and efficiency of the proposed scheme, in which the Blockchain structure costs fewer network resources and gives a more economic solution to against further cybercrime attacks. (C) 2019 Published by Elsevier B.V.
When searching for a brand name in search engines, it is very likely to come across websites that sell fake brand's products. In this paper, we study how to tackle and measure this problem automatically. Our solution consists of a pipeline with two learning stages. We first detect the ecommerce websites (including shopbots) present in the list of search results and then discriminate between legitimate and fake ecommerce websites. We identify suitable learning features for each stage and show through a prototype system termed RI.SI.CO. that this approach is feasible, fast, and highly effective. Experimenting with one goods sector, we found that RI.SI.CO. achieved better classification accuracy than that of non-expert humans. We next show that the information extracted by our method can be used to generate sector-level 'counterfeiting charts' that allow us to analyze and compare the counterfeit risk associated with different brands in a same sector. We also show that the risk of coming across counterfeit websites is affected by the particular web search engine and type of search query used by shoppers. Our research offers new insights and some very practical and useful means for analyzing and measuring counterfeit ecommerce websites in search-engine results, thus enabling targeted anti-counterfeiting actions.
Networking, operating systems, and cybersecurity skills are exercised best in an authentic environment. Students work with real systems and tools in a lab environment and complete assigned tasks. Since all students typically receive the same assignment, they can consult their approach and progress with an instructor, a tutoring system, or their peers. They may also search for information on the Internet. Having the same assignment for all students in class is standard practice efficient for learning and developing skills. However, it is prone to cheating when used in a summative assessment such as graded homework, a mid-term test, or a final exam. Students can easily share and submit correct answers without completing the assignment. In this paper, we discuss methods for automatic problem generation for hands-on tasks completed in a computer lab environment. Using this approach, each student receives personalized tasks. We developed software for generating and submitting these personalized tasks and conducted a case study. The software was used for creating and grading a homework assignment in an introductory security course enrolled by 207 students. The software revealed seven cases of suspicious submissions, which may constitute cheating. In addition, students and instructors welcomed the personalized assignments. Instructors commented that this approach scales well for large classes. Students rarely encountered issues while running their personalized lab environment. Finally, we have released the open-source software to enable other educators to use it in their courses and learning environments.
"The article analyses the economic aspects of establishing the practice of remote client identification by Russian credit institutions. Developing such practices is necessitated by the permanent digitization of the interaction between subjects of banking relations. The article presents the results of an analysis of the economic aspects of remote client identification by banks in developed and developing countries (Austria, Great Britain, India, China, USA, Sweden, Switzerland, and South Korea). The research reveals the theoretical and practical peculiarities of the banking biometric identification system currently functioning in Russia. On the basis of this analysis, the author identifies economic reasons for the lack of motivation of most Russian banks to develop new practices. For the first time in Russian economic literature, the key problems that impede remote client identification have been discussed systematically, including: a) risk of unauthorized access to the central database; b) concerns about privacy violations; c) lack of free access to materials on software testing, guarantees of developer companies, operators, etc.; d) risk of unauthorized access to the UBS by outsiders successfully using identification procedures; e) exposure to financial risks from unauthorized access to the UBS, transferred to clients; f) unsatisfactory results of combating banking cybercrime in Russia. The author advances several robust proposals to overcome these problems and minimize the economic risks of applying remote client identification systems in Russia, taking into account the positive and negative experiences of foreign countries."
Cybercrime can be associated with undisclosed social media accounts deliberately used to conduct unethical or illegal activities such as cyberbullying, fraudulent transactions, human trafficking, etc. The objective of this paper is to identify whether two social media accounts belong to the same person by examining the accounts' writing, i.e. comments and posts. To that end, this preliminary study introduces a new algorithm, ChunkedHCs, specifically designed for the authorship verification task to decide whether a pair of texts are written by the same person. In the domain of machine learning and deep learning, there have been previous authorship verification approaches, which often involve complex feature selections or sophisticated pre-processing steps due to the complexity of topic heterogeneity. Such limits provide motivations to seek a simpler yet more robust approach that could offer competitive verification ability. ChunkedHCs is based on the statistical testing Higher Criticism (Donoho and Jin, 2004) and the HC-based similarity algorithm (Kipnis, 2020a & 2020b) (Kestemont et al., 2020). Using Reddit users' data, ChunkedHCs offer a promising performance with an accuracy of 0.94 and an F1 of 0.9381 for texts between 29,000 and 30,000 characters. It is speculated that the algorithm could also be highly applicable to identify if two accounts are used by the same person for other social media platforms such as Facebook, Twitter and even dark web forums. Various avenues of further research on ChunkedHCs are also proposed. (C) 2021 The Authors. Published by Elsevier Ltd.
The emergence of a large number of new malicious code poses a serious threat to network security, and most of them are derivative versions of existing malicious code. The classification of malicious code is helpful to analyze the evolutionary trend of malicious code families and trace the source of cybercrime. The existing methods of malware classification emphasize the depth of the neural network, which has the problems of a long training time and large computational cost. In this work, we propose the shallow neural network-based malware classifier (SNNMAC), a malware classification model based on shallow neural networks and static analysis. Our approach bridges the gap between precise but slow methods and fast but less precise methods in existing works. For each sample, we first generate n-grams from their opcode sequences of the binary file with a decompiler. An improved n-gram algorithm based on control transfer instructions is designed to reduce the n-gram dataset. Then, the SNNMAC exploits a shallow neural network, replacing the full connection layer and softmax with the average pooling layer and hierarchical softmax, to learn from the dataset and perform classification. We perform experiments on the Microsoft malware dataset. The evaluation result shows that the SNNMAC outperforms most of the related works with 99.21% classification precision and reduces the training time by more than half when compared with the methods using DNN (Deep Neural Networks).
Cryptography or message hiding techniques can be a solution to avoid cybercrime. One of the techniques is Playfair Cipher that is a cryptographic algorithm. It can hide messages by forming a message key in the form of a 5x5 square-shaped key matrix to accommodate 25 capital letter characters by removing the letter J and replacing it with the letter I and making plaintext messages into letter pairs (bigram) to do the encryption process with some rules and provisions in this algorithm. So that a ciphertext is formed which has the same character length as the plaintext. The Playfair Cipher algorithm mechanism is the weakness of Playfair Cipher, which is the limitation in accommodating characters. Eliminate the letter J that causes ambiguity and the length of the plaintext character and the same ciphertext so that it is easy to solve with the frequency of occurrence of the bigram. Modifying Playfair cipher algorithm by extending into 5x19 key matrix with ninety-five characters to hold any types of characters (capital letters, small, numbers, symbols) solving the ambiguity. The expansion techniques will form different ciphertext results with its plaintext to overcome the weakness of Playfair cipher. The aim of this research was to produce a Playfair cipher modification algorithm that could cover the weakness of Playfair cipher and result in differences in character length on plaintext and ciphertext implementation results from Playfair cipher modification.
This paper provides a survey of prediction, and forecasting methods used in cyber security. Four main tasks are discussed first, attack projection and intention recognition, in which there is a need to predict the next move or the intentions of the attacker, intrusion prediction, in which there is a need to predict upcoming cyber attacks, and network security situation forecasting, in which we project cybersecurity situation in the whole network. Methods and approaches for addressing these tasks often share the theoretical background and are often complementary. In this survey, both methods based on discrete models, such as attack graphs, Bayesian networks, and Markov models, and continuous models, such as time series and grey models, are surveyed, compared, and contrasted. We further discuss machine learning and data mining approaches, that have gained a lot of attention recently and appears promising for such a constantly changing environment, which is cyber security. The survey also focuses on the practical usability of the methods and problems related to their evaluation.
Capture the Flag challenges are a popular form of cybersecurity education, where students solve hands-on tasks in an informal, game-like setting. The tasks feature diverse assignments, such as exploiting websites, cracking passwords, and breaching unsecured networks. However, it is unclear how the skills practiced by these challenges match formal cybersecurity curricula defined by security experts. We explain the significance of Capture the Flag challenges in cybersecurity training and analyze their 15,963 textual solutions collected since 2012. Based on keywords in the solutions, we map them to well-established ACM/IEEE curricular guidelines to understand which skills the challenges teach. We study the distribution of cybersecurity topics, their variance in different challenge formats, and their development over the past years. The analysis showed the prominence of technical knowledge about cryptography and network security, but human aspects, such as social engineering and cybersecurity awareness, are neglected. We discuss the implications of these results and relate them to contemporary literature. Our results indicate that future Capture the Flag challenges should include non-technical aspects to address the current advanced cyber threats and attract a broader audience to cybersecurity. (C) 2020 Elsevier Ltd. All rights reserved.
The anonymous system Tor uses an asymmetric algorithm to protect the content of communications, allowing criminals to conceal their identities and hide their tracks. This malicious usage brings serious security threats to public security and social stability. Statistical analysis of traffic flows can effectively identify and classify Tor flow. However, few features can be extracted from Tor traffic, which have a weak representational ability, making it challenging to combat cybercrime in real-time effectively. Extracting and utilizing more accurate features is the key point to improving the real-time detection performance of Tor traffic. In this paper, we design an efficient and real-time identification scheme for Tor traffic based on the time window method and bidirectional statistical characteristics. In this paper, we divide the network traffic by sliding the time window and then calculate the relative entropy of the flows in the time window to identify Tor traffic. We adopt a sequential pattern mining method to extract bidirectional statistical features and classify the application types in the Tor traffic. Finally, extensive experiments are carried out on the UNB public dataset (ISCXTor2016) to validate our proposal's effectiveness and real-time property. The experiment results show that the proposed method can detect Tor flow and classify Tor flow types with an accuracy of 93.5% and 91%, respectively, and the speed of processing and classifying a single flow is 0.05 s, which is superior to the state-of-the-art methods.
Multi-purpose Messaging Mobile App (MMMA) combines several functionalities in a single APP to provide integrated service that brings tremendous convenience to users. Therefore, MMMAs become more and more popular. However, the prevalence of MMMAs also makes them a hotbed for cybercrime. Among them, Click Farming fraud requires special attention, as it causes substantial pecuniary losses and is challenging to detect. In this paper, we describe Multi-view Heterogeneous Temporal Graph Neural Network (MHT-GNN), a framework for detecting Click Farming fraudsters in a popular MMMA called WeChat. We first adopt a Heterogeneous Temporal Graph (HTG) to model spatial, heterogeneous and temporal information contained in MM-MA data. We then extract two different types of user history sequences as two views of user behavior patterns from HTG. MHT-GNN contains a pretraining phase and a detection phase. The main components in MHT-GNN include Inductive Heterogeneous GNN Encoder, Temporal Snapshot Sequence Encoder, and User Relation Sequence Encoder. The first encoder aims to capture spatial information and the heterogeneity in each snapshot of HTG. The later two encoders are designed to incorporate temporal information to better reveal user's behavior patterns and MHT-GNN leverages them to capture the two different views of user history behavior data. We conduct experiments on a real-world, million-scale dynamic graph extracted from WeChat. Experimental results demonstrate the effectiveness of MHT-GNN: it significantly exceeds existing detection methods, and it is able to block Click Farming fraud activities.
Complex Big Data systems in modern organisations are progressively becoming attack targets by existing and emerging threat agents. Elaborate and specialised attacks will increasingly be crafted to exploit vulnerabilities and weaknesses. With the ever-increasing trend of cybercrime and incidents due to these vulnerabilities, effective vulnerability management is imperative for modern organisations regardless of their size. However, organisations struggle to manage the sheer volume of vulnerabilities discovered on their networks. Moreover, vulnerability management tends to be more reactive in practice. Rigorous statistical models, simulating anticipated volume and dependence of vulnerability disclosures, will undoubtedly provide important insights to organisations and help them become more proactive in the management of cyber risks. By leveraging the rich yet complex historical vulnerability data, our proposed novel and rigorous framework has enabled this new capability. By utilising this sound framework, we initiated an important study on not only handling persistent volatilities in the data but also further unveiling multivariate dependence structure amongst different vulnerability risks. In sharp contrast to the existing studies on univariate time series, we consider the more general multivariate case striving to capture their intriguing relationships. Through our extensive empirical studies using the real world vulnerability data, we have shown that a composite model can effectively capture and preserve long-term dependency between different vulnerability and exploit disclosures. In addition, the paper paves the way for further study on the stochastic perspective of vulnerability proliferation towards building more accurate measures for better cyber risk management as a whole.
The fundamental changes in fight against crime policy by choosing forensic science as a strategic tool in this fight are currently undergoing in European Union (EU). Hence, every Member State has specific system of forensic science institutions, the unique conception of forensic science and different legal systems as well. Meanwhile, the vision for European Forensic Science 2020 is one page length abstract document, providing only the guidelines for creation of European Forensic Science Area and including a list of areas (terrorism, cybercrime, etc.) covered, but not prescribing any implementation measures. Acting in accordance with the project The Conception of Implementation of vision for European Forensic Science 2020 in Lithuania (EUROVIFOR) funded by Research Council of Lithuania, the authors have accomplished analysis of scientific literature and legal regulations governing the application of special knowledge indifferent countries representing different schools of forensic science: Germanic (Germany, Austria), Roman (France), the Anglo-Saxons (Great Britain), Eastern Europe (Poland, Latvia, Estonia, Lithuania, Bulgaria, Slovenia, Ukraine, Russia, Belarus). This paper presents the results of abovementioned analysis showing the need of harmonization of legal regulations, the main directions of such a harmonization, its positive potential in promoting cooperation of law enforcement authorities across EU and some other problems in forensic science sector to be solved in order European Forensic Science Area would be created by 2020. This research was funded by a grant (No. MIP089/2014) of the Research Council of Lithuania.
Botnets, the groups of illegally controlled infected devices on the Internet have had a history of two decades already. This history shows an evolution of the infection techniques, the scope of the target devices, and their usage. Thus, the new direction is the usage of sophisticated data leakage techniques by state-sponsored hacker groups. Our article analyses this evolution while focusing on Botnet usage for cyber espionage. We present the Botnet architecture in the context of network science research, lifecycle, applied network protocols, and capabilities. Next, we analyze two examples, the APT28 group activities and the VPNFilter Botnet, which demonstrate the real-life cyber espionage capability of this technique.
Phishing is an online fraud that deceives visitors by impersonating a legitimate website to steal their confidential or personal information. This is a well-known form of cybercrime. With the aim of detecting phishing sites, several phishing site detection techniques have recently been created. However, it fails to achieve the desired goal and has a large number of drawbacks, including low accuracy, long learning curve, and low-power embedded hardware. For covering such drawbacks, this work proposes an efficient URLs Phishing detection technique. Our technique depends on Software Defined Network (SDN) technology, clustering and feature method, and Conventional Neural Network (CNN) algorithm. Feature selection technique is based on Recursive Feature Elimination (RFE) with Support Vector Machine (SVM) algorithm. The SDN is used to transfer the URLs phishing detection process out of the user's hardware to the controller layer, continuously train on new data, and then send its outcomes to the SDN-Switches. RFE-SVM and CNN are used to increase accuracy of phishing detection. Therefore, the proposal model does not require retrieving the content of the target website or using any third-party services. It captures the information and sequential patterns of URL strings without requiring a prior knowledge about phishing, and then uses the sequential pattern features to quickly classify the actual URL. The experimental results showed that our proposal highlighted the robustness and accuracy in distinguishing between phishing and legitimate sites. Our suggestion achieves 99.5% phishing detection accuracy.
The paper observes a method of generating the original crypto currency based on the well-established blockchain technology coupled with a new approach of unrelated third-party payments receipt and double-layer encryption for security reasons. At the first stage, the scholars studied and reviewed blockchain operating principles for using them in the crypto currency being developed. The researchers chose an optimal method of encrypting the transmitted data based on the irredundant SHA-256 algorithm that answered the required encryption complexity. The authors used the Proofof-work method for cybercrime defences and block verification. The next step for the scholars was to develop and test a distributed database for storing information about coins, users, keys, transactions made and being processed. This database includes MySQL database management system, which has an application programming interface API and connectors for numerous programming languages, a library for NET platform languages. Double-layer encryption was added to the resulting system in order to improve data crypto reliability (soundness). Specialized applications for generating user keys and validating operations were actualized. 6000 coins were generated for that purpose. Finally, a graphical web-interface was designed for users' work with coins in PHP scripting programming language, simulations were performed, and all maintained processes were tested. The presented stages allow the developers to introduce their own crypto currencies with various additional features, which can be limited only by their own imagination and technological means, and can be launched on the cryptomarket.
The widespread adoption of data-driven applications in critical infrastructures has arisen with security and privacy concerns. Blockchain has received considerable attention to protect critical infrastructures (e.g., healthcare and transportation) that could be subjected to intentional and unintentional cyberattacks. Blockchain patterns as reusable solutions have been used in critical infrastructure software to fulfill security requirements while delivering reliable and trusted services to citizens. Thus, this work provides a comprehensive review of blockchain patterns to examine how they can steer the advancement of critical infrastructures. Through a critical analysis of existing blockchain pattern literature, we identify realistic limitations, lessons learned and open research issues entirely dedicated to advancing blockchain-based antifragile critical infrastructures.
The world is facing an energy crisis. The smart building concept is presented by many researchers using IoT devices that do not have sufficient computational power to compute the data to decide about the results. According to an estimation, there will be 50 billion IoT devices in 2020. Most IoT devices send data to the cloud for processing. Latency and network usage will be increased in cloud servers because the cloud servers will not be able to handle millions of requests spontaneously. In this paper, we have proposed a framework that minimizes latency and energy consumption in cloud computing. The proposed framework uses edge computing and most of the processing is performed on fog nodes. The framework contributes significantly to energy saving by supporting behavioral and physical changes in the cloud network.
The Smart Grid (SG) is nowadays an essential part of modern society, providing two-way energy flow and smart services between providers and customers. The main drawback is the SG complexity, with an SG composed of multiple layers, with devices and components that have to communicate, integrate, and cooperate as a unified system. Such complexity brings challenges for ensuring proper reliability, resilience, availability, integration, and security of the overall infrastructure. In this paper, we introduce a new smart grid testing management platform (herein called SGTMP) for executing real-time hardware-in-the-loop SG tests and experiments that can simplify the testing process in the context of interconnected SG devices. We discuss the context of usage, the system architecture, the interactive web-based interface, the provided API, and the integration with co-simulations frameworks to provide virtualized environments for testing. Furthermore, we present one main scenario about the stress-testing of SG devices that can showcase the applicability of the platform.
For years, attackers have exploited vulnerabilities in Internet of Things (IoT) devices. Previous research has examined target selection in cybercrime, but there has been little investigation into the factors that influence target selection in attacks on IoT. This study aims to better understand how attackers choose their targets by analyzing the frequency of specific exploits in 11,893 IoT malware binaries that were distributed between 2018-2021. Our findings indicate that 78% of these binary files did not specifically target IoT vulnerabilities but rather scanned the Internet for devices with weak authentication. To understand the usage of exploits in the remaining 2,629 binaries, we develop a theoretical model from relevant literature to examine the impact of four latent variables, i.e. exposure, vulnerability, exploitability, and patchability. We collect indicators to measure these variables and find that they can explain to a significant extent (R-2=0.38) why some vulnerabilities are more frequently exploited than others. The severity of vulnerabilities does not significantly increase the frequency with which they are targeted, while the presence of Proof-of-Concept exploit code does increase it. We also observe that the availability of a patch reduces the frequency of being targeted, yet that more complex patches are associated with higher frequency. In terms of exposure, more widespread device models are more likely to be targeted by exploits. We end with recommendations to disincentivize attackers from targeting vulnerabilities.
While organizations spend millions of dollars on developing security systems at the highest level, one of the most significant areas of weaknesses, and loss remain their employees. Lack of employee training and security expertise, therefore, can cause huge loss, despite other measure being put in place. Cyberattacks are often able to commit cybercrime due to a lack of qualified cyber-security staff and the limited number of IT staff employed to keep pace with continuing security development and advancement. Testing, training and employing staff therefore is a critical measure for all organizations to reduce the vulnerabilities yet seems to be an area still not fully addressed. Businesses and organizations need to provide training to promote understanding for staff at every level, so they are aware of their roles and responsibilities in protecting against security threats. However, this is a colossal undertaking, and until this learning gap is resolved, financial institutions must continue to fight and efficiently manage cybersecurity threats. The aim of the current research paper is to present and propose a semi-automated risk assessment framework and a security maturity model, which can be helpful for auditors, security officers and managers. It is based on the ISO 27001 and utilize the relevant standards as well. The related risk management solution is a web-based software application. The current study targeted information security in Kosovo, specifically in the banking sector, IT industry and insurance field.
Rapid technological improvements have brought significant hazards to sensitive data and information. Cyberspace has connected various data structures, ranging from private communications/transactions to government activities. Cyberattacks are growing more complex which emphasizes the need to improve cybersecurity. Cyber security is more crucial as everything becomes more digital and as the number of connected devices keeps increasing. Cyber security techniques are used to keep networks, applications, and devices safe from intruders. Cloud and IoT technologies have expanded the complexity of computing, communication, and networking infrastructures, making cybercrime prevention more difficult. It takes a long time to develop threat recognition algorithms by the existing methods. Innovative strategies, like employing deep learning tools for cybersecurity, are anticipated to provide a solution to the issue. Deep learning approaches have many benefits which include the ability to solve complex problems quickly, high levels of automation, the best use of informal information, the capacity to generate excellent results at a lower cost, and the ability to recognize complex interactions. A diverse range of applications can be employed in deep learning models to make decisions based on predictions in the daily routine. The significant benefits of deep learning-enabled cyber security have improved security and reduced risks. The intensity of this systematic study provides consolidated knowledge about recent trends and serves as a foundation for future research in Deep learning-enabled Cybersecurity. This paper highlights the potential challenges and current cybersecurity issues with cutting-edge Deep Learning technologies.
Since cyber-attacks are ever-increasing in number, intensity, and variety, a strong need for a global, standardized cyber-security knowledge database has emerged as a means to prevent and fight cybercrime. Attempts already exist in this regard. The Common Vulnerabilities and Exposures (CVE) list documents numerous reported software and hardware vulnerabilities, thus building a community-based dictionary of existing threats. The MITRE ATT&CK Framework describes adversary behavior and offers mitigation strategies for each reported attack pattern. While extremely powerful on their own, the tremendous extra benefit gained when linking these tools cannot be overlooked. This paper introduces a dataset of 1813 CVEs annotated with all corresponding MITRE ATT&CK techniques and proposes models to automatically link a CVE to one or more techniques based on the text description from the CVE metadata. We establish a strong baseline that considers classical machine learning models and state-of-the-art pre-trained BERT-based language models while counteracting the highly imbalanced training set with data augmentation strategies based on the TextAttack framework. We obtain promising results, as the best model achieved an F1-score of 47.84%. In addition, we perform a qualitative analysis that uses Lime explanations to point out limitations and potential inconsistencies in CVE descriptions. Our model plays a critical role in finding kill chain scenarios inside complex infrastructures and enables the prioritization of CVE patching by the threat level. We publicly release our code together with the dataset of annotated CVEs.
A large number of domains are abused every day for cybercrime. At the same time, the fight against abusive domains is not the fight of one person or organization but a battle that requires the cooperation of the entire community. However, very little research has been done to quantify the positive benefits of this strategy for dealing with abusive domains. As a result, using pornography and gambling domain names as examples, we present the first empirical study evaluating the usability and effectiveness of all Internet entities (e.g., registrars and hosting providers) in the DNS ecosystem for receiving and handling abusive domain reports. First, the paper thoroughly demonstrates the mechanisms for receiving and handling abusive domain reports at various Internet entities in China. Second, we select and report the appropriate 2433 abusive domains to 43 service providers across six categories of Internet entities. Finally, we discover the methods and response time used by each Internet entity to handle abuse reports based on the changes in reported domains. Based on the above data, we analyze and evaluate the effectiveness of Internet entities in dealing with abusive domains. Moreover, we indicate the scope of protection and disadvantages of each method, i.e., whether the abusive domain can escape handling. The paper aims to provide a more detailed overview and reference for the security communities, service providers, and Internet entities concerned with dealing with abusive domains.
Steadily growing Internet trade and calculations in electronic money on the Internet create new difficulties of protection of the rights within national legal systems. Despite the legislation is becoming stricter in terms of the procedure of electronic payments and the appropriate license is needed, some payment service providers work illegally. The owners of the exclusive rights to goods delivery and trademarks bear losses due to Internet trade, that allows purchasing goods remotely, that means out of the jurisdiction of the national right of the exclusive distributor. More and more daily functions are shifted from the human who is competent and delictual to robots and artificial intelligence, which don't have any rights and duties in legal understanding. It has significant effect on the content and element of offense. The issue goes to the subject of offense as it is not always possible to establish if it is an undelictual human owing to various factors. More and more offenses have cross-border nature. Working within the national legal system of one state, the companies in parallel violate the rights and the interests of competitors protected by the legislation of another state. There is a necessity to develop international legal system that enables to protect effectively all the participants of the world market. International cooperation within cybercrime is not sufficient as it covers only the most serious crimes in virtual networks, disregarding more serious offenses. (c) 2019 Published by Future Academy www.FutureAcademy.org.UK
One of the tasks Law Enforcement Agencies are responsible for is to find evidence of criminal activities in the Darknet. However, visiting thousands of domains to locate visual information containing illicit acts manually requires a considerable amount of time and human resources. To support this task, in this paper, we explore the automatic classification of images uploaded to Tor darknet. Unfortunately, the foreground objects on such images are not always presented standalone, without background, due to the environmental conditions. To address this challenge on the digital investigation of Tor darknet visual content, we propose to classify automatically only relevant parts of the image combining saliency maps, i.e. to select the regions with the most salient information, with Bag of Visual Words (BoVW). We introduce Semantic Attention Keypoint Filtering (SAKF), a filtering strategy that removes non-significant features at a pixel level that mainly do not belong to the object of interest or foreground. We assessed SAKF on seven publicly available datasets, obtaining from 1.64 to 15.73 points higher accuracies than the method set as the baseline, i.e. BoVW using dense SIFT (Scale-Invariant Feature Transform) descriptors. We also compared SAKF filtering performance against the deep features extracted from two well-known Convolutional Neural Network (CNN) architectures, namely MobileNet and ResNet50. Experimental results reveal the effectiveness of the proposed approach and highlight that the use of automatic image classification could be advantageous to support daily Law Enforcement Agencies investigations on Tor darknet. (C) 2019 Elsevier Ltd. All rights reserved.
Family relationships are the basis for a stable family atmosphere. Now with the internet, which has become a tangible part of our lives, membership of virtual communities, including social networks, has grown. It has created fundamental changes in the structure of social relationships and human interaction, forming a new framework of virtual interactions that transcend international borders. Due to their significance, unavoidability, availability, confidentiality, ease of use, and intrusive nature through interaction and sharing, virtual communities now extend beyond the individual and related level of risk. This study aimed to assess the cultural and behavioral risks of virtual interactions for Saudi families and their level of incidence and likelihood. It belonged to descriptive studies, using the social survey for a sample (1524) from Hail region families between 15 December 2022 and 31 January 2023, and was based on the list of cultural and behavioral risks of virtual interactions. Results showed that the most dangerous social networking sites for Saudi families were TikTok, Twitter, Snapchat, YouTube, and Instagram respectively. Matrix results showed that 66.7% of virtual interaction threats have a high severity and likelihood, namely: adopting atheistic ideas, spreading the thought of hate, undermining the values of the Saudi family, and adopting ideas that incite violence from cultural threats, privacy hacks, cyber-bullying, fraud, violence, social isolation and cybercrime from behavioral threats. The study recommends adopting a social risk management (SRM) concept, especially regarding risks to family, because it is a new dimension of social protection.
Researchers have given a lot of attention to the relationship between religion and crime, finding that religion tends to have a deterring influence on crime-related attitudes and behaviors. This paper first examined the relationship between religiosity and cyberbullying, then ascertained how justice system could prevent cyberbullying behavior, through social and educational initiatives. The data was collected through normative juridical methods to provide a constructive understanding of the perspective of criminal law related to cyberbullying. It was revealed that the constitutional law took cognizance of cyberbullying, more than the religious laws, to curb cyberbullying in Indonesia on the pretext that cybercrimes took place in the cyberspace and not in real public. The findings also revealed an absence of the religious laws, and the increase in the cyber-sectarian conflict in Indonesia, where social media and other websites are indulged in slander and hoax with a view to insult Islamic leaders and their groups based on their religious affiliations, faith or ethnicity. Like the West and other developed nations, Indonesia has also thus witnessed cyber bullying and cybercrime incidents targeting individuals belonging to a particular religion, faith or ideology. The justice institutions and Ministry of Religious Affairs (Kementerian Agama) made several attempts to prevent this but the absence of robust religious laws were felt. The study concluded with recommending to take fresh insight into cyberbullying and its combating legal measures in the Indonesian religious justice system, appropriately aided by social and community initiatives.
Cybersecurity threats, including those involving machine learning, mal -ware, phishing, and cryptocurrency, have become more sophisticated. They target sensitive information and put institutions, governments, and individuals in a con-tinual state of risk. In 2019, phishing attacks became one of the most common and dangerous cyber threats. Such attacks attempt to steal sensitive data, such as login and payment card details, from financial, social, and educational websites. Many universities have suffered data breaches, serving as a prime example of victims of attacks on educational websites. Owing to advances in phishing tactics, strategies, and technologies, the end-user is the main victim of an attack scenario. According to several studies, the end-user can play a significant role in preventing a phishing attack. Therefore, this study was conducted to investigate the levels of user aware-ness regarding cyber threats and explore the relationship between the knowledge on cybercrimes and the awareness of phishing, within the context of cybercrime targeting educational websites. An observational experiment using 'think aloud' method was conducted with 20 students from Taif University. The results indi-cated that although the participants demonstrated an advanced level of informa-tion technology experience as specialists in computer science and computer engineering, their susceptibility to phishing was high. The results of this study will contribute to the cybersecurity research field in terms of proposing risk man-agement plans, delivering embedded training to end-users, and improving spam detecting tools.
In the age of globalization and sophistication of information technology, society is increasingly exposed to current changes and criminal threats such as personal security threats through online applications like Facebook. Therefore, this study discusses the forms of personal security threats through Facebook and its solution in Malaysia. Facebook is one of the most popular communication mediums in Malaysia and has millions of users both domestically and abroad. It is friendly, easy, fast and can share all your daily activities whether in writing, picture or video status. This study aims to highlight the forms of personal security threats that start on Facebook, give public awareness to the Malaysian community while using Facebook and provide recommendations on remedial measures before or after fraud on social media. This study used qualitative methods to obtain data and information from the Royal Malaysian Police, the Malaysian Communications and Multimedia Commission as well as the library study. The study found that Facebook helps people to connect, trading and communicate with individuals and communities. However, it also has an impact on personal safety for individuals, communities and organizations through cybercrime threats, fraud, data theft, exploitation and leakage of personal information, love fraud, slander, cyberbullying, encourage teen problems and cheating couples. This study can help the public to be more cautious when using Facebook, not just provide personal information to strangers and give them exposure to the actions they need to take when they become victims of fraud.
In the current Internet of things era, all companies shifted from paper-based data to the electronic format. Although this shift increased the efficiency of data processing, it has security drawbacks. Healthcare databases are a precious target for attackers because they facilitate identity theft and cybercrime. This paper presents an approach for database damage assessment for healthcare systems. Inspired by the current behavior of COVID-19 infections, our approach views the damage assessment problem the same way. The malicious transactions will be viewed as if they are COVID-19 viruses, taken from infection onward. The challenge of this research is to discover the infected transactions in a minimal time. The proposed parallel algorithm is based on the transaction dependency paradigm, with a time complexity O((M+NQ+N boolean AND 3)/L) (M = total number of transactions under scrutiny, N = number of malicious and affected transactions in the testing list, Q = time for dependency check, and L = number of threads used). The memory complexity of the algorithm is O(N+KL) (N = number of malicious and affected transactions, K = number of transactions in one area handled by one thread, and L = number of threads). Since the damage assessment time is directly proportional to the denial-of-service time, the proposed algorithm provides a minimized execution time. Our algorithm is a novel approach that outperforms other existing algorithms in this domain in terms of both time and memory, working up to four times faster in terms of time and with 120,000 fewer bytes in terms of memory.
In response to internal and external stimuli, the Royal Bhutan Police (RBP) faces the challenges of organizational change. As it continues its transition from military to civilian policing, responds to rapid social transformation in Bhutan, and adjusts to changes in service demands, the RBP must assess and improve its organizational and individual level competencies. This study considers issues associated with this transition in policing approach. Through the analysis of evidence from the Royal Bhutan Police's (RBP) commissioned officers, it specifically considers the agency's police culture and assesses competency areas requiring attention. The study seeks to identify the training needs of all 20 RBP districts in Bhutan. This was accomplished through a survey that targeted all RBP sworn officers. With an 82.9% response rate, the survey reveals that most of the study participants expressed the need for training in investigating cybercrime, bomb threat response, responding to terrorism, investigating accidental fire and arson, disaster management information technology skills, and rescuing and evacuating casualties. It also finds evidence to support an assessment that the RBP force needs training in areas directly related to modifying its culture to adjust to its civilian policing mission. Additional findings include incumbents in the rank of Police Major and above self-rating as more competent than Police Captain and below, and that male officers self-rate as more competent than female officers. The study results support the RBP's efforts to develop more relevant in-service training and may inform professionalizing police forces in other developing countries.
Cybercrime is an online crime committing fraud, stealing identities, violating privacy or hacking the personal information. A high level of information security in banking can be attained through striving to achieve an integrity, confidentiality, availability, assurance, and accountability. This Pandemic situation (COVID-19) paved the way for the customers to avoid traditional ways of banking and adapt to digital transactions. This banking digitalization increases in the utilization of cashless transactions like digital money (Cryptocurrency). Cyber security is imperative to preserve sensitive information, therefore, Blockchain technology has been adapted to provide security. Transactions done via Blockchain are tested through every block, which makes transactions secure and helps the banking system to work faster. The proposed algorithm WFB is used to estimate the average queue rate and avoid unwanted block generation. Then the trapezoidal fuzzy technique optimizes the allocation of blocks. An objective of this investigation is to enhance the security in banking systems from Cybercrimes by verifying Rain Drop Service (RDS) and Fingerprint Biometric without the need of any central authority. Once the service is completed, the service is a dropout and the following new service will be provided (Hence the name RDS). For the strong authentication scheme to fight against bank fraud, RSA encryption technique has been implemented successfully. Therefore, Blockchain technology increases the need for cyber security as a part of design architecture which intends to detect the stemming attacks in real time instead of repairing the damage.
Background: The pricing of illicit drugs is typically approached within the risks and prices framework. Recent sociological and economic studies of prices in online drug markets have stressed the centrality of reputation for price formation. In this paper, we propose an account of price formation that is based on the risks and prices framework, but also incorporates internal social organization to explain price variation. We assess the model empirically, and extend the current empirical literature by including payment methods and informal ranking as influences on drug pricing. Methods: We apply our model to estimate the prices of cannabis, cocaine, and heroin in two online drug markets, cryptomarkets ( n = 92 . 246). Using multilevel linear regression, we assess the influence of product qualities, reputation, payment methods, and informal ranking on price formation. Results: We observe extensive quantity discounts varying across substances and countries, and find premia and discounts associated with product qualities. We find evidence of payment method price adjustment, but contrary to expectation we observe conflicting evidence concerning reputation and status. We assess the robustness of our findings concerning reputation by comparing our model to previous approaches and alternative specifications. Conclusion: We contribute to an emerging economic sociological approach to the study illicit markets by develop-ing an account of price formation that incorporates cybercrime scholarship and the risks and prices framework. We find that prices in online drug markets reflect both external institutional constraint and internal social pro-cesses that reduce uncertainty.
Identifying the software used in a cybercrime can play a key role in establishing the evidence against the perpetrator in the court of law. This can be achieved by various means, one of which is to utilize the RAM contents. RAM comprises vital information about the current state of a system, including its running processes. Accordingly, the memory footprint of a process can be used as evidence about its usage. However, this evidence can be influenced by several factors. This paper evaluates three of these factors. First, it evaluates how the used programming language affects the evidence. Second, it evaluates how the used platform affects the evidence. Finally, it evaluates how the search for this evidence is influenced by the implicitly used encoding scheme. Our results should assist the investigator in its quest to identify the best amount of evidences about the used software based on its execution logic, host platform, language used, and the encoding of its string values. Results show that the amount of digital evidence is highly affected by these factors. For instance, the memory footprint of a Java based software is often more traceable than the footprints of languages such as C++ and C#. Moreover, the memory footprint of a C# program is more visible on Linux than it is on Windows or Mac OS. Hence, often software related values are successfully identified in RAM memory dumps even after the program is stopped.
The era of Internet of Things and 4th Industrial Revolution generate digital generation who has different life style. No doubt that the digital technologies become an important and centre of the generation life. The technologies change the way they play, communicate, work, and study. As a result the technologies force educational institutions, from primary school up to university, to change their learning system. Therefore many institutions have introduced and implemented variety of technology based learning, such as open learning, social learning, flipped learning, Massive Open Online Courses, etc. Accordingly, the institutions have to provide those technologies for all students, teachers, staffs, and their stakeholders for professional purposes, i.e. teaching and learning, and personal purposes, i.e. communicating, creating and socialising. Thus it is challenging the traditional concept of learning. The institutions fully understand numerous benefits of using digital technology for both professional and personal purposes. The technology creates the learning environment that everyone can teach anyone, more personal learning, and create more creativity and innovation beyond the limits of our imagination. On the other hand the institutions should conscious disadvantages of using the technology without knowing digital literacies. Digital literacies are key skill to develop to actively managing users' online presence. Teachers, students, and all staffs must aware to safe use of digital technologies. The institutions have a responsibility to educate cyber safety to people responsible behaviour when using digital technologies to prevent or to minimize cyberbullying, cybercrime, and plagiarism.
A novel concept of a metric hull has recently been introduced to encompass a set of objects by a few selected border objects. Following one of the metric-hull computation methods that generate a hierarchy of metric hulls, we introduce a metric index structure for unstructured and complex data, a Metric Hull Tree (MH-tree). We propose a construction of MH-tree by a bulk-loading procedure and outline an insert operation. With respect to the design of the tree, we provide an implementation of an approximate kNN search operation. Finally, we utilized the Profimedia dataset to evaluate various building and ranking strategies of MH-tree and compared the results with M-tree.
"With the advent of information and communication technology, the digital space is becoming a playing ground for criminal activities. Criminals typically prefer darkness or a hidden place to perform their illegal activities in a real-world while sometimes covering their face to avoid being exposed and getting caught. The same applies in a digital world where criminals prefer features which provide anonymity or hidden features to perform illegal activities. It is from this spirit the Darkweb is attracting all kinds of criminal activities conducted over the Internet such as selling drugs, illegal weapons, child pornography, assassination for hire, hackers for hire, and selling of malicious exploits, to mention a few. Although the anonymity offered by Darkweb can be exploited as a tool to arrest criminals involved in cybercrime, an in-depth research is needed to advance criminal investigation on Darkweb. Analysis of illegal activities conducted in Darkweb is in its infancy and faces several challenges like lack of standard operating procedures. This study proposes progressive standard operating procedures (SOPs) for Darkweb forensics investigation. We provide the four stages of SOP for Darkweb investigation. The proposed SOP consists of the following stages; identification and profiling, discovery, acquisition and preservation, and the last stage is analysis and reporting. In each stage, we consider the objectives, tools and expected results of that particular stage. Careful consideration of this SOP revealed promising results in the Darkweb investigation."
Processing large volumes of various data requires index structures that can efficiently organize them on secondary memory. Methods based on pivot permutations have become popular because of their tremendous querying performance. Pivot permutations can be perceived as a recursive Voronoi tessellation with a fixed set of anchors. Its disadvantage is that it cannot adapt to the data distribution well, which leads to cells unbalanced in occupation and unevenly filled disk buckets. In this paper, we address this issue and propose a novel schema called the BM-index. It exploits a weighted Voronoi partitioning, which is able to respect the data distribution. We present an algorithm to balance the data partitions, and show its correctness. The secondary memory is then accessed efficiently, which is shown in experiments executing k-nearest neighbors queries on a real-life image collection CoPhIR.
"The article explores the relationship between screen time and physical and mental health of young people, such as visual impairment, sleeplessness, eating disorders, as well as Internet addiction, loneliness, and depression.The article contains the results of the survey, conducted in November-December 2019 in Russia (Saratov region) among university students aged 18-22 (n=386) to identify the connection between virtual communication in social networks and the youth's mental health. The main research method was a survey. The questionnaire consisted of 25 questions, which were divided into four blocks: 1) the amount of screen time daily; 2) subjective attitude towards virtual friendship and communication; 3) characteristics of individuals, communicating with online; 4) the attitude to online communication as a way to overcome loneliness. As a result, on-line activity of young people does not lead to the development of sociability, building real friendships or romantic relationships, it takes a lot of time, enhancing the feeling of loneliness and provoking the emergence of depression. The author highlights the problem areas that impede the effective solution of these problems: the ambiguity of the experts' opinions regarding the impact of digital technology on the development of modern teenagers and youth; the conflict of interests between providers, business structures, advertisers and minor users; poor Internet literacy of parents and educators compared with modern teenagers and youth that hinders the development of adequate technologies for early detection, prevention and providing assistance to minors who could become real or potential victims of cybercrime."
This paper focuses on the strengths and weaknesses of in Jordanian E-banking sector. It also discusses the local awareness and familiarization of E-banking in Small to Midsized Enterprises-SMEs of Jordan and gives a consistent assessment of Jordan's current and future E-banking best practices. This paper initially has used secondary data to provide a study of F-banking in Jordan, how to overcome the hurdles in SMEs of Jordan necessary for SMEs to help facilitate E-banking adoption. The research is subject to top academic journal articles, corporate project data and reports, media articles, government and non government corporation-based documents and other appropriate information. Data were also collected by using interviews from Jordan E-business based organizations those are offering goods and services on electronic channels and professionals involved with E-banking related activities. The study found that F-banking is steadily transforming the way businesses to be conducted and changing the business environment in Jordan. E-banking can provide speedier, faster and reliable services to the customers for which they are relatively happy. E-banking services not only can develop new competitive advantages, it can improve its relationships with customers. As a developing country, Jordan is not fully known about E-banking sector. As a result, this paper also overviews the issues associated with E-banking e.g. cybercrime and try to explore the future challenges and prospects in Jordan. This paper also compares the local E-business sites with worldwide brand E-business sites to make an effective solution of Jordan's E-banking.
As countries are embracing the economic and social potential of the Internet of Everything (IoE)-the intelligent connection of people, processes, data, and things, the world is seeing the cyberspace as a new battleground Along these lines, the information age security policy, strategy, and management face exceptional challenges. Traditional strategies and approaches to security need revision to apply to a future where threats can propagate instantaneously and where the identity or location of an adversary may not be known. Is Africa prepared, preparing, or at risk? Using the Cyber Readiness Index the paper also assesses the future risks the top 12 emerging economies in Africa that have embraced ICT and the Internet may face and compares their maturity and commitment to protecting those investments using an initial objective assessment of where each country stands in cyber security across five areas including Articulation and publication of a National Cyber Security Strategy, availability of operational Computer Emergency Response Team (CERT) or Computer Security Incident Response Team (CSIRT) demonstration of commitment to protect against cybercrime, availability of information sharing mechanism, investment in cyber security basic and applied research and funding cyber security initiatives broadly. This differs from previous studies that have focused only on the cyber readiness of the North Pole. Findings raise fundamental questions and concerns for the African Union (AU) on fostering partnerships between African governments to promote better information sharing by further developing collaboration tools to heighten cyber resilience against pre-empted potential cyber war and terrorism especially between or among regional economic powers
Monitoring plays a crucial role in the operation of any sizeable distributed IT infrastructure. Whether it is a university network or cloud datacenter, monitoring information is continuously used in a wide spectrum of ways ranging from mission-critical jobs, e.g. accounting or incident handling, to equally important development-related tasks, e.g. debugging or fault-detection. Whilst pursuing a novel vision of new-generation event-driven monitoring systems, we have identified that a particularly rich source of monitoring information, computer logs, is also one of the most problematic in terms of automated processing. Log data are predominantly generated in an adhoc manner using a variety of incompatible formats with the most important pieces of information, i.e. log messages, in the form of unstructured strings. This clashes with our long-term goal of designing a system enabling its users to transparently define real-time continuous queries over homogeneous streams of properly defined monitoring event objects with explicitly described structure. Our goal is to bridge this gap by normalizing the poorly structured log data into streams of structured event objects. The combined challenge of this goal is structuring the log data, whilst considering the high velocity with which they are generated in modern IT infrastructures. This paper summarizes the contributions of a dissertation thesis, Normalization of Unstructured Log Data into Streams of Structured Event Objects dealing with the matter at hand in detail.
The number of malware infected machines from all over the world has been growing day by day. New malware variants appear in the wild to evade the malware detection and classification systems and may infect with ransomware or crypto miners for adversary financial gain. A recent colonial pipeline ransomware attack is an example of these attacks that impacted daily human activities, and the victim had to pay ransom to restore their operations. Windows-based systems are the most adopted systems across different industries for running applications. They are prone to get targeted by installing the malware. In this paper, we propose a Deep Learning (DL)-based Convolutional Neural Network (CNN) model to perform the malware classification on Portable Executable (PE) binary files using the fusion feature set approach. We present an extensive performance evaluation of various DL model architecture and Machine Learning (ML) classifier i.e. Support Vector Machine (SVM), on multi-aspect feature sets covering the static, dynamic, and image features to select the proposed CNN model. We further leverage the CNN-based architecture for effective classification of the malware using different combinations of feature sets and compare the results with the best-performed individual feature set. Our performance evaluation of the proposed model shows that the model classifies the malware or benign files with an accuracy of 97% when using fusion feature sets. The proposed model is robust and generalizable and showed similar performances on completely unseen two malware datasets. In addition, the embedding features of the CNN model are visualized, and various visualization methods are employed to understand the characteristics of the datasets. Further, large-scale learning and stacked classifiers were employed after the penultimate layer to enhance the CNN classification performance.
"A comprehensive taxonomy of fraud is presented based on morphological analysis, attribute listing and matrix analysis. Fraud matrix and tree classification frameworks are presented and discussed. They are then utilized to classify and explain a number of the different types of frauds, shown in the fraud matrix classification framework. First, triangular attributes of fraud are formulated, followed by fraud channels and elementary fraud features. Several well-known fraud types are identified using the proposed fraud classification framework. Further, new fraud types are discovered using the framework, for example, transactional frauds, automated frauds, synchronized fraud, unwitting accomplice, and 'Robin Hood' fraud. The importance of the taxonomy is that it can be used to classify both existing and newly identified fraud types in a way and manner that has not been previously reported; that is, it offers understanding to the different classes of frauds, the inherent threat actors behind such frauds, their capabilities, intent and the resulting nature of the frauds. This taxonomy has potential to offer insight in how appropriate countermeasures to mitigating the different types of frauds could be formulated. (C) 2020 Elsevier Ltd. All rights reserved."
Log message abstraction is a common way of dealing with the unstructured nature of log data. It refers to the separation of static and dynamic part of the log message, so that both parts can be accessed independently, allowing the message to be abstracted into a more structured representation. To facilitate this task, so-called message types and the corresponding matching patterns must be first discovered, and only after that can be this pattern-set used to pattern-match individual log messages in order to extract dynamic information and impose some structure on them. Because the manual discovery of message types is a tiresome and error-prone process, we have focused our research on data mining algorithms that are able to discover message types in already generated log data. Since we have identified several deficiencies of the existing algorithms, which are limiting their capabilities, we propose a novel algorithm for message type discovery addressing these deficiencies.
"Recent years have seen rapid increases in cybercrime. The use of effective software security activities plays an important part in preventing the harm involved. Objective research on industry use of software security practices is needed to help development teams, academic researchers, and educators to focus their activities. Since 2008, a team of researchers, including two of the authors, has been gathering objective data on the use of 121 software security activities. The Building Security In Maturity Model (BSIMM) study explores the activity use of 675,000 software developers, in companies including some of the world's largest and most security-focused. Our analysis of the study data shows little consistent growth in security activity adoption industry-wide until 2015. Since then, the data shows a strong increasing trend, along with the adoption of new activities to support cloud-based deployment, an emphasis on component security, and a reduction in security professionals' policing role. Exploring patterns of adoption, activities related to detecting and responding to vulnerabilities are adopted marginally earlier than activities related to preventing vulnerabilities; and activities related to particular job roles tend to be used together. We also found that 12 developer security activities are adopted early, together, and notably more often than any others. From these results, we offer recommendations for software and security engineers, and corresponding education and research suggestions for academia. These recommendations offer a strong contribution to improving security in development teams in the future."
The study of discursive understandings of cybervictimisation draws on a dataset of crime news reporting and asks the question of if and how cybervictimisation is construed in ways that differ from other types of (non-digital) victimisation. Building on a critical discourse perspective employing corpus-based text analysis methods, the composition of news discourses about cybervictimisation are analysed, alongside the relationship between such representations and news media discourse on crime victimisation generally. The aim is to see what effect the presence of a digital dimension has for how the notion of victimisation is socially and culturally understood. The study shows, first, that news reporting on cybervictimisation has a strong bias towards crimes that fit well with the notion of the ideal victim' (such as sexual victimisation and bullying) while excluding other types like hacking and identity theft. The question is raised whether victim' discourse is able to account for the latter types or if new understandings and concepts will emerge. Second, the study shows that discourses promoting understandings of technology as contributing to amplifying danger, and that represent technology as potentially undermining social order, are strong in cybervictimisation news reports. These discourses are consequential for who is seen as a legitimate victim and not. Just as it can be very difficult to identify and apprehend perpetrators of cybercrime, so is also the identification and definition of cybervictims ambiguous and demands to be further researched.
Of all the types of cybercrime that exist, hackers are the cybercriminals who have probably engaged both the imagination of the general public and the interest of the entertainment industry the most. They are also those who have elicited the greatest quantity of psychological academic literature. It seems that we have an unsatisfied desire to comprehend why any individual would be drawn to this type of activity, which seems in some cases to have little immediate benefit for the cybercriminal. This chapter aims to determine if we have discovered all that we need to about the psychology and motivations of hackers. Despite the vast quantities of literature in this area, it seems that we still do not have a thorough grasp on the mentality of the hacker. The chapter will commence with some background information regarding the methods used by hackers, a description of the history of hacking behaviour and terminology, and the legal dimensions of hacking. Following this, the chapter will consider the very diverse motives of hackers, as determined by psychological and criminological research. The personalities of computer hackers will then be examined, with special consideration of how psychological profiling could be used to help in solving hacking cases. Issues regarding punishment and prevention of hacking attacks will then be examined, and finally the difficulties in carrying out hacker research and potential directions for future research in this area will be explored.
Concern about national security has increased significantly since the terrorist attacks on 11 September 2001. The CIA, FBI, and other federal agencies are actively collecting domestic and foreign intelligence to prevent future attacks. These efforts have in turn motivated local authorities to more closely monitor criminal activities in their own jurisdictions. A major challenge facing all law-enforcement and intelligence-gathering organizations is accurately and efficiently analyzing the growing volumes of crime data. For example, complex conspiracies are often difficult to unravel because information on suspects can be geographically diffuse and span long periods of time. Detecting cybercrime can likewise be difficult because busy network traffic and frequent online transactions generate large amounts of data, only a small portion of which relates to illegal activities. Data mining is a powerful tool that enables criminal investigators who may lack extensive training as data analysts to explore large databases quickly and efficiently.(1) Computers can process thousands of instructions in seconds, saving precious time. In addition, installing and running software often costs less than hiring and training personnel. Computers are also less prone to errors than human investigators, especially those who work long hours. We present a general framework for crime data mining that draws on experience gained with the Coplink project (http://ai.bpa.arizona.edu/coplink), which researchers at the University of Arizona have been conducting in collaboration with the Tucson and Phoenix police departments since 1997.
Smartphones have become increasingly popular and nowadays with the using of 3G networks, the needs in terms of connectivity in a business environment are substantial. Malicious use of such devices is highly dangerous since users may be victims of such use. In this paper, we present two statistical methods (Minimum Covariance Determinant (MCD) and Minimum Volume Ellipsoid (MVE) used to detect abnormal smartphone's applications. Initial experiments results prove the efficiency and the accuracy of the MVE and MCD in detecting abnormal smartphone's applications.
Recent advances in deep learning have led to significant improvements in single image super-resolution (SR) research. However, due to the amplification of noise during the upsampling steps, state-of-the-art methods often fail at reconstructing high-resolution images from noisy versions of their low-resolution counterparts. However, this is especially important for images from unknown cameras with unseen types of image degradation. In this work, we propose to jointly perform denoising and super-resolution. To this end, we investigate two architectural designs: in-network combines both tasks at feature level, while pre-network first performs denoising and then super-resolution. Our experiments show that both variants have specific advantages: The in-network design obtains the strongest results when the type of image corruption is aligned in the training and testing dataset, for any choice of denoiser. The pre-network design exhibits superior performance on unseen types of image corruption, which is a pathological failure case of existing super-resolution models. We hope that these findings help to enable super-resolution also in less constrained scenarios where source camera or imaging conditions are not well controlled. Source code and pretrained models are available at https://github.com/angelvillar96/super-resolution-noisy-images.
"The Domain Name System (DNS) is a critical infrastructure of any network, and, not surprisingly a common target of cybercrime. There are numerous works that analyse higher level DNS traffic to detect anomalies in the DNS or any other network service. By contrast, few efforts have been made to study and protect the recursive DNS level. In this paper, we introduce a novel abstraction of the recursive DNS traffic to detect a flooding attack, a kind of Distributed Denial of Service (DDoS). The crux of our abstraction lies on a simple observation: Recursive DNS queries, from IP addresses to domain names, form social groups; hence, a DDoS attack should result in drastic changes on DNS social structure. We have built an anomaly-based detection mechanism, which, given a time window of DNS usage, makes use of features that attempt to capture the DNS social structure, including a heuristic that estimates group composition. Our detection mechanism has been successfully validated (in a simulated and controlled setting) and with it the suitability of our abstraction to detect flooding attacks. To the best of our knowledge, this is the first time that work is successful in using this abstraction to detect these kinds of attacks at the recursive level. Before concluding the paper, we motivate further research directions considering this new abstraction, so we have designed and tested two additional experiments which exhibit promising results to detect other types of anomalies in recursive DNS servers."
Recently, the rapid popularity of smart phones significantly impacts the day-to-day activities of humans due to the increased access to social media and instant messaging services. The misuse of social media services leads to different cybercrime activities. Both the mobile device and cloud are the most significant evidential sources for the social network application forensics. It is essential to establish a mobile cloud forensic process to deal with the crime activities performed by the intelligent mobile user. Also, improving the cloud event traceability is a challenging task in a massive cloud environment. This work presents a mobile cloud forensic process that incorporates the time synchronization and inter and intra-application analysis along with the traditional forensic procedure. Time synchronization is a prerequisite process that facilitates the investigator to perform the application forensics in the mobile cloud concisely. An inter and intra-application analysis process ensures the extraction of the forensic-rich evidence and enriches the performance of the cloud event traceability by utilizing the metadata of potential mobile evidence. The proposed forensic process maintains a chronological timeline of evidence before correlating the potential evidence of the mobile and cloud. Eventually, it submits the extracted evidence with a chain of custody information to validate the forensic investigation process. It significantly enhances the accuracy of the investigation and produces evidence by applying the step by step process in the Android WeChat social network application. (C) 2020 The Authors. Published by Elsevier B.V.
"In recent years the number of cyber-attacks has dramatically increased, affecting military, government, business and home users. For example, the UK Ministry of Defence claims to have blocked and investigated over 1000 serious cyber-attacks in 2010 while in 2011 Detica reported that the cost of cybercrime in the UK is estimated to be 27 pound billion per annum. In cyber-attacks numerous methods exist that can be used to discover information about the attacking entity, otherwise termed as attribution. Attribution is a desired quality to counter a variety of attackers. Cyber-crime attribution can aid police investigations in identifying cyber criminals. In cyber warfare and conflict an attribution capability is desired to enhance decision making of Computer Network Operations (CNO). Attribution of terrorist cyber-attacks may help to prevent future attacks. Highly publicised attacks such as Stuxnet and Night Dragon have been subject to intense analysis, yet published attribution of these attacks has been minimal. The complexity of reliable attribution is increased by an attacker's ability to route attacks through compromised systems, anonymised networks, proxy servers and various jurisdictional boundaries. There are numerous technical attribution techniques ranging from traceback, malware inspection and honeypot deployment. In this paper we present a taxonomy to classify these techniques, using five different classes: acquired attributes, proposed/in use, external party involvement, sabotage opportunity and prepositioning depth. The novelty of this paper is its scope; classifying the landscape of technical attribution techniques."
"The COVID-19 brought several new challenges for the students and lecturers, particularly in higher education. The conventional physical classroom was almost instantly pushed into a new environment: a remote session class. And this lecturing paradigm shift occurred, necessarily, in a short period of time. The lecturing players suffered the pressure of using different learning tools and methodologies while exposing themselves to, in most part of cases, unexpected cybersecurity threats. The purpose of this article is to clarify all the new menaces that arose during this period. Also, to understand the level of exposure and impacts on the lecturer and student to this quick but necessary paradigm shift. Methodologically: two focus groups were undertook to capture representations of students about uses of computer technology; differences in exposure to threats before and during the pandemic and impacts on society; student's needs regarding the topic and suggestions about strategies for increasing overall knowledge about it. Answers were treated recurring to software webQDA (R)-Qualitative Data Analysis Software for analysis of qualitative data. Results depict the exposure to cybersecurity issues and some level of knowledge about them, inducing preventive practices when using informatics technology. Sense of increasing insecurity in society is associated with the statement of increasing activity regarding cybercrime, giving way to the assumption of the need of awareness and knowledge of general public regarding this issues, and rise suggestions on teaching approaches directed to the subject."
With the rapid development of science and technology pushing people into the information age, the appearance of information technology is a major change for the accounting industry, which will redefine the traditional accounting profession. The new management techniques and concepts also gradually change the traditional accounting working mode by promoting the development of accounting professionals from traditional accounting to accounting information. In recent years, cybercrime and attacking information system vulnerability have occurred frequently. This study utilizes the model and technology in the design of an accounting information management system by simply explaining the idea of cloud technology and examining its logical structure of cloud technology. After that, it designs the cloud platform architecture of the accounting information management system (AIMS) by building the SaaS model. It defines the distributed storage mode of the cloud platform and tests the cluster performance of the system after completing the system design and construction to judge the application effect of the system. Finally, the system operation time, local rows of data, and load balancing are tested experimentally. These results demonstrate that the response relationships of distinct tasks are modified higher than TeraSort, TeraSort higher than Inquiry, and response time is proportional to the amount of Reduce slots when input tasks are the same. The analysis shows that the large data cloud platform used in this paper has high operational efficiency, acceleration rate, and task execution rate.
Countless cybercrime instances have shown the need for detecting and blocking obscene material from social media sites. Deep learning methods (DLMs) outperformed in recognizing obscene content flooded on many online platforms. However, these contemporary DLMs primarily treat the recognition of obscene content as a simple task of binary classification, rather than focusing on the labelling of obscene areas. Hence, many of these methods could not pay attention to the fact that misclassification samples are so diverse. Therefore, this paper focuses on two aspects (i) developing a deep learning model that could classify and label the obscene portion, and (ii) generating a labelled obscene image dataset with a wide variety of obscene samples to minimize the risks of inaccurate recognition. We have proposed a method named S3Pooling based bottleneck attention module (BAM) embedded MobileNetV2-YOLOv3 (SBMYv3) for automatic detection of obscene content using an attention mechanism and a suitable pooling strategy. The key contributions of our article are: (i) generation of a well-labelled obscene image dataset with a variety of augmentation strategies using Pix-2-Pix GAN (ii) modifications to the backend architecture of YOLOv3 using MobileNetV2 and BAM to ensure focused and accurate feature extraction, and (iii) selection of an optimal pooling strategy, that is, S3Pooling strategy, while taking the design of the feature extractor into account. The proposed SBMYv3 model outperformed other state-of-the-art models with 99.26% testing accuracy, 99.39% recall, 99.13% precision, and 99.13% IoU values respectively.
The number of fraud occurrences in electronic banking is rising each year. Experts in the field of cybercrime are continuously monitoring and verifying network infrastructure and transaction systems. Dedicated threat response teams (CSIRTs) are used by organizations to ensure security and stop cyber attacks. Financial institutions are well aware of this and have increased funding for CSIRTs and antifraud software. If the company has a rule-based antifraud system, the CSIRT can examine fraud cases and create rules to counter the threat. If not, they can attempt to analyze Internet traffic down to the packet level and look for anomalies before adding network rules to proxy or firewall servers to mitigate the threat. However, this does not always solve the issues, because transactions occasionally receive a gray rating. Nevertheless, the bank is unable to approve every gray transaction because the number of call center employees is insufficient to make this possible. In this study, we designed a machine-learning-based rating system that provides early warnings against financial fraud. We present the system architecture together with the new ML-based scoring extension, which examines customer logins from the banking transaction system. The suggested method enhances the organization's rule-based fraud prevention system. Because they occur immediately after the client identification and authorization process, the system can quickly identify gray operations. The suggested method reduces the amount of successful fraud and improves call center queue administration.
The use of cyber weapons raises many issues, one of which is the scope of legal requirements affecting the legal review of cyber weapons under Additional Protocol I and customary international law. This paper explores the review of cyber weapons intended for use below the threshold of armed conflict As the line between war and peace is often increasingly blurred and the majority of cyber incidents are below the threshold of armed conflict, the laws and principles of international humanitarian law do not apply. In this paper, we engage in a scenariobased thought experiment exploring the legal framework affecting the use of cyber weapons outside armed conflict. In such situations, the well-known article 36 of Additional Protocol I and customary international law are not triggered. As a result, there is no explicit legal obligation to conduct a cyber weapons review in situations when cyber weapons are deployed in situations falling below the threshold of armed conflict. Our starting point is that even though international humanitarian law is not applicable, the use of cyber weapons is not completely unregulated. In the paper, we search for answer to following research question: what are the legal requirements for weapons review in situations where their intended use is for situations below the threshold of armed conflict? We identify the black-letter legal framework and explore the state practice of NATO member states where available. The paper argues that there are many obligations to be considered when deploying cyber weapons in situations below the threshold of armed conflict. The conclusion is that there is no obligation to conduct a review outside Article 36 of Additional Protocol I. That being said, there are definitely policy benefits in conducting broader software assessment to ensure respect to international law obligations of a state.
As an increasing amount of crime takes on a digital aspect, law enforcement bodies must tackle an online environment generating huge volumes of data. With manual inspections becoming increasingly infeasible, law enforcement bodies are optimising online investigations through data-mining technologies. Such technologies must be well designed and rigorously grounded, yet no survey of the online data-mining literature exists which examines their techniques, applications and rigour. This article remedies this gap through a systematic mapping study describing online data-mining literature which visibly targets law enforcement applications, using evidence-based practices in survey making to produce a replicable analysis which can be methodologically examined for deficiencies.
"Most people are exposed to risks both in the online and offline world. Several studies have provided definitions and measures of cybervictimization based on different theoretical approaches and most of them have focused on specific forms of cybercrime, depicting a limited portrayal of victimization. The current study explored victimization configurations in a sample of 749 university undergraduates from Spain (61.6% women; M age = 26.9), utilizing latent class analyses to account for the nature and frequency of various types of online and offline victimization along their life span. Among them, 35.9% were victims of a cyberattack, 24.4% reported being victims of cyberfraud and 49% of property crime. The analysis uncovered two classes of cybervictims-consisting of economic cybervictimization (victims of economic cybercrimes only) and cyber-polyvictimization (victims of various types of cybercrimes)-and allowed us to compare them with a group of non-victims. Younger respondents (15 to 25 years old), conventional university students, women, people with lower incomes and LGBTQI+ individuals have a higher representation in the cyber-polyvictimization class. In addition, members of this class have suffered more offline victimization in all the areas analyzed. The present study has found co-occurrence between online and offline victimization, thus reinforcing the relevance of simultaneously studying both areas and the interaction between them. From this empirical ground, prevention strategies should not be focused merely on opportunity factors related to the online interactions and behavior of potential victims, without facing the deep human and social roots of victimization."
Synthetic biology has the potential to positively transform society in many application areas, including medicine. In common with all revolutionary new technologies, synthetic biology can also enable crime. Like cybercrime, that emerged following the advent of the internet, biocrime can have a significant effect on society, but may also impact on peoples' health. For example, the scale of harm caused by the SARS-CoV-2 pandemic illustrates the potential impact of future biocrime and highlights the need for prevention strategies. Systematic evidence quantifying the crime opportunities posed by synthetic biology has to date been very limited. Here, we systematically reviewed forms of crime that could be facilitated by synthetic biology with a view to informing their prevention. A total of 794 articles from four databases were extracted and a three-step screening phase resulted in 15 studies that met our threshold criterion for thematic synthesis. Within those studies, 13 exploits were identified. Of these, 46% were dependent on technologies characteristic of synthetic biology. Eight potential crime types emerged from the studies: bio-discrimination, cyber-biocrime, bio-malware, biohacking, at-home drug manufacturing, illegal gene editing, genetic blackmail, and neuro-hacking. 14 offender types were identified. For the most commonly identified offenders (>3 mentions) 40% were outsider threats. These observations suggest that synthetic biology presents substantial new offending opportunities. Moreover, that more effective engagement, such as ethical hacking, is needed now to prevent a crime harvest from developing in the future. A framework to address the synthetic biology crime landscape is proposed.
The surge in cybercrime has emerged as a pressing concern in contemporary society due to its far-reaching financial, social, and psychological repercussions on individuals. Beyond inflicting monetary losses, cyber-attacks exert adverse effects on the social fabric and psychological well-being of the affected individuals. In order to mitigate the deleterious consequences of cyber threats, adoption of an intelligent agent-based solution to enhance the speed and comprehensiveness of cyber intelligence is advocated. In this paper, a novel cyber intelligence solution is proposed, employing four semantic agents that interact autonomously to acquire crucial cyber intelligence pertaining to any given country. The solution leverages a combination of techniques, including a convolutional neural network (CNN), sentiment analysis, exponential smoothing, latent Dirichlet allocation (LDA), term frequency-inverse document frequency (TF-IDF), Porter stemming, and others, to analyse data from both social media and web sources. The proposed method underwent evaluation from 13 October 2022 to 6 April 2023, utilizing a dataset comprising 37,386 tweets generated by 30,706 users across 54 languages. To address non-English content, a total of 8199 HTTP requests were made to facilitate translation. Additionally, the system processed 238,220 cyber threat data from the web. Within a remarkably brief duration of 6 s, the system autonomously generated a comprehensive cyber intelligence report encompassing 7 critical dimensions of cyber intelligence for countries such as Russia, Ukraine, China, Iran, India, and Australia.
Objective: Our proposed study investigates the impact of cybersecurity awareness among children in terms of protecting them in cyberspace. The prevalence of cybercrime among children at the global and national levels imposes serious concerns. Cybercrimes penetrated largely during the digital transformation of education that facilitated children to access cyberspace unconditionally. Methodology: The research is based on reviewing the academic literature relevant to cybersecurity awareness's and its impact on protecting children in cyberspace. Main Findings: The findings highlight the significant role of cybersecurity awareness in reducing the impact of cyberattacks against children. Awareness of cybersecurity and the protection of children in cyberspace are interlinked with each other. Comprehensive cybersecurity awareness programs for children can significantly reduce the spread. Widespread awareness programs in the educational curriculum while utilizing all the required safety criteria to guarantee that the children are Trustworthy, secure, and ensuring their data safety is crucial step to consider. Social Implications: The article highlights the cybersecurity awareness measures that should be considered for children to provide them safe cyberspace platforms. Secure cyberspace for children to get education during the global spread of the Corona pandemic is the dire current social need. Originality/Novelty of the Study: The global spread of the Coronavirus has considerably affected people's daily lives. Due to increased dependability on online platforms during pandemic, we can name this situation as cyber-pandemic. Data breaches and cyberattacks rises dramatically due to a lack of cybersecurity awareness among the citizens.
Despite the efforts of information security experts, cybercrimes are still emerging at an alarming rate. Among the tools used by cybercriminals, malicious domains are indispensable and harm from the Internet has become a global problem. Malicious domains play an important role from SPAM and Cross-Site Scripting (XSS) threats to Botnet and Advanced Persistent Threat (APT) attacks at large scales. To ensure there is not a single point of failure or to prevent their detection and blocking, malware authors have employed domain generation algorithms (DGAs) and domain-flux techniques to generate a large number of domain names for malicious servers. As a result, malicious servers are difficult to detect and remove. Furthermore, the clues of cybercrime are stored in network traffic logs, but analyzing long-term big network traffic data is a challenge. To adapt the technology of cybercrimes and automatically detect unknown malicious threats, we previously proposed a system called MD-Miner. To improve its efficiency and accuracy, we propose the MD-Miner(P) here, which generates more features with identification capabilities in the feature extraction stage. Moreover, MD-Miner(P) adapts interaction profiling bipartite graphs instead of annotated bipartite graphs. The experimental results show that MD-Miner(P) has better area under curve (AUC) results and found new malicious domains that could not be recognized by other threat intelligence systems. The MD-Miner(P) exhibits both scalability and applicability, which has been experimentally validated on actual enterprise network traffic.
Brand search results are poisoned by fake ecommerce websites that infringe on the trademark rights of legitimate holders. In this article, we study how to tackle and measure this problem automatically. We present a pipeline with two machine learning stages that can detect the ecommerce websites present in the list of brand search results and distinguish between legitimate and fake econunerce websites. For each classification task, we identify and extract suitable learning features and study their relative importance. Through a prototype system termed RI.SI.CO., we show that this approach is feasible, fast, and more accurate than both existing systems for trustworthiness assessment and non-expert humans. We next introduce two complementary metrics for evaluating the counterfeit incidence in brand search results: namely, a chart-based and a single-value measure. They allow us to analyze and compare counterfeit at various levels, including single brands within a specific sector as well as whole sectors. Experimenting with two luxury goods sectors, we report a number of interesting findings about how the main search parameters (e.g., search engine, query type, number of search results seen) affect counterfeiting and how this activity changes with time. On the whole, our research offers new insights and some very practical and useful means of analyzing and measuring counterfeit in brand search results, thus increasing awareness of and knowledge about this phenomenon and enabling targeted anti-counterfeiting actions.
"The research is aimed at substantiating the need to use a qualifying element with the use of information and telecommunication networks in the Criminal Code of the Russian Federation to strengthen criminal responsibility for certain crimes related to the dissemination of prohibited information, and at formulating proposals for optimizing the criminal law protection of information security. The methodology deals with implementing knowledge from various branches of science, including information theory and jurisprudence. Traditional methods of jurisprudence are utilized simultaneously in an active manner. The results: The investigation underlines the significance of the criminal-law-related protection of the information space against the dissemination of information prohibited; reveals the possibilities of defining crimes as aggravated;-analyzes the cases of the use of the qualifying element committing a crime with the use of information and telecommunication networks;-checks the article on the equivalence of influence on the level of public danger of the crimes of the given element and the element committing a crime by means of the media;-confirms that the heterogeneity of the resources of the Internet or other networks cannot serve as an argument for denying such equivalence;-emphasizes the need to take into account such properties of these technologies as accessibility for the mass audience, relative anonymity of the user, remote functioning and transboundary nature in assessing the public danger of crimes committed with the use of information and telecommunication networks;-proves the need to assign the status of a qualifying element to the use of information and telecommunications networks in relation to such crimes as slander, public calls for the outbreak of an aggressive war and the rehabilitation of Nazism;-reveals technical and legal deficiencies of the criminal law description of the qualifying element being analyzed; and-determines the ways of their elimination. Methodology The methodology of the research involves knowledge from various branches of science including information theory and jurisprudence. At the same time, traditional methods of jurisprudence including the structured system analysis of criminal law, formal and logical proofs, etc. are utilized in an active manner. The conclusion draws the following inferences: (1) findings on the expediency of using the qualifying element being analyzed both to reflect one of the typical variants of the public way of committing a crime or the demonstrative nature of the latter and to take into account the exceptional characteristics of the distribution of prohibited information by means of information and telecommunication technologies; (2) recommendations to eliminate the shortcomings of criminal legislation related to the consolidation of the qualifying element with the use of information and telecommunication networks."
Cybercrime was estimated to cost the global economy $945 billion in 2020. Increasingly, law enforcement agencies are using social network analysis (SNA) to identify key hackers from Dark Web hacker forums for targeted investigations. However, past approaches have primarily focused on analyzing key hackers at a single point in time and use a hacker's structural features only. In this study, we propose a novel Hacker Evolution Identification Framework to identify how hackers evolve within hacker forums. The proposed framework has two novelties in its design. First, the framework captures features such as user statistics, node-level metrics, lexical measures, and post style, when representing each hacker with unsupervised graph embedding methods. Second, the framework incorporates mechanisms to align embedding spaces across multiple time-spells of data to facilitate analysis of how hackers evolve over time. Two experiments were conducted to assess the performance of prevailing graph embedding algorithms and nodal feature variations in the task of graph reconstruction in five time-spells. Results of our experiments indicate that Text-Associated Deep-Walk (TADW) with all of the proposed nodal features outperforms methods without nodal features in terms of Mean Average Precision in each time-spell. We illustrate the potential practical utility of the proposed framework with a case study on an English forum with 51,612 posts. The results produced by the framework in this case study identified key hackers posting piracy assets.
"The petro-chemical industry is a critical infrastructure that is vulnerable to cybercrime. In particular, industrial process control systems contain many vulnerabilities and are known targets for hackers. A cyberattack to a chemical facility can cause enormous risks to the economy, the environment, and public health and safety. This gives rise to the question how corporate cybersecurity has developed; how it is governed; and whether it should be subject to public oversight. This paper presents a case study of the governance of cybersecurity in the petrochemical industry in the Rotterdam Mainport area in the Netherlands, which reflects the 'new governance' view that cybersecurity can best be governed through voluntary public-private partnerships. The paper finds however that actual collaborative governance is not developing in the petrochemical industry in the port of Rotterdam; that corporate awareness and investment in cybersecurity stay behind standards, and that cybersecurity is not included in regulatory inspections. The paper places these findings in the context of three problems often associated with 'new governance' particularly pressing in cybersecurity governance: a weak role of government in public-private collaborative arrangements; an expectation that businesses will invest in self-regulation even in the absence of incentives to do so, and a lack of information exchange. In the port of Rotterdam, these problems result in a lack of obligations and accountability pressure on petrochemical corporations, leaving on of the most important chemical industrial hazards of today, largely unregulated."
Today, incorporating advanced machine learning techniques into intrusion detection systems (IDSs) plays a crucial role in securing mobile edge computing systems. However, the mobility demands of our modern society require more advanced IDSs to make a good trade-off between coping with the rapid growth of traffic data and responding to attacks. Thus, in this paper, we propose a lightweight distributed IDS that exploits the advantages of centralized platforms to train and learn from large amounts of data. We investigate the benefits of two promising bioinspired optimization algorithms, namely Ant Lion Optimization and Ant Colony Optimization, to find the optimal data representation for the classification process. We use Deep Forest as a classifier to detect intrusive actions more robustly and generate as few false positives as possible. The experiment results show that the proposed approach can enhance the reliability of lightweight intrusion detection systems in terms of accuracy and execution time.
Legal and commercial concepts must be well coordinated in maritime sector for better efficiency of logistic chain. Maritime sector is sensitive to global market fluctuations and politics. Therefore, each component of the entire logistic chain is almost equally important. Effective ship brokering has potential and relational impacts on ship owners, importers, exporters and banking sectors. Logistic chain efficiency is based on each compartment of the system. This is also necessary to build economically robust international trading infrastructure. Among many others, fake brokering, cybercrimes and unregulations are some of the major problems faced to brokering sector in Turkey. Therefore, a quantitative decision making approach was suggested to improve brokering activities in Turkey. For this purpose, the problems in ship brokering activities in Turkey were examined and an attempt was made to determine possible solutions. The factors that are effective on current problems were examined in detail in the study and the correlations between these problems and their structure were analyzed with fuzzy DEMATEL (The Decision Making Trial and Evaluation Laboratory) method. Furthermore, an integrated model approach was devised to address and rank the significance of the alternatives that could provide applicable solutions to the problems using the fuzzy TOPSIS (Technique for Order Performance by Similarity to Ideal Solution) method. Study findings demonstrated that some of the most important problems experienced in ship brokerage activities in Turkey were K7 (issues related to certification, supervision, development and authorities), K6 (informal brokerage and non-prosecution), K3 (problems experienced with foreign languages) and K5 (fake brokers and cybercrime).
Social Engineering holds one of the most critical threats to public and private organizations. In this paper we focus on phishing threats by measuring the positive impact that awareness methods may provide to them in a long-term period to companies and public bodies. The assessment criterion uses two phishing attacks in a period of 18 weeks. The phishing attack comprises a hook mail containing a link to a credentials harvesting website. Users' reaction and user agent fingerprints are used in order to calculate a risk score for each victim. By applying chi square - tests it was found that there is a statistically significant score improvement for participants that were trained via the awareness methods. Furthermore, a risk analysis is conducted to identify, quantify and prioritize potential risks that could negatively affect the end-user's operations. The main idea concerning this proposed technique is the fact that the assessment methods can assist the employees to develop skills and abilities in order to use the digital world safely, avoiding phishing attacks. The risk analysis findings indicate that the awareness approach has significant improvement in long term lasting risk reduction. The study was conducted as part of the European Horizon 2020 DOGANA project which aims to deploy effective mitigation strategies and lead to reduce the risk created by modern Social Engineering 2.0 attack techniques. The results obtained in this paper corroborate the results obtained by the EU funded project SAINT from the econometric analysis and modeling of the cybercrime and cyber security markets.
Human needs consist of five levels, which are: physiological needs, safety needs, love needs, esteem needs and self-actualization. All these needs lead to human behavior. If the environment of a person is positive, healthy behavior is developed. However, if the environment of the person is not healthy, it can be reflected in his/her behavior. Machines are intelligent enough to mimic human intelligence by using machine learning and artificial intelligence techniques. In the modern era, people tend to post their everyday life events on social media in the form of comments, pictures, videos, etc. Therefore, social media is a significant way of knowing certain behaviors of people such as abusive, aggressive, frustrated and offensive behaviors. Behavior detection by crawling the social media profile of a person is a crucial and important idea. The challenge of behavior detection can be sorted out by applying social media forensics on social media profiles, which involves NLP and deep learning techniques. This paper is based on the study of state of the art work on behavior detection, and based on the research, a model is proposed for behavior detection. The proposed model outperformed with an F1 score of 87% in the unigram + bigram class, and in the bigram + trigram class, it gave an F1 score of 88% when compared with models applied on state of the art work. This study is a great benefit to cybercrime and cyber-security agencies in shortlisting the profiles containing certain behaviors to prevent crimes in the future.
Over the past ten years, there has been a significant increase in computer network intrusions, partly due to a thriving black market for cybercrime and the availability of advanced tools for committing such breaches. The most effective method for stopping unwanted intrusions and identifying abnormal network behavioral patterns is an intrusion detection system (IDS). In IDS, transfer learning techniques are frequently employed. An ML-based IDS experiences problems with data imbalance and a greater false detection ratio due to a small training dataset. These ID systems can quickly and automatically recognize harmful threats. The network requires a complex security solution because dangerous threats constantly develop and appear. As a result, developing an efficient and intelligent ID system is a substantial scientific challenge. We suggested an effective ensemble strategy that improved the spotted hyena optimization algorithm (ISHO) and the honey badger algorithm (HBA) to address the data imbalance and overfitting problem. The dataset is balanced by increasing the number of data samples and the detection precision. The Squeeze-and-Excitation (SE)-Deep Residual Network 152 (SE-ResNet152) approach is utilized to remove the less critical features. Every iterative phase includes using a list of decision trees, which monitor the performance of the categorizer and prevent overfitting issues. We use the datasets UNSW-NB15, CSE-CIC IDS 2018, and CICIDS2019 to simulate and assess the model. Compared to other approaches, the proposed approach performs well on three datasets and obtains above 99% accuracy, precision, recall, and F-measure.
While the world is becoming increasingly interdependent, the countries are evermore dependent on information systems, high-speed communications and artificial intelligence. That is increasing the risks and threats to citizens' privacy, global trade, the resilient of critical infrastructure and even the readiness of the armed forces. Traditional security measures are lagging behind new challenges and they are insufficient to prevent emerging risks. The potential damage that these threats can inflict on security and economy grows tremendously, but countermeasures needs to be sophisticated and thus difficult for implementation. The 21st century instead to be the era of technologies and information revolutions is becoming of synonymous of global risks and uncertainty. This article explore the transformation of relations in cyberspace as a result of globalization and the development of information technologies, that create preconditions for new forms of criminal activity cyber-war, cyber terrorism and cybercrime. The globalization of social and political processes lead to globalization of crime, which now is transnational. Today becomes more and more obvious the need for conceptual clarity and innovative approaches in security study for cyber threats in modern society. Understanding of interconnection between globalization in cyberspace and increasing insecurity in it as a consequence of that globalization and the relation between new information technology and emerging threats coning with their use is crucial for security of our societies. The article explore cyber security as one of the biggest challenges for today's interconnected world and how to achieve it resilient development and progress.
PurposeThe purpose of this preliminary empirical research study is to understand how environmental disruption such as brought on by the COVID-19 pandemic induces shifts in organisational culture, information security culture and subsequently employee information security compliance behaviour. Design/methodology/approachA single-organisation case study was used to develop understanding from direct experiences of organisational life. Both quantitative and qualitative data were collected using a sequential mixed methods approach, with the qualitative phase following the quantitative to achieve complementarity and completeness in analysis. For the quantitative phase, 48 useful responses were received after a questionnaire was sent to all 150-200 employees. For the qualitative phase, eight semi-structured interviews were conducted. Statistical software was used to analyse the quantitative data and NVivo software was used to analyse the qualitative data. FindingsThe pandemic-induced environmental disruption manifested as a sudden shift to work-from-home for employees, and relatedly an increase in cybercrime. The organisational response to this gave rise to shifts in both organisational and information security culture towards greater control (rule and goal orientations) and greater flexibility (support and innovation orientations), most significantly with information security culture flexibility. The net effect was an increase in employee information security compliance. Originality/valueThe vast literature on organisational culture and information security culture was drawn on to theoretically anchor and develop parsimonious measures of information security culture. Environmental disruptions such as those caused by the pandemic are unpredictable and their effects uncertain, hence, the study provides insight into the consequences of such disruption on information security in organisations.
The South African economy is not only the second largest in Africa, but it is also the most diversified, industrialized and technologically advanced on the continent. This technological advance is evidenced by its citizenry's comparatively high-level of digital connectivity and interconnectedness. On the reverse side, South Africa (SA) has the third highest number of cybercrime victims internationally. Maintaining and expanding its competitive technological advantage requires of South Africa to have a robust national cybersecurity endeavour. This needs to include an innovative, high-impact cybersecurity awareness campaign that effectively reaches a diverse population. This paper's primary aim is to propose such a high-impact drive, namely a broad-based national cybersecurity awareness campaign that levers the South African minibus taxi industry. The paper's three objectives pertain to the 'why' and 'how' of such a campaign. The paper's first objective is to substantiate the need for a broad-based cybersecurity awareness campaign (in short: why is it needed?). The second objective is to substantiate why the taxi industry constitutes an optimal platform for a game changing campaign. Thirdly, the paper advances a proposition on a Taxi Industry Cybersecurity Awareness Campaign (TICAC) (i.e. how can the taxi industry be a game changer?). We qualify the TICAC as a tentative, high-level conceptual proposition subject to much further research on the theoretical/academic and practical levels. It is hoped that the paper would be of value to also other countries - developed and developing - in utilising private and/or public transport industries as platforms for cybersecurity awareness initiatives.
In recent years, Nigeria's image has always been negatively depicted in the global media, as the country's name is associated with some of the world's most sophisticated cybercriminals. The situation with the country's perceived dented reputation, most especially in the Southeast Asia, Western Europe and the United States of America, is ripe for the anti-cybercrime discourse to take root, and subsequently, become a fertile ground for various parties to contribute to the grand discourse from different perspectives. This article highlights the way Nigerian government, through its revenues generating agency, the Federal Inland Revenue Services (FIRS), utilizes a print media warning advertisement (WA) to discursively construct and showcase its efforts in combating cybercrimes. The study utilizes Fairclough's three-layered model for approaching discourse to analyse the FIRS-sponsored WA, which was published in The Guardian newspaper on 2 May 2013. The study incorporates analytical tools from the visual grammar (VG) and the multimodal discourse analysis (MDA) to examine the visual dimensions constituting the frame of the WA. The study revealed how the Nigerian government, through the FIRS sponsored WA, has attempted to discursively draw the attention of the general public to the potential dangers associated with the cybercriminals and their activities as well as suggesting the best ways to escape falling into their traps. The study recommends that governments and other civil societies should explore other means of creating more awareness to the general public, given the speed at which cyber-related crimes upsurge globally at the present time.
"Digital business has marked an era of transformation, but also an unprecedented growth of cyber threats. While digital explosion witnessed by the banking sector since the COVID-19 pandemic has been significant, the level and frequency of cybercrimes have gone up as well. Cybercrime officials attribute it to remote working-people using home computers or laptops with vulnerable online security than office systems; malicious actors relentlessly developing their tactics to find new ways to break into enterprise networks and grasping defence evasion; persons unemployed during the pandemic getting into hacking; cloud and data corruption; digital fatigue causing negligence; etc. This study adopts a case-based approach to explore the importance of business ethics, information sharing and transparency to build an information-driven society by scouting the case of Punjab and Maharashtra Cooperative (PMC) Bank, India. PMC defaulted on payments to its depositors and was placed under Reserve Bank of India's directions due to financial irregularities and a massive fraud perpetrated by bank officials by orchestrating the bank's IT systems. The crisis worsened when panic-stricken investors advanced their narrative through fake news peddled via social media channels, resulting in alarm that caused deaths of numerous depositors. It exposed several loopholes in information management in India's deposit insurance system and steered the policy makers to restructure the same, thus driving the country consistent with its emerging market peers. The study further identifies best practices for aligning employees towards ethical behaviour in a virtual workplace and the pedagogical approaches for information management in the new normal."
Saudi Arabia has a goal of ensuring that it has at least two cities among the top 100 smart cities of the future. However, increasing connectivity and incorporation of smart solutions in cities still raises concerns over cyber security with threats arising daily including denial of services and phishing as some of the most significant. Saudi Arabia, therefore, needs a cybersecurity policy framework that will ensure effective protection for all stakeholders in the smart city from these cyber threats. User acceptance is foremost important in any new technology, including smart-cities. Due to ongoing cyber threats and in the absence of an efficient cyber policies, Saudi end-user community is not keen to accept newer technologies where their interaction with online medium is required. The proliferation of smart cities globally affords the opportunity to analyze and compare the efforts made in Saudi Arabia with other nations like the USA, India and Singapore which is the premier smart city model in the globe currently. This review looks at the similarities and differences between KSA's cyber security policy framework with these three nations. The review will note some of the defining characteristics and approaches to cyber security in the smart cities of USA, India, and Singapore. After reviewing the current framework in Saudi Arabia, this paper will make suggestions such as updating Saudi's cybercrime legislation like in the US or formulating a master cyber security plan as seen in Singapore that will improve KSA's framework creating the best framework model for cyber security in its smart cities.
"Grown internet usage by individual and industries have also increased the attack vector in cyberspace rapidly. Botnet is a digital weapon used by attackers to commit cybercrime in stealthiest way for all type of illegal online activity. Botnet is well articulated attack responsible for many malicious activities in large volume and mass effective against any targeted organization such as confidential data theft, financial loss, distribution of pirated products, e-business extortion and network or service disruption. Because of its global nature of infection and innovative covert techniques of malware development to evade detection, it is also known as advance persistent threat. An analysis of this APT revealed the advancement in sophistication of bot malware by encryption methods, concealed network connections and silent escape as an effective tool for profit-motivated e-crime. Reverse engineering is procedure to analyze malware to classify its type, hazard, impact on machine, information outflow and removal of signature technique. Botnet (APT) detection needs improvised process to identify the channel, architecture and encryption weakness. In bot examination; Programming style, network protocol and behavior analysis can mitigate the APT by creating signature, prototype of behavior based approach and elimination of C&C servers. Reverse engineering is excellent way for defense the modern botnets to immune valuable information by identifying the evidence behavior, log collection and digital forensics. The main aim of study is to determine the most adequate approach to recreate a botnet incident. Network security is prime concern to avoid state sponsored attacks like botnet so security of digital nation and e-governance can be assured."
The COVID-19 pandemic changed the lives of millions of citizens worldwide in the manner they live and work to the so-called new norm in social standards. In addition to the extraordinary effects on society, the pandemic created a range of unique circumstances associated with cybercrime that also affected society and business. The anxiety due to the pandemic increased the probability of successful cyberattacks and as well as their number and range. For public health officials and communities, location tracking is an essential component in the efforts to combat the disease. The governments provide a lot of mobile apps to help health officials to trace the infected persons and contact them to aid and follow up on the health status, which requires an exchange of data in different forms. This paper presents the one-time stamp model as a new cryptography technique to secure different contact forms and protect the privacy of the infected person. The one-time stamp hybrid model consists of a combination of symmetric, asymmetric, and hashing cryptography in an entirely new way that is different from conventional and similar existing algorithms. Several experiments have been carried out to analyze and examine the proposed technique. Also, a comparison study has been made between our proposed technique and other state-of-the-art alternatives. Results show that the proposed one-time stamp model provides a high level of security for the encryption of sensitive data relative to other similar techniques with no extra computational cost besides faster processing time.
Non-consensual intimate image dissemination (NCII), or else better known as revenge pornography is a form of technology-facilitated sexual violence that can have devastating effects on the victim. This is one of the first studies examining how demographic characteristics (gender, sexual orientation), personality traits (Dark Tetrad), and attitudes (aggrieved entitlement, sexual entitlement, sexual image abuse myth acceptance) predict NCII perpetration and victimization. In a sample of 810 undergraduate students (72.7% female and 23.3% male), 13.7% of the participants had at some point in their life, distributed nude, or sexual pictures of someone else without consent and 28.5% had experienced such victimization. NCII perpetration was predictive of NCII victimization and vice versa. Using binomial logistic regression, we found that women, members of the LGBQ+ community, those scoring higher in sadism, and participants with a history of NCII perpetration were more likely to report that someone had distributed their nude or sexual image without consent. Further, we found that those scoring higher in narcissism and sadism, along with those with a history of NCII victimization were more likely to report they had distributed the nude or sexual image of someone else without consent. Finally, the findings suggest that the relationship between victims and perpetrators is quite a bit more varied than the term revenge pornography implies.
In recent years cyber terrorism has become increasingly used with the globalization of technology and people's access to high-speed internet. It takes place exclusively in the online environment, the advantage being that it offers an increased level of anonymity to users. Terrorist groups are targeting the misappropriation of social media accounts, focused on Distributed Denial of Service activities, exploiting communications services and banking services for fraudulent misappropriation of financial accounts. Cyberterrorism generates very high costs for the national economy, such as the involvement of specialists for detecting and correcting intrusion, declining productivity and income, costs of information theft, regaining the reputation of an institution or company, the costs relating to the resumption of production and the provision of services, the loss of information concerning intellectual property, financial manipulation using stolen information, the cost of securing computer networks and assuring them in the event of intrusion, costs generated by the time spent on recovering stolen data. In 2018 both the US and Egypt took action on these activities, the US modified its national cyber strategy and Egypt adopted the new law project to combat cybercrime. The main cyber threats they face are: threats of intrusion and sabotage of IT infrastructures, cyberterrorism and cyberwar, threats to digital identity and theft of private data, malware programs. The targets of the attacks were public sector entities, financial organizations, health care organizations, retail and accommodation. The reasons for why hackers attack these organizations are money-related, malware infected by emails, commercial and industrial espionage.
The Internet of Things (IoT) and the Android operating system have made cutting-edge technology accessible to the general public. These are affordable, easy-to-use, and open-source technology. Android devices connect to different IoT devices such as IoT-enabled cameras, Alexa powered by Amazon, and various other sensors. Due to the escalated growth of Android devices, users are facing cybercrime through their Android devices. This article aims to provide a comprehensive study of the IoT and Android systems. This article classifies different attacks on IoT and Android devices and mitigation strategies proposed by different researchers. The article emphasizes the role of the developer in secure application design. This article attempts to provide a relative analysis of several malware detection methods in the different environments of attacks. This study expands the awareness of certain application-hardening strategies applicable to IoT devices and Android applications and devices. This study will help domain experts and researchers to gain knowledge of IoT systems and Android systems from a security point of view and provide insight into how to design more efficient, robust, and comprehensive solutions. This article discusses different attack vectors and mitigation strategies available to both developers and in the open domain. Certain guidelines are also suggested for application and platform developers, as well as application databases (Google play store), to limit the risk of attack, and users can form their own defense with knowledge regarding keeping hardware and software updated and securing their system with a strong password.
In the last decade, Social Networks (SNs) have deeply changed many aspects of society, and one of the most widespread behaviours is the sharing of pictures. However, malicious users often exploit shared pictures to create fake profiles, leading to the growth of cybercrime. Thus, keeping in mind this scenario, authorship attribution and verification through image watermarking techniques are becoming more and more important. In this paper, we firstly investigate how thirteen of the most popular SNs treat uploaded pictures in order to identify a possible implementation of image watermarking techniques by respective SNs. Second, we test the robustness of several image watermarking algorithms on these thirteen SNs. Finally, we verify whether a method based on the Photo-Response Non-Uniformity (PRNU) technique, which is usually used in digital forensic or image forgery detection activities, can be successfully used as a watermarking approach for authorship attribution and verification of pictures on SNs. The proposed method is sufficiently robust, in spite of the fact that pictures are often downgraded during the process of uploading to the SNs. Moreover, in comparison to conventional watermarking methods the proposed method can successfully pass through different SNs, solving related problems such as profile linking and fake profile detection. The results of our analysis on a real dataset of 8400 pictures show that the proposed method is more effective than other watermarking techniques and can help to address serious questions about privacy and security on SNs. Moreover, the proposed method paves the way for the definition of multi-factor online authentication mechanisms based on robust digital features.
Cloud computing paradigm continues to revolutionize the way business processes are being conducted through the provision of massive resources, reliability across networks and ability to offer parallel processing. However, miniaturization, proliferation and nanotechnology within devices has enabled digitization of almost every object which eventually has seen the rise of a new technological marvel dubbed Internet of Things (IoT). IoT enables self-configurable/smart devices to connect intelligently through Radio Frequency Identification (RFID), WI-FI, LAN, GPRS and other methods by further enabling timeously processing of information. Based on these developments, the integration of the cloud and IoT infrastructures has led to an explosion of the amount of data being exchanged between devices which have in turn enabled malicious actors to use this as a platform to launch various cybercrime activities. Consequently, digital forensics provides a significant approach that can be used to provide an effective postevent response mechanism to these malicious attacks in cloudbased IoT infrastructures. Therefore, the problem being addressed is that, at the time of writing this paper, there still exist no accepted standards or frameworks for conducting digital forensic investigation on cloud-based IoT infrastructures. As a result, the authors have proposed a cloud-centric framework that is able to isolate Big data as forensic evidence from IoT (CFIBD-IoT) infrastructures for proper analysis and examination. It is the authors' opinion that if the CFIBD-IoT framework is implemented fully it will support cloud-based IoT tool creation as well as support future investigative techniques in the cloud with a degree of certainty.
The consequences of Industry 4.0 have adverse side effects on cybercrime growth, which requires creating an effective cybersecurity system for companies. Therefore, this study aims to develop a composite indicator of company cybersecurity to assess its development needs. For this purpose, the authors modified Porter's method by constructing a superposition matrix based on the growth rates of cyber threats and risks, calculating their quantitative characteristics and a composite indicator. The computations are based on indicators for 2016-2022 characterizing cybersecurity vulnerabilities and the consequences of cyber threats: the share of companies experiencing one, six or more successful cyberattacks, considering the likely and very likely success of cyberattacks on them in the next 12 months, security threat and concern indices, the share of companies with a growing security budget affected by ransomware and experiencing a shortage of skilled IT security personnel, the cost of stolen or compromised credentials. As a result, cybersecurity needs increased significantly for 2020-2022, mainly due to digital transformation and the cyber threats growth after the COVID-19 pandemic. A comparative analysis of the proposed indicator with those characterizing the development of Industry 4.0 showed that the need for a reliable cybersecurity system is much more important than the active development of modern technologies. Spending on IT is also increasing, but not enough to meet the needs of cybersecurity development, except for the 2022 results. The proposed indicator is defined for companies worldwide, but its versatility allows the methodology to be applied to enterprises of various industries and sizes.
The rise in internet users has brought with it the impending threat of cybercrime as the Internet of Things (IoT) increases and the introduction of 5G technologies continues to transform our digital world. It is now essential to protect communication networks from illegal intrusions to guarantee data integrity and user privacy. In this situation, machine learning techniques used in data mining have proven to be effective tools for constructing intrusion detection systems (IDS) and improving their precision. We use the well-known AWID3 dataset, a comprehensive collection of wireless network traffic, to investigate the effectiveness of machine learning in enhancing network security. Our work primarily concentrates on Krack and Kr00k attacks, which target the most recent and dangerous flaws in IEEE 802.11 protocols. Through diligent implementation, we were able to successfully identify these threats using an IDS model that is based on machine learning. Notably, the resilience of our method was demonstrated by our ensemble classifier's astounding 99% success rate in detecting the Krack attack. The effectiveness of our suggested remedy was further demonstrated by the high accuracy rate of 96.7% displayed by our neural network-based model in recognizing instances of the Kr00k attack. Our research shows the potential for considerably boosting network security in the face of new threats by leveraging the capabilities of machine learning and a diversified dataset. Our findings open the door for stronger, more proactive security measures to protect IEEE. 802.11 networks' integrity, resulting in a safer online environment for all users.
e-Wallets and m-banking apps became more and more popular in the developed world, approaching a point of tipping. This can be due to the global use of big and small merchants of paying equipment and the ubiquity of e-wallet and m-banking apps adoption. Many consumers are using e-wallets and m-banking apps that can be an effective cybercrime option. e-Wallets and m-banking apps allow financial transactions via smartphones that give cybercriminals a lucrative opportunity. Mobile technology has become increasingly mainstream and continually strengthening, with the focus on mobile apps protection and forensic analysis developing. In this paper, the security aspect of five popular e-wallets in Malaysia were analyzed. This paper also provides a security analysis of another five leading m-banking apps. The security analysis is based on a security principle that is recommended by Open Web Application Security (OWASP) under Mobile Security Testing Guide (MSTG) and Mobile Security Threats (MST). The static analysis has been done by using three mobile application-testing tools. This study included a variation of vulnerability scanning, code review and, most significantly, penetration testing. Each app complied with the security requirement, but their security features and characteristics, such as encryption, security protocols, and app services, are different to each other. This study was carried out using a DELL computer with Intel Core i7 CPU, 3.40 GHz CPU, 6 GB RAM. Finally, the results revealed the secure e-wallet and m-banking apps among the selected apps.
We live in an information society in which development occurs rapidly. Digitalization affects all aspects of society, cutting through all societal functions. Digital platforms make it possible to match demands on things and services and deliver them at a lower cost. Automatization and robotics affect demand for manual operations and workers. Moreover, working life is changing, as it increasingly utilizes an emerging complementary relationship between machines and people. Needs for more knowledge and skills also arise. This article provides a preliminary plan as to how and in what respects the issue of information security might be incorporated into university teachers' expertise. For example, in distance learning-a form of education distribution requiring computer equipment that is to be provided by the student-information security is crucial. From the educator's point of view, student identification-in connection with exams, for example-is obviously critical. Digital on-campus exams are another pertinent theme, entailing many opportunities for doing things in new or better ways in comparison with paper exams and the time-consuming administration they involve. New questions arise, however. Certain aspects of information and communication technology entail risks-such as Distributed Denial-of-Service attacks, cybercrime, and sabotage in general. This article will discuss the need for university teachers to be prepared to respond adequately and effectively to information security issues. Examples are given showing how the University of Boras approaches security considerations. A preliminary analysis of the need to include information security in the general knowledge required of all university teachers in the era of eLearning is given.
In the network security cybercrime technologies have brought many good things by means of the internet: electronic commerce, easy access to vast stores of reference material, collaborative computing, e-mail, and new avenues for advertising and information distribution, to name a few. As with most technological advances, there is also a other side: criminal hackers. Governments, companies, and private citizens around the world are anxious to be a part of this revolution, but they are afraid that some hacker will break into their Web server and replace their logo with pornography, read their e-mail, steal their credit card number from an online shopping site, or implant software that will secretly transmit their organization's secrets to the open Internet. With these concerns and others, the ethical hacker can help. This paper describes ethical hackers: their skills, their attitudes, and how they go about helping their customers find and plug up security holes. Hacking is the word that shakes everyone whenever it is said or heard by someone. Everyone born in this world with attitude wants to be a Hacker. But it is not a job of a new born baby or an old grown lady. A Hacker needs a brilliant mind to hack anything. There are many rules that he should learn to become an Ethical Hacker which is also called as penetrate testing. These rules include knowledge of HTML, Java Scripts, Computer Tricks, Cracking & Breaking etc. etc. In this paper I explain about the hacking techniques and the functions of how it takes place in the network and the methods to be solved.
Lato sensu when we encounter in treatises on harmony the notion of octaves and parallel/anti-parallel or direct fifths, we find out that these movements of the voices in the tonal-functional harmony are as a rule prohibited for a number of reasons. Alexandru Pascanu motivated these interdictions related to the parallel fifths as follows the reason for this categorical interdiction does not reside in the fact that it sounds bad (as they say), but rather in a stylistic imperative. The parallel fifths, reminiscences of the primitive polyphonic forms, create a modal atmosphere that contradicts the tonal ambience. The challenge in this article consists in making a parallel between the rules of harmony, of voice leading, that we musicians know only too well, and those other rules - the rules of law (applicable in cyberspace) - which could be very concisely defined as: those general and abstract rules that regulate the conduct of law subjects in their legal relationships or simply the rules of social conduct or morals. Using transposition and focusing on the rules that address the active or passive participants in cyberspace, we shall be able to identify a series of rules/norms that govern this electronic realm, and implicitly, reveal a range of illicit conducts, as a consequence of the breach of these rules by users. The availability and accessibility of the Internet have brought about personal and social changes. There has been an important increase in the number of human activities that have moved face-to-face encounters from the physical space into cyberspace through online activities - particularly during this pandemic - and these new habits are slowly changing customs, practices, priorities, governing, and even human culture. In cyberspace people behave in a manner that requires new concepts in psychology, as well as in the realm of national, European, and international law, which require the use of old psychological knowledge, as well as the articulation of new ideas in order to understand and explain the human behaviour in cyberspace.
"Introduction: The growing offer of products and services in a virtual environment resulted in a significant increase in the volume of personal and organizational data that transit through data networks and are stored in different places, by different entities. Interactions in this environment are continuously monitored, whether for needs related to the interests of companies or organizations, or to curb the most diverse illicit practices. These monitoring activities, which configure a state of permanent vigilance, raise concerns related to the treatment of data collected and processed, with important ethical and legal implications, and which may not be properly covered by legal instruments. Objective: The research sought to identify which studies published between 2010 and 2020 deal with ethical or legal issues related to privacy in the virtual environment. Methodology: A literature review was carried out, adopting content analysis as a technique. As a technological resource, Excel & REG; and ATLAS.TI & REG; software were used. Results: Only 19 of the 39 documents found are in line with the research objective, most of which are theses and dissertations. Publications are concentrated in the second half of the analyzed period. Conclusion: The analyses showed a growing need to monitor the information circulating in the virtual environment, involving sender and receiver, covering security issues, increased volume of data, new technologies, and cybercrime among other issues. The number and distribution of articles over the analyzed period indicate that these themes require even greater investigations, contemplating broader perspectives of monitoring activities in virtual environments."
The aim of the paper is to analyse the likely implications of Generative AI (GAI) on various aspects of business and the economy. Amid the rapid growth and maturing of Generative AI technologies such as Large Language Models (like ChatGPT by OpenAI) a rapid growth of both immediate and potential applications can be seen. The implications for the economy and industries of this technological shift will be discussed. The foreseeable scenarios for the level and types of adoption that GAI might achieve-from useful analytical tool, invaluable assistant to the white-collar workers of the world to being trusted with a wide array of business and life-critical decision making. Both disruptive and premium service opportunities are foreseen. For instance, general purpose models may provide quality service-such as copywriting-to overserved customers leaving human writers as the premium option. In this context, overserved customers would be those who would be satisfied with a non-human, potentially less creative content. On the other hand highly specialized models-specifically trained in a given domain and with access to proprietary knowledge can possibly provide a premium service over that provided by human experts. It is expected that some jobs will be replaced by new AI applications. However, new workplaces will emerge. Not only the obvious expert-level data scientist roles but also low grade, model supervisors-people training the models, assessing the quality of responses given and handling escalations. Lastly new cybercrime risks emerging from the rise of GAI are discussed.
Background and Purpose: The Covid-19 pandemic has sparked global challenges, economic disruption, and an increase in related crimes, such as scams, fraud, and cybercrime. It has also changed the routines of individuals, businesses, and governments in combating financial crimes, especially professional accountants. As front-liners in the anti-money laundering (AML) regime, professional accountants play a vital role in combating financial crime, including money laundering activities that have risen during the pandemic. However, limited study has investigated the money laundering reporting framework for the professional accountants. Hence, this study aims to investigate the money laundering framework for professional accountants that suits the post-pandemic period. Methodology: This study conducted a content analysis, focussing on the current red flags for money laundering activities specifically for the professional accountants mentioned in prior literature and publicly available reports such as Financial Action Task Force (FATF), ICAEW and ACCA Global. Since this study focuses on the money laundering framework for the professional accountants, this study also referred to the MIA Competency Framework that highlights the importance of professional scepticism and professional judgement. Findings: Findings showed that professional accountants must exercise high professional scepticism and judgement when dealing with money laundering risk indicators related to related party transactions, beneficial ownership and e-commerce activities. This is to provide quality reporting to the authorities and to proceed with money laundering investigations. Contributions: This study will contribute to the regulators and professional accountant by proposing a money laundering framework for the professional accountants that would assist them in fulfilling their role as money laundering reporting entity.
Cybercrime affects companies worldwide, costing millions of dollars annually. The constant increase of threats and vulnerabilities raises the need to handle vulnerabilities in a prioritized manner. This prioritization can be achieved through Common Vulnerability Scoring System (CVSS), typically used to assign a score to a vulnerability. However, there is a temporal mismatch between the vulnerability finding and score assignment, which motivates the development of approaches to aid in this aspect. We explore the use of Natural Language Processing (NLP) models in CVSS score prediction given vulnerability descriptions. We start by creating a vulnerability dataset from the National Vulnerability Database (NVD). Then, we combine text pre-processing and vocabulary addition to improve the model accuracy and interpret its prediction reasoning by assessing word importance, via Shapley values. Experiments show that the combination of Lemmatization and 5,000-word addition is optimal for DistilBERT, the outperforming model in our experiments of the NLP methods, achieving state-of-the-art results. Furthermore, specific events (such as an attack on a known software) tend to influence model prediction, which may hinder CVSS prediction. Combining Lemmatization with vocabulary addition mitigates this effect, contributing to increased accuracy. Finally, binary classes benefit the most from pre-processing techniques, particularly when one class is much more prominent than the other. Our work demonstrates that DistilBERT is a state-of-the-art model for CVSS prediction, demonstrating the applicability of deep learning approaches to aid in vulnerability handling. The code and data are available at https://github.com/Joana-Cabral/CVSS_Prediction.
The objective of this paper, flowing from an active post-graduates research program, is to provide a best practice strategy framework for developing countries to secure cyberspace, taking cognisance of the realities and constraints within a developing milieu. Cybersecurity policies and related strategies are required for developing countries in order to effectively safeguard against cyber related threats (the same as for developed countries). These policies and strategies for developing countries will differ from those of developed countries due to the unique realities within a developing world. Africa in specific is presently seen as a hotbed for cybercrime, and one of the reasons is that many African countries do not have a proper framework, policies and procedures to properly protect cyberspace. Experience has also shown that a pure adoption by developing countries of the cyber frameworks of developed nations will not always be effective, especially due to the unique requirements and realities within developing worlds, such as limited resources, infrastructure, technologies, skills and experience. The approach taken for the research program, and discussed in this paper, is based on a comprehensive literature study on several existing cybersecurity policies and strategies from both developed and developing countries. From this the drivers / elements for national cybersecurity policies and strategies were identified. These drivers were than adapted to specifically relate to the requirements of developing countries, and then, utilising the identified and adapted drivers, our best practice strategy framework for developing countries to secure their cyberspace was developed. This document will be very useful for those African countries venturing into defining relevant policies and procedures.
Since February 2022, the Russian-Ukrainian armed conflict significantly impacted the digital landscape. This study examines the conflict's impacts on the local and global digital ecosystem. Using grey literature, we analyzed English-language data sources primarily published between November 2022 and April 2023, which were based on data sources from the commencement of the conflict to the publication date. The investigation reveals insights into four categories: cyber and kinetic warfare, telecommunications, IT and cloud computing, digital geopolitics and resilience. The main insight in cyber warfare is that cyber attacks on Ukraine's digital infrastructure were only partially effective despite the massive involvement of cybercrime groups implementing adaptive wiping and DDoS attacks on behalf of national interests. The cyber defense success resulted from the allies' support for Ukraine and a well-established and implemented national cybersecurity strategy. Kinetic attacks against data centers have accelerated the migration of data and applications to cloud computing. The most incredible legacy of the current conflict is the positive paradigm shift in the cloud's security and privacy capabilities for sensitive systems' continuous operation as an alternative to system disability in locally destroyed data centers. The third insight reveals Telecom's reliance on satellite network suppliers and mobile power equipment, such as High-Capacity batteries, which are in shortage worldwide. Digital geopolitics alters the paradigms of digital resilience. Global tech companies assume the role of digital nations and superpowers while taking sides. During the conflict, we observe for the first time in practice the digital blockade by global techies of one side of the conflict and the digital support of the other, thereby shaping paradigms regarding digital sovereignty as digital resilience.
Cyberbullying is a type of cybercrime that has become a new phenomenon, rapidly increasing on the cyberspace, as it utilizes the Internet technology to harass people, especially among teenagers and youth on online learning platforms. This study aimed to analyze the factors influencing LGBTQ cyberbullying on online learning platforms among university students in Thailand. A quantitative research design was used to collect data from 400 university students in Thailand through questionnaires. The data were analyzed through a structural equation model. The results found that most Thai students had a low level of cyberbullying behavior, as they respected each other, especially LGBTQ students. Moreover, there were three major factors comprising demographics, the situation, and online learning platform behavior that had a significant direct effect on the outcome of LGBTQ cyberbullying victims. The demographic factors (gender, motivation, psychology, and technology using behavior) had a direct effect on the situation factors (perceived support, parental involvement, and university climate and environment), and a direct effect on online learning platform factors (teachers, classmates, dialog of online learning, group work, and relationship between classmates and the teacher). Moreover, the LGBTQ online learning platform factor had a direct effect on the outcome of cyberbullying (social equality, mindset, intellect, physical, and society). The relative Chi-square (chi 2 / df) of 1.194 indicated that the model was suitable. The comparative fit index (CFI) was 0.991, the goodness of fit index (GFI) was 0.971, and the model based on the research hypothesis was consistent with the empirical data. The root mean square error of approximation (RMSEA) was 0.022.
Bitcoin and other cryptocurrencies are well-known for their privacy properties that allow for the anonymous exchange of money. Bitcoin tracking with taint analysis remains challenging as it does not account for the change in Bitcoins' ownership or the usage of Privacy-Enhancing Technologies (PETs) to obscure Bitcoins' movement, and often produces unessential incidents with transactions unlikely to be related to the targeted activity. In this paper, we propose to improve the Bitcoin taint analysis tracking process that adapts to the context of address ownership and avoid following unrelated transactions. First, we introduce an approach in which we incorporate Bitcoin taint analysis with address profiling. Second, we propose two context-based taint analysis strategies. Third, we introduce a set of metrics using hypothesised behaviours related to illegal Bitcoins and recognisable patterns within the blockchain. We conducted an experiment using sample data from known Bitcoin theft cases to illustrate and evaluate the approach. The results on address profile integration reveal distinct transaction behaviours in tracking theft cases following all the metrics, such as address reuse, address size and transaction fee payment. One of the context-based tracking strategies, Dirty-First, shows positive potential for illustrating illegal Bitcoins' spending and obscuring strategies. The majority of the six metrics we defined give distinct results in transaction behaviours between the theft cases and the control groups. Our context-based tracking methodology provides a solution for one of the shortcomings in the current Bitcoin tracking methodology and the next step for future cryptocurrency and cybercrime forensic research. (c) 2022 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).
The announced problems did not arise today from scratch. It seemed that in modern society, with its developed system of hygiene, medicine, sanitary and epidemiological measures for the prevention of diseases (prevention, vaccination, etc.), the mass and uncontrolled spread of dangerous infectious diseases that covered not only local areas, but and entire countries, remained only in the memories of the distant past. But! As it turned out, the existing means of control were not enough, and now a new coronavirus with the code name - COVID-19 is rapidly spreading around the world and the spread of which is determined by the WHO pandemic. One of the measures to combat the spread of the virus was the isolation of citizens and, accordingly, a decrease of their activity. But the activity of criminals has not decreased - on the contrary, new circumstances give them the opportunity to more effectively carry out their actions. Therefore, the development of a typical psychological (criminalistic) portrait of a criminal with a conditional name quarantine, seems relevant and promising. Against the backdrop of outbreaks of coronavirus, more and more people do not part with medical masks, respirators. Although the mask provides some protection against infection, it significantly complicates the process of biometric authentication, which complicates the work of even artificial intelligence technologies.
Online users have completely changed the initial concept of social networks and how they have been perceived in recent years. Online Social Networks (OSNs) have completely changed in form, scope and nature, by the way Internet users create, view, share, present and/or use information. As a result, many vulnerabilities have emerged and the security threats, attacks, and malware of OSNs continue to increase because malicious or malevolent users commonly referred to, as 'attackers' often take advantage of the inferred trust relationships inherent in social networking to promote cybercrimes. This paper aims to investigate commonly used approaches to malware propagation on online social network systems, by examining their attack vectors and present a qualitative review of the findings. The study shows that online social networks are still vulnerable to Cross site scripting (XSS) and Clickjacking attacks. We further show that Clickjacking using malware URL links is still the dominant source of malware propagation on online social networks. Finally, we propose areas for further research used to stem the use of OSNs as a medium for malware distribution.
The term 'data', ubiquitous in the Digital Age, etymologically refers to a piece of information 'given' (datum). In this article, I argue that the term 'capta' would be more accurate, since information is often taken from us. Capturing information replicates normative elements of abuse, surveillance, control and harm becoming central and problematic within the emergence of the 'onlife'. I illustrate my argument via an ethnomethodological consideration of my attempt to resist the unwilling capture of personal information. Since 2016, I have engaged in what I call an 'offlife' existence, phasing out all devices and platforms that covertly capture personal data. However, my experiment has proven problematic, impractical and has even been perceived as being anti-social, especially in light of the Covid-19 pandemic. In the conclusion of this article, I consider the problematics of engaging in some form of resistance to data collection.
It has recently been discovered that large companies and nations observe their customers and citizens, disregarding any remaining moral and technological lines, being able to listen to telephone conversations and monitoring communications, through powerful monitoring and surveillance programs. Elsewhere on the planet, nations in turmoil or wrapped in a cloak of censorship persecute their citizens, controlling them by denying them access to the free Web without the threat of repercussions that threatens their dignity To support the present research, an analysis of platforms that allow anonymous and secure browsing and a study of technologies and programs with potential privacy breach and computer intrusion were performed. The main objective of this work was to analyse the computer monitoring and surveillance technologies, identifying the available tools, trying to find potential solutions, developing and providing a methodology that enhances any desktop, server or mobile operating system, with characteristics that combat the exposed in this summary. (C) 2021 The Authors. Published by Elsevier B.V.
The paper discusses means to identify potential impacts of data flows on customers' security, and privacy during online payments. The main objectives of our research are looking into the evolution of cybercrime new trends of online payments and detection, more precisely the usage of mobile phones, and describing methodologies for digital trace identification in data flows for potential online payment fraud. The paper aims to identify potential actions for identity theft while conducting the Reconnaissance step of the kill chain, and documenting a forensic methodology for guidance and further data collection for law enforcement bodies. Moreover, a secondary objective of the paper is to identify, from a user's perspective, transparency issues of data sharing among involved parties for online payments. We thus declare the transparency analysis as the incident triggering a forensic examination. Hence, we devise a semi-automated traffic analysis approach, based on previous work, to examine data flows, and data exchanged among parties in online payments. For this, the main steps are segmenting traffic generated by the process payment, and other sources, subsequently, identifying data streams in the process. We conduct three tests which include three different payment gateways: PayPal, Klarna-sofort, and Amazon Pay. The experiment setup requires circumventing TLS encryption for the correct identification of forensic data types in TCP/IP traffic, and potential data leaks. However, it requires no extensive expertise in mobile security for its installation. In the results, we identified some important security vulnerabilities from some payment APIs that pose financial and privacy risks to the marketplace's customers.
Increasing demand for digital evidence in criminal investigations is driving decentralization of forensic capabilities closer to the crime scene. Law enforcement agencies are struggling to keep pace with technological developments, cybercrime growth, and scientific advances. In federated environments, digital forensic knowledge and practices vary widely across regions. To reduce delays, wasted resources, missed opportunities, mistakes, and misinterpretations, there is a pressing need to balance the democratization of digital forensic capabilities with knowledge management and sharing between decentralized regions. There are multiple forms of knowledge to be managed, including procedural, technical, investigative, scientific, behavioral, crime analysis, and forensic intelligence. In addition, there are multiple knowledge producers and consumers, including police investigators, digital forensic practitioners, criminal intelligence analysts, attorneys, and judges. Knowledge management becomes even more challenging when multiple interdependent regions are involved, speaking different languages. Taking all of these factors into consideration, this work presents an inter-regional knowledge management solution for improving the quality, consistency, reliability, efficiency, cost-effectiveness, and return on investment of digital forensic capabilities. The basis of this work is a community-driven initiative of Swiss regional police authorities. Interviews were conducted with 15 digital forensic units to determine their current knowledge management practices and needs. The results were then generalized into a prioritized set of requirements for inter-regional digital forensic knowledge management that may be applicable in other countries. These requirements were used to evaluate knowledge management platforms, and one was selected. Implementation, operations, and maintenance challenges of an inter-regional digital forensic knowledge management platform are discussed.
Purpose The purpose of this paper is to decipher the law relating to cybercrimes regulation and benchmarking best practices that could be adopted to address regulatory weaknesses in some countries. In many countries, cybercrimes regulation is undermined by a lack of robust regulatory regimes. The few regimes that are available are fragmented with no coherent global strategy to deal with these offences across countries and regions. There is a lot of scholarly literature to corroborate the fact that lack of requisite laws on cyber and financial crimes has rendered states lame ducks when faced with well-organized and resourced criminal organizations. Design/methodology/approach This paper articulates intricacies of regulating money laundering and cybercrimes using data from selected African countries and beyond. Generic issues on financial crimes, cybercrimes, case law and policy documents drawn from different jurisdictions have been examined based on the objectives of the study. Cybercrime activities and anti-money laundering (AML) regulatory models have been evaluated drawing on experiences of selected countries in Africa and other countries. Questions whether suspicious activity reports are appropriate as a model to counter incidences of cybercrime activities or whether other options should be considered were also examined. Most notably, the risk-based assessment model such as profiling of high-risk clients rather than reporting every transaction will be compared and possibly suggested as a suitable alternative in financial crimes regulation. The authors have evaluated the data and AML regulatory approaches and other policy measures to curtail the foregoing threats. There is a possibility that AML tools used by financial institutions and banking activities could be used to prevent the growing threat of cybercrimes. The paper has also been enriched by case studies of tenuous legal systems and fragmentation of laws on cybercrimes and financial crimes and how these gaps have been exploited to fuel incidences of illicit criminal activities around the globe. The paper has also used empirical data including visits to banks and financial institutions on the nexus between the threat of cybercrimes and money laundering prevention. The authors have been selective, evaluating cases from 2000s to date. This timeline was particularly important because of the increased incidences of computers and money laundering threats globally. After analysing the data, the authors were able to delineate that there is a close connection between the foregoing two crimes, how they operate in practice, differences and similarities in the counter-measures used to mitigate their negative effect globally. Thus, in the authors' contention, this is a novel study that is likely to spur farther research on law and policy against cyber and AML crimes not only in Uganda but also in other jurisdictions. At the same time, the findings of the study could complement, and perhaps also complete, the work of scholars who have written papers on cybercrimes to advocate for regulatory changes fight against these offences. The study will also complement the work of other researchers who have challenged the segregation of cybercrimes and financial crimes in local and international regulatory discourses. This research aims to make a significant contribution to the study of cybercrimes and how they are regulated in international law. Findings The findings of the paper have confirmed that the high incidences of money laundering and cybercrimes today are partly fuelled by inherent weaknesses in the global regulatory system and partly fuelled by weaknesses at an individual state level. Many countries have enacted a raft of anti-cyber and AML legislation but this notwithstanding, these laws have not been used to stem cross-border crimes globally. This is partly explained by the fact that many enforcement institutions lack the requisite capacity to institute measures through which to implement engendered laws and policies easily. The regulatory capacity of many countries has been eviscerated by deficiencies in infrastructure and systems.
Cyber threats are attempts to secure unauthorized access to, change, or delete private information, to demand money from victims, or to disrupt business. Cybercrime includes everything from identity theft, malware threats, email and online fraud, to bank fraud. Businesses and individuals use this method to guard their data centers and other digital systems. The lack of scalability, sluggish response times, and inability to spot advanced and insider threats are among some of the problems with conventional approaches to network security. These flaws highlight the need for research to build more efficient and all-encompassing security methods to guard against the expanding variety of network attacks. Cybercriminals use AI and data poisoning, as well as model theft strategies to automate their attacks. A cyber security technique based on artificial intelligence is presented in this study for financial sector management (CS-FSM). In order to map and prevent unexpected risks from devouring a business, artificial intelligence is one of the best technologies. Using the proposed technique, cyberattack problems can be classified and solved. To ensure the security of financial sector information, algorithms such as the Enhanced Encryption Standard (EES) encrypt and decrypt data. By learning from the training data, the K-Nearest Neighbor (KNN) algorithm produces predictions. In the financial sector, it is used to detect and stop malware attacks. The proposed method increases cyber security systems' performance by increasing their defense against cyberattacks. CS-FSM enhances data privacy (18.3%), scalability (17.2%), risk reduction (13.2%), data protection (16.2%), and attack avoidance (11.2%) ratios.
Digital forensic examinations have grown in breadth and depth at a currently unsustainable rate. Digital Investigations now feature in around 90% of criminal cases, demonstrating that digital evidence is crucial to forensic investigations. Due to the high number of cases, most law enforcement units have significant backlogs of devices waiting for analysis. As the field of Digital Investigation has grown, it is no longer solely related to criminal investigations, with the techniques also supporting civil, private, and corporate activities. Given the evident challenges, it is logical that more digital forensic experts are needed to keep pace with the field's complexities and demands. Identifying what characteristics and skills make a digital forensic expert enables an evaluation to ensure that any new staff are fit for purpose. There is a growth in academic, civil, corporate, and intelligence-based activity within the field. Each area defines their standards, field scope, and expertise level. Still, as any case has the potential to become a matter of criminal investigation, surely the focus needs to be on the standards required to ensure evidence is admissible for that purpose. As expertise levels can vary, it is also necessary to challenge the level at which an expert is defined and the implications of this decision. By identifying what makes an expert in this unique forensic science area, it is possible to explore the potential challenges the field faces in obtaining, retaining, and training staff. This article is categorized under: Digital and Multimedia Science > Cybercrime Investigation
Over the last decade, userland memory forensics techniques and algorithms have gained popularity among practitioners, as they have proven to be useful in real forensics and cybercrime investigations. These techniques analyze and recover objects and artifacts from process memory space that are of critical importance in investigations. Nonetheless, the major drawback of existing techniques is that they cannot determine the origin and context within which the recovered object exists without prior knowledge of the application logic. Thus, in this research, we present a solution to close the gap between application-specific and application-generic techniques. We introduce OAGen, a post-execution and app-agnostic semantic analysis approach designed to help investigators establish concrete evidence by identifying the provenance and relationships between in-memory objects in a process memory image. OAGen utilizes Points-to analysis to reconstruct a runtime's object allocation network. The resulting graph is then fed as an input into our semantic analysis algorithms to determine objects' origin, context, and scope in the network. The results of our experiments exhibit OAGen's ability to effectively create an allocation network even for memory-intensive applications with thousands of objects, like Facebook. The performance evaluation of our approach across fourteen different Android apps shows OAGen can efficiently search and decode nodes, and identify their references with a modest throughput rate. Further practical application of OAGen demonstrated in two case studies shows that our approach can aid investigators in the recovery of deleted messages and the detection of malware functionality in post-execution program analysis.
The advent of new media has opened up opportunities and space for youths to conduct business online. Nowadays, online shopping has become one of the main activities for Internet users because it saves an individual's time and energy. Although this modern technology provides various benefits, from another point of view it can pose a threat to Internet users, especially youths such as becoming victims of online fraud. The issue of fraudulent purchases of goods online is becoming more alarming. It is increasingly happening when the country faces the Covid-19 pandemic where the use of e-commerce is increasing among the people. A holistic effort needs to be made to view, address and curb the issues of buying goods online among youths. In this context, the knowledge and practices of consumers are vital so that they will not become victims of cybercrime. Therefore, this study aims to identify the level of knowledge, attitudes and practices of youths towards information privacy and security of online purchases. This study uses a quantitative approach in which a total of 400 respondents in Klang, Selangor have answered a questionnaire on the issue of online purchasing. The results of the study found that privacy and information security issues influence the attitudes and practices of youths when making online purchases. This proves that youths who have the knowledge and awareness are able to protect them from becoming victims of cyber crime and in turn are able to face the risks of online purchases.
"Online shopping has become increasingly popular as a result of various shopping apps which are easy to use and allow users to search, explore and purchase products using their smart phones. High usage of mobile phones by millennials has resulted in an increased activity on online platforms. However, the growth of e-commerce and online transactions have also brought with it the risks of cybercrime, which has resulted in causing anxiety to online customers. Online identity theft is a mounting problem that affects individuals all over the world. Identity theft is becoming common; about 1 in 15 adults became victims of identity fraud in the year 2017 (Bellemare, 2019) and the fear of victimization has also increased with one in five victims of identity theft having experienced it more than once. Despite the growing fear of victimization discouraging online shopping, it is still growing and is considered as a preferred method by the young generation. The present study explores the determinants of online shopping and assesses the impact of fear of victimization as a mediating variable between online shopping determinants and online shopping tendency. The results of the study indicate that there is no relationship between the attitude of the consumer with online shopping tendency. The results of the study also report a negative effect of the perceived risk on the consumer's shopping tendency. However, trust was found to have an impact on online shopping tendency. The mediating effect of fear of victimization was evident in the study."
The main goal of our research was to evaluate the need of elderly computer users for special training on cybersecurity. The article highlights the results of our study which deals with the elderly and their information practices. We used 268 questionnaires from Russia, Belarus, Lithuania, Denmark and Sweden. Only members of social organizations were included in the sample. It was important to find out whether pensioners use the Internet and what for. It turned out that the spectrum of used features is quite narrow. On the other hand, there is also an increase of using Internet for public services, for paying for goods and other services. The data of this study was based on the analysis of the annual Norton Report 2013 and typical cyber crime cases where pensioners were victims. So this data led us to the conclusion about high risk of ICT use among elderly, which also accompanies opportunities for them. Governments in most European countries and the European Commission are making great efforts in the fight against cybercrime. But elderly people are not regarded as a potential risk group in the existing cybersecurity strategies. The analysis of computer literacy training programs for elderly people led us to the conclusion that cybersecurity issues are not considered or considered in a very small amount in this kind of programs. The results of our research allowed us to identify common problem areas for all countries participating in the study. The analysis showed the need of specialized training course on cyber security for the elderly and also the guidelines for its conduct.
Authorship verification is a crucial process employed to determine the authorship of a given text by analyzing distinct aspects of the writer's style, such as vocabulary, syntax, and punctuation. This process has gained significant research attention in various domains, including intellectual property rights, plagiarism detection, cybercrime investigations, copyright infringement, and forensics. While extensive studies have been conducted on multiple languages worldwide, encompassing Western European languages like Italian and Spanish, as well as Asian languages such as Bengali and Chinese, the investigation of authorship verification in Urdu has been comparatively limited, despite its status as a prominent South Asian language. This limitation can be attributed to the intricate and distinctive morphology of Urdu, which necessitates specific methodologies that cannot be directly applied in the same manner as other languages. To bridge this gap, we propose an innovative approach for authorship verification in Urdu, leveraging Convolutional Neural Networks (CNNs) with three distinct hyper-tuned parameters: ADAM, SGD, and RMSProp. To facilitate the development of this approach, we have curated a new corpus called UAVC-22, specifically tailored for Urdu authorship verification. This corpus offers enhanced robustness in terms of authors' classes and unique words. We have developed 9 authorship verification models, utilizing three different text embedding techniques, namely Word2Vec, GloVe, and FastText, we have performed a comparative analysis with traditional machine learning models such as Support Vector Machines (SVM) and Random Forest to assess the superiority and efficacy of the CNN-based approach. The optimized CNN-ADAM model with FastText achieved the highest accuracy of 98% for the Urdu dataset UAVC-22.
"Introduction: The growing offer of products and services in a virtual environment resulted in a significant increase in the volume of personal and organizational data that transit through data networks and are stored in different places, by different entities. Interactions in this environment are continuously monitored, whether for needs related to the interests of companies or organizations, or to curb the most diverse illicit practices. These monitoring activities, which configure a state of permanent vigilance, raise concerns related to the treatment of data collected and processed, with important ethical and legal implications, and which may not be properly covered by legal instruments. Objective: The research sought to identify which studies published between 2010 and 2020 deal with ethical or legal issues related to privacy in the virtual environment. Methodology: A literature review was carried out, adopting content analysis as a technique. As a technological resource, Excel & REG; and ATLAS.TI & REG; software were used. Results: Only 19 of the 39 documents found are in line with the research objective, most of which are theses and dissertations. Publications are concentrated in the second half of the analyzed period. Conclusion: The analyses showed a growing need to monitor the information circulating in the virtual environment, involving sender and receiver, covering security issues, increased volume of data, new technologies, and cybercrime among other issues. The number and distribution of articles over the analyzed period indicate that these themes require even greater investigations, contemplating broader perspectives of monitoring activities in virtual environments."
Digital evidence is critical in cybercrime investigations because it is used to connect individuals to illegal activity. Digital evidence is complicated, diffuse, volatile, and easily altered, and as such, it must be protected. The Chain of Custody (CoC) is a critical component of the digital evidence procedure. The aim of the CoC is to demonstrate that the evidence has not been tampered with at any point throughout the investigation. Because the uncertainty associated with digital evidence is not being assessed at the moment, it is impossible to determine the trustworthiness of CoC. As scientists, forensic examiners have a responsibility to reverse this tendency and officially confront the uncertainty inherent in any evidence upon which they base their judgments. To address these issues, this article proposes a new paradigm for ensuring the integrity of digital evidence (CoC documents). The new paradigm employs fuzzy hash within blockchain data structure to handle uncertainty introduced by error-prone tools when dealing with CoC documents. Traditional hashing techniques are designed to be sensitive to small input modifications and can only determine if the inputs are exactly the same or not. By comparing the similarity of two images, fuzzy hash functions can determine how different they are. With the symmetry idea at its core, the suggested framework effectively deals with random parameter probabilities, as shown in the development of the fuzzy hash segmentation function. We provide a case study for image forensics to illustrate the usefulness of this framework in introducing forensic preparedness to computer systems and enabling a more effective digital investigation procedure.
Cybercrime is a growing and worrisome problem, particularly when it involves minors. Cyberaggression among adolescents in particular can result in negative legal and psychological consequences for people involved. Therefore, it is important to have instruments to detect these incidents early and understand the problem to propose effective measures for prevention and treatment. This paper aims to design a new self-report, the Cyber-Aggression Questionnaire for Adolescents (CYBA), to evaluate the extent to which the respondent conducts aggressions through a mobile phone or the internet and analyse the factorial and criterion validity and reliability of their scores in a sample of adolescents from Asturias, Spain. The CYBA was administered to 3,148 youth aged between 12 and 18 years old along with three self-reports to measure aggression at school, impulsivity, and empathy. Regarding factorial validity, the model that best represents the structure of the CYBA consists of three factors (Impersonation, Visual-sexual Cyber-aggression, and Verbal Cyber-aggression and Exclusion) and four additional indicators of Visual Cyber-aggression-Teasing/Happy Slapping. Regarding criterion validity, the score on the CYBA correlates positively with aggression at school and impulsivity and negatively with empathy. That is the way cyber-aggression correlates with these three variables, according to previous empirical evidence. The reliability of the scores on each item and factor of the CYBA are adequate. Therefore, the CYBA offers a valid and reliable measure of cyber-aggression in adolescents. (C) 2016 Colegio Oficial de Psicologos de Madrid. Published by Elsevier Espana, S.L.U.
"The easy accessibility of stored data on the cloud storage with the use of wide range of digital devices offers both the economic and technical opportunities to its subscribers. These benefits can also be exploited by malicious users to carry out illegal activities. When such illegal activities (cybercrimes) are carried out, it is essential for digital forensic investigators to identify the malicious usages, the dynamics of the crime, identify the perpetrators or the individuals behind the crime, reconstruct the crime patterns, interpret the criminal activities and charge the personalities involved to the court of law. The sustainability of digital forensics depends on the use of appropriate technology to curb various forms of cybercrimes. During forensic investigation artificial intelligence techniques and the use of appropriate forensic tools play important roles to detect activities related to cybercrime. One of the technical challenges associated with cloud forensics investigation is the inability of forensic investigators to obtain raw data from the Cloud Service Providers (CSPs) as a result of privacy issue; this necessitates the need for client forensics. The aim of this paper is to propose a model based on traceability technique to illustrate how the extracted digital artifacts from Windows 10 and an android smartphone can be mapped and linked to the cloud storage accessed and to illustrate the patterns of the activities with 5Ws1H-based expression (what, who, where, when, why and how). The model is set out to assist forensic investigators to easily identify, track and reconstruct a post-event timeline of the activities that takes place on cloud storage with the use of client devices and thereby saves time and enhances better visualization of the crime patterns."
Phishing is the easiest way to use cybercrime with the aim of enticing people to give accurate information such as account IDs, bank details, and passwords. This type of cyberattack is usually triggered by emails, instant messages, or phone calls. The existing anti-phishing techniques are mainly based on source code features, which require to scrape the content of web pages, and on third-party services which retard the classification process of phishing URLs. Although the machine learning techniques have lately been used to detect phishing, they require essential manual feature engineering and are not an expert at detecting emerging phishing offenses. Due to the recent rapid development of deep learning techniques, many deep learning-based methods have also been introduced to enhance the classification performance. In this paper, a fast deep learning-based solution model, which uses character-level convolutional neural network (CNN) for phishing detection based on the URL of the website, is proposed. The proposed model does not require the retrieval of target website content or the use of any third-party services. It captures information and sequential patterns of URL strings without requiring a prior knowledge about phishing, and then uses the sequential pattern features for fast classification of the actual URL. For evaluations, comparisons are provided between different traditional machine learning models and deep learning models using various feature sets such as hand-crafted, character embedding, character level TF-IDF, and character level count vectors features. According to the experiments, the proposed model achieved an accuracy of 95.02% on our dataset and an accuracy of 98.58%, 95.46%, and 95.22% on benchmark datasets which outperform the existing phishing URL models.
Internet of Things security targets around secure network enabled devices that interface with one another on wireless network. IoT security is the secure component attached to the Internet of Things, and it endeavors to ensure IoT devices and systems against cybercrime. The Internet of Things is a developing pattern, with a cascade of new components hitting the market. As this grows widely there occurs an issue: When you're associated with everything, there are many approaches to hack your private data. That can make you an appealing objective for individuals who need to make a benefit off of your own information. Each associated devices you possess can include another protection concern, particularly since the majority of the interface with your cell phone. Any innovation accessible today has not come to its 100 % capacity. It generally has a hole to go. Thus, we can say that Internet of Things has a huge innovation in a world that can assist different advances with reaching its exact and finish 100 % ability also. This is widely called network intrusion which means an intruder steals one's private data over the network. IoT energizes the correspondence between devices, also popularly known as Machine-to-Machine (M2M) communication. Along with this, the physical devices can remain associated. Due to association of physical devices, there occurs network traffic which leads to denial of services and bottleneck. To overcome these consequences in wireless sensor network, data routing protocols are implemented. This proposed system emphasizes how data traffic over wireless network are regulated by data routing protocol using genetic algorithm with the traffic information collected through IoT devices.
This empirical study is an exploration of the influence methods, fear appeals, and urgency cues applied by phishers to trick or coerce users to follow instructions presented in coronavirus-themed phishing emails. To that end, a content analysis of 208 coronavirus-themed phishing emails has been conducted. We identified nine types of phishing messages crafted by phishers. Phishing emails purporting to provide information about the spread of the disease were the most common type of unsolicited emails. Authority, liking and commitment emerged as the most common influence methods. Fear appeals and urgency cues were present in almost all of the sampled phishing messages. Finally, the analysis of coronavirus-themed phishing emails revealed a shift in the modus operandi of phishers. The implications of these results are discussed in this paper.
Smishing (SMS phishing) is a cybercrime in which criminals send fraudulent messages, including malicious links, to steal the victims' private data or cause financial losses. The damage caused by smishing has become more severe, particularly with the proliferation of mobile devices. In smishing, a major difficulty faced by victims is discrimination between normal and smishing messages. To resolve this problem, we present an on-device smishing classifier based on a deep-learning model. In real-world scenarios, access to a substantial, authentic dataset is crucial. We trained and evaluated the classifier using real SMS datasets containing approximately 250,000 smishing messages and 950,000 normal messages obtained from victims in Korea. To ensure privacy, the classifier operates solely on mobile devices without externally transmitting any data. It utilizes a lightweight method that does not require significant computing power on mobile devices. We explored several models to determine a suitable model for mobile devices and optimized it using real datasets. Furthermore, our statistical analysis of actual smishing messages revealed that 98% of smishing messages are variants of previously sent messages. To address the prevalence of variant smishing messages, we propose a text evasion attack tool called EVA that is capable of generating pseudo-variant messages from a given message using an adversarial attack approach. We used this tool to evaluate and enhance the robustness of our classifier against various messages. Our classifier exhibited exceptional classification accuracy (0.99) while being lightweight (at 127 kB) and robust against variant smishing messages (attack success rate of 0.41).
"Cybercrimes have increased against Android devices due to the increased usage of Instant Messaging, Global Positioning Systems (GPS) and Webcam Applications that are built into the Android device, resulting in invasion of the victim's privacy. The existing studies demonstrate how to utilize the vulnerabilities of the Android device; however, none have proposed a comprehensive study highlighting the hacking tricks and their countermeasures. This study demonstrates how to discover and fully control the Android device using existing tools. Furthermore, it proposes a novel GPS Tracking Application. The purpose of this research is twofold: 1. To demonstrate how to disclose the victim's sensitive information after performing diverse hacking tricks; and 2. To implement countermeasures for each Android hacking tricks. The author believe that such a scenario is needed for implementing awareness among Android device users. Also, it shows Android and Instant Messaging Application developers to mitigate existing vulnerabilities, thereby enhancing security levels."
The purpose of this study is to explore the need for digital forensic investigations at universities in South Africa and to determine if a university is forensically ready to conduct effective digital forensic investigations. For this research paper, the University of Cape Town is used as a case study. The research explores the existing digital forensic investigation process at the university and determines its capability to conduct a comprehensive digital forensic investigation. Both qualitative and quantitative research methods were applied to this study using questionnaires, interviews and secondary data (documentation). Existing research within the field of digital forensics tends to focus on for-profit organisations, while educational institutions, such as the case study in this paper, have been largely ignored. Educational institutions have a strong emphasis on teaching, learning, research and support, thus the drive to increase digital forensic capability is less than at for-profit organisations. However, it is becoming increasingly important for all organisations to develop digital forensic capability due to the ever increasing rate of cybercrime. The key findings in this research established that the University of Cape Town had no demand for digital forensic investigations on campus and therefore had no formal digital forensic investigation process in place. The university had a minimal capacity to conduct these investigations and had no policy to manage digital forensic investigations. It was also found that digital forensic readiness should be developed at the university to ensure the capability to conduct digital forensic investigations. Further, some of the existing processes at the university could be adapted to develop a digital forensic readiness capability.
Digital forensics is now essential in addressing cybercrime and cyber-enabled crime but potentially it can have a role in almost every other type of crime. Given technology's continuous development and prevalence, the widespread adoption of technologies among society and the subsequent digital footprints that exist, the analysis of these technologies can help support investigations. The abundance of interconnected technologies and telecommunication platforms has significantly changed the nature of digital evidence. Subsequently, the nature and characteristics of digital forensic cases involve an enormous volume of data heterogeneity, scattered across multiple evidence sources, technologies, applications, and services. It is indisputable that the outspread and connections between existing technologies have raised the need to integrate, harmonise, unify and correlate evidence across data sources in an automated fashion. Unfortunately, the current state of the art in digital forensics leads to siloed approaches focussed upon specific technologies or support of a particular part of digital investigation. Due to this shortcoming, the digital investigator examines each data source independently, trawls through interconnected data across various sources, and often has to conduct data correlation manually, thus restricting the digital investigator's ability to answer high-level questions in a timely manner with a low cognitive load. Therefore, this research paper investigates the limitations of the current state of the art in the digital forensics discipline and categorises common investigation crimes with the necessary corresponding digital analyses to define the characteristics of the next-generation approach. Based on these observations, it discusses the future capabilities of the next-generation unified forensics analysis tool (U-FAT), with a workflow example that illustrates data unification, correlation and visualisation processes within the proposed method.
Phishing is one of the simplest ways in cybercrime to hack the reliable data of users such as passwords, account identifiers, bank details, etc. In general, these kinds of cyberattacks are made at users through phone calls, emails, or instant messages. The anti-phishing techniques, currently under use, are mainly based on source code features that need to scrape the webpage content. In third party services, these techniques check the classification procedure of phishing Uniform Resource Locators (URLs). Even though Machine Learning (ML) techniques have been lately utilized in the identification of phishing, they still need to undergo feature engineering since the techniques are not well-versed in identifying phishing offenses. The tremendous growth and evolution of Deep Learning (DL) techniques paved the way for increasing the accuracy of classification process. In this background, the current research article presents a Hunger Search Optimization with Hybrid Deep Learning enabled Phishing Detection and Classification (HSOHDL-PDC) model. The presented HSOHDL-PDC model focuses on effective recognition and classification of phishing based on website URLs. In addition, SOHDL-PDC model uses character-level embedding instead of word-level embedding since the URLs generally utilize words with no importance. Moreover, a hybrid Convolutional Neural Network-Long Short Term Memory (HCNN-LSTM) technique is also applied for identification and classification of phishing. The hyperparameters involved in HCNN-LSTM model are optimized with the help of HSO algorithm which in turn produced improved outcomes. The performance of the proposed HSOHDL-PDC model was validated using different datasets and the outcomes confirmed the supremacy of the proposed model over other recent approaches.
Reducing physical distances within cyberspace allowed information to travel almost instantly while the international cybercrime phenomenon has risen almost inevitably. Most of the slips in cybersecurity come into being due to a poor management of the online environment, due to a lack of knowledge within the technological features in current use and due to the improper way to work with data. This issue leads to the necessity of outlining a cybersecurity culture along with respecting the rights related to online privacy. There is not yet a universally valid solution to the complex problem of cybersecurity, the field being very dynamic, the technological progress - very fast, while the individual cannot find enough self protection against organizations specialized in fraud, fake news dissemination, data retrieval, storage and destruction. The attitude towards cybersecurity does not represent a special case, it only completes and supports individual's attitude towards society. We must yet mention that these alarming scenarios are backed up by the business environment which looks at selling security solutions in growing numbers. Malware protection programs represent one of the most important measures to ensure cybersecurity measures. The dynamics of the internet and the frequency of attacks impose the use of efficient methods that identify suspicious behaviour. The situations in which signature detection fails, machine learning use can lead to acceptable results. This type of computational learning (supervised learning) implies using a set of data that make the ins and outs to be known, while the resulting model determines the connections and makes predictions over a new set of data. The present paper showcases the research done so far within The Integrated Software Platform for the mobile malware analysis.
Disaster Recovery and Business Continuity are ` hot topics' these days. Every organisation should have a proper response and recovery plan in place -whether it operates in public or private sector, and whether it is small or large. In particular since recent adverse events such as the Japan Tsunamis, potential Pandemic Flu outbreaks, increased risk of Cybercrime and IT system failures. And many organisations do have plans, in order to be able to confidently respond to crisis situations that impact on the availability of their building, their staff, their IT systems, their voice communications or perhaps one of their critical external suppliers. However, making a plan is not enough. and teaching people the content of the plan and their role in the event of a disaster doesn't provide the desired results if only done by means of written instructions, traditional presentations/hand-outs and e-learning methods. A more practical approach is required for staff and other stakeholders to feel emotionally, physically and psychologically prepared for a disaster or major incident. Disaster drills and simulations are a hands-on way to let participants experience their role and ` be in it' for a period of time, before a real disaster strikes. It has proven to be the only lasting method that instils clear-headedness, courage and creativity, which will be invaluable in a real-life incident. This presentation will show you some of the key ingredients to successful disaster simulations, including the use of Audio Visual tools, social media and role plays. The presenter will share her global experiences from Australia, Africa, Asia and Europe gained during her role as Business Continuity trainer and facilitator.
Soft set theory is the most developed tool for demonstrating uncertain, vague, not clearly defined objects in a parametric manner. Bipolar uncertainty incorporates a significant role in apprehending discrete and applied mathematical modeling and decision analysis of various physical systems. Graphical and algebraic structures can be studied more precisely when bipolar parametric linguistic properties are to be dealt with, emphasizing the need of a bipolar mathematical approach with soft set theory. In this research paper, we apply the powerful technique of bipolar fuzzy soft sets to hypergraphs and present a novel framework of bipolar fuzzy soft hypergraphs. We elaborate various methods for the construction of bipolar fuzzy soft hypergraphs. We discuss the concept of linearity in bipolar fuzzy soft hypergraphs and study isomorphism properties of bipolar fuzzy soft line graphs of bipolar fuzzy soft hypergraphs, dual and 2-section of bipolar fuzzy soft hypergraphs. We present an application of bipolar fuzzy soft information for analyzing chat conversations of pedophiles and detecting online child grooming cases.
Currently, within the world, cybercrime is becoming increasingly rampant-often targeting civil infrastructure like power stations and other critical systems. A trend that is being noticed with these attacks is their increased use of embedded devices in denial-of-service (DoS) attacks. This creates a substantial risk to systems and infrastructures worldwide. Threats to embedded devices can be significant, and network stability and reliability can suffer, mainly through the risk of battery draining or complete system hang. This paper investigates such consequences through simulations of excessive loads, by staging attacks on embedded devices. Experimentation within Contiki OS focused on loads placed on physical and virtualised wireless sensor network (WSN) embedded devices by launching DoS attacks and by exploiting the Routing Protocol for Low Power and Lossy Networks (RPL). Results from these experiments were based on the metric of power draw, mainly the percentage increase over baseline and the pattern of it. The physical study relied on the output of the inline power analyser and the virtual study relied on the output of a Cooja plugin called PowerTracker. This involved experiments on both physical and virtual devices, and analysis of the power draws characteristics of WSN devices with a focus on embedded Linux platforms and Contiki OS. Experimental results provide evidence that peak power draining occurs with a malicious-node-to-sensor device ratio of 13-to-1. Results show a decline in power usage with a more expansive 16-sensor network after modelling and simulating a growing sensor network within the Cooja simulator.
Fin-Tech is the merging of finance and technology, to be considered a key term for technology-based financial operations and money transactions as far as Fin-Tech is concerned. In the massive field of business, mobile money transaction security is a great challenge for researchers. The user authentication schemes restrict the ability to enforce the authentication before the account can access and operate. Although authentication factors provide greater security than a simple static password, financial transactions have potential drawbacks because cybercrime expands the opportunities for fraudsters. The most common enterprise challenge is mobile-based user authentication during transactions, which addresses the security issues against fraudsters. The confirmation of a user legitimation before the money transaction is highlighted by mechanisms and technologies from previous studies that may be helpful in user authentication. This study aims to identify the technologies for user authentication and the opportunity for their transformation to mobile money transaction security despite having all the legally required data for a transaction. This proposed review has identified the role of multifactor authentication techniques for authentication to mitigate the risk of fraudulent transactions-the analysis through 92 articles taken from famous publishers. The most relevant articles address authentication problems, of which 54 percent describe money transaction security, and the rest highlight the supporting technology for user authentication. The study platform described the technology-based approaches with the appreciation of new ideas for secure money transactions. The QR code and multifactor authentication are key terms that increased security by 46%. In addition, this also ensures the user is legitimate using advanced technologies and algorithms to predict and discover transaction risks and discourage fraudsters from trying.
Outlier (also known as anomaly) detection technology is widely applied to many areas, such as diagnosing diseases, evaluating credit, and investigating cybercrime. Recently, several studies, based on frequent itemset mining (FIM), have been proposed to detect outliers in categorical data. For efficiency, these FIM-based studies pruned (ignored) the majority of data by either imposing a threshold or restricting the length of the pattern or both, and they further adopted the limited information to evaluate observations. In spite of high efficiency, such a pruning approach encounters the problem of distortion, i.e., the accuracy decreases to a low level of discernment or even causes the contrary judgment in certain cases. In this paper, we introduce the concept relative patterns discovery from a new perspective on association analysis. To efficiently explore the relative patterns, we devise a hash-index-based intersecting approach (called the HA). Based on the knowledge of relative patterns, we propose an unsupervised approach (called the UA) to evaluate which observations are anomalous. Instead of using the limited information, our method can differentiate the features of observations without the problem of distortion. The results of the empirical investigation, conducted with eight real-world datasets on the UCI Machine Learning Repository, demonstrate that our method generally outperforms the previous studies not only in accuracy but also inefficiency. We also demonstrate that the execution complexity of our method is significantly efficient, especially in high-dimensional data. Furthermore, our method can represent a natural panorama of data, which is appropriate in controlled experiments for discovering more decisive factors in outlier detection. (C) 2014 Elsevier B.V. All rights reserved.
Developing a long-lasting, secure Industry 4.0 system presents a significant challenge for businesses and other interested parties. Industrial control systems (ICSs) are particularly vulnerable to cybercrime because of the operating systems' excessive availability and high robustness requirements. This research investigates five graph-theory-based measures to evaluate the robustness of industrial network topologies against three centrality-based attacks and one random attack. Experiments are conducted to examine the three levels of the ICS network topology, from the field devices to the controllers and the enterprise devices. The results are twofold. On the one hand, the closeness-based attack is the most harmful since it has the highest destructive potential and needs to attack only half of the total nodes in the network to reach the lowest robustness level. The betweenness-based attack follows closely in terms of destruction, whereas the degree-based attack is less destructive but rapidly degrades the robustness of the network. On the other hand, the flow robustness measure provides the best performance in the presence of any of the studied attacks, showing strong perception of robustness reduction when only one percent of the total nodes in the network are attacked. For this reason, the flow robustness measure is suitable to identify and locate the targeted attacks at their early stages, preventing them from becoming more catastrophic. Finally, the results suggest that the industrial network security system should combine at least two measures to ensure robustness against the most destructive attacks and their early-stage detection. The research also confirmed the results by implementing attacks and measures on a real gas transmission network.
Most of the sophisticated attacks in the modern age of cybercrime are based, among other things, on specialized phishing campaigns. A challenge in identifying phishing campaigns is defining a classification of patterns that can be generalized and used in different areas and campaigns of a different nature. Although efforts have been made to establish a general labeling scheme in their classification, there is still limited data labeled in such a format. The usual approaches are based on feature engineering to correctly identify phishing campaigns, exporting lexical, syntactic, and semantic features, e.g., previous phrases. In this context, the most recent approaches have taken advantage of modern neural network architectures to record hidden information at the phrase and text levels, e.g., Long Short-Term Memory (LSTM) and Convolutional Neural Networks (CNNs). However, these models lose semantic information related to the specific problem, resulting in a variation in their performance, depending on the different data sets and the corresponding standards used for labeling. In this paper, we propose to extend word embeddings with word vectors that indicate the semantic similarity of each word with each phishing campaigns template tag. These embedded keywords are calculated based on semantic subfields corresponding to each phishing campaign tag, constructed based on the automatic extraction of keywords representing these tags. Combining general word integrations with vectors is calculated based on word similarity using a set of sequential Kalman filters, which can then power any neural architecture such as LSTM or CNN to predict each phishing campaign. Our experiments use a data indicator to evaluate our approach and achieve remarkable results that reinforce the state-of-the-art.
"Internet users are continually exposed to phishing as cybercrime in the 21st century. The objective of phishing is to obtain sensitive information by deceiving a target and using the information for financial gain. The information may include a login detail, password, date of birth, credit card number, bank account number, and family-related information. To acquire these details, users will be directed to fill out the information on false websites based on information from emails, adverts, text messages, or website pop-ups. Examining the website's URL address is one method for avoiding this type of deception. Identifying the features of a phishing website URL takes specialized knowledge and investigation. Machine learning is one method that uses existing data to teach machines to distinguish between legal and phishing website URLs. In this work, we proposed a method that combines correlation and recursive feature elimination to determine which URL characteristics are useful for identifying phishing websites by gradually decreasing the number of features while maintaining accuracy value. In this paper, we use two datasets that contain 48 and 87 features. The first scenario combines power predictive score correlation and recursive feature elimination; the second scenario is the maximal information coefficient correlation and recursive feature elimination. The third scenario combines spearman correlation and recursive feature elimination. All three scenarios from the combined findings of the proposed methodologies achieve a high level of accuracy even with the smallest feature subset. For dataset 1, the accuracy value for the 10 features result is 97.06%, and for dataset 2 the accuracy value is 95.88% for 10 features."
The increasing levels of criminal media being shared in peer-to-peer (P2P) networks pose a significant challenge to law enforcement agencies. One of the main priorities for P2P investigators is to identify cases where a user is actively engaged in the production of child sexual abuse (CSA) media - they can be indicators of recent or on-going child abuse. Although a number of P2P monitoring tools exist to detect paedophile activity in such networks, they typically rely on hash value databases of known CSA media. As a result, these tools are not able to adequately triage the thousands of results they retrieve, nor can they identify new child abuse media that are being released on to a network. In this paper, we present a new intelligent forensics approach that incorporates the advantages of artificial intelligence and machine learning theory to automatically flag new/previously unseen CSA media to investigators. Additionally, the research was extensively discussed with law enforcement cybercrime specialists from different European countries and Interpol. The approach has been implemented into the iCOP toolkit, a software package that is designed to perform live forensic analysis on a P2P network environment. In addition, the system offers secondary features, such as showing on-line sharers of known CSA files and the ability to see other files shared by the same GUID or other IP addresses used by the same P2P client. Finally, our evaluation on real CSA case data shows high degrees of accuracy, while hands-on trials with law enforcement officers demonstrate the toolkit's complementarity to extant investigative workflows. (C) 2016 The Authors. Published by Elsevier Ltd.
Detection of network attacks is a challenging task, especially concerning detection coverage and timeliness. The defenders need to be able to detect advanced types of attacks and minimize the time gap between the attack detection and its mitigation. To meet these requirements, we present a stream-based IP flow data processing application for real-time attack detection using similarity search techniques. Our approach extends capabilities of traditional detection systems and allows to detect not only anomalies and attacks that match exactly to predefined patterns but also their variations. The approach is demonstrated on detection of SSH authentication attacks. We describe a process of patterns definition and illustrate their usage in a real-world deployment. We show that our approach provides sufficient performance of IP flow data processing for real-time detection while maintaining versatility and ability to detect network attacks that have not been recognized by traditional approaches.
"Phishing is a type of cybercrime in which cyber-attackers pose themselves as authorized persons or entities and hack the victims' sensitive data. E-mails, instant messages and phone calls are some of the common modes used in cyberattacks. Though the security models are continuously upgraded to prevent cyberattacks, hackers find innovative ways to target the victims. In this background, there is a drastic increase observed in the number of phishing emails sent to potential targets. This scenario necessitates the importance of designing an effective classification model. Though numerous conventional models are available in the literature for proficient classification of phishing emails, the Machine Learning (ML) techniques and the Deep Learning (DL) models have been employed in the literature. The current study presents an Intelligent Cuckoo Search (CS) Optimization Algorithm with a Deep Learning-based Phishing Email Detection and Classification (ICSOA-DLPEC) model. The aim of the proposed ICSOA-DLPEC model is to effectually distinguish the emails as either legitimate or phishing ones. At the initial stage, the pre-processing is performed through three stages such as email cleaning, tokenization and stop-word elimination. Then, the N-gram approach is; moreover, the CS algorithm is applied to extract the useful feature vectors. Moreover, the CS algorithm is employed with the Gated Recurrent Unit (GRU) model to detect and classify phishing emails. Furthermore, the CS algorithm is used to fine-tune the parameters involved in theGRUmodel. The performance of the proposed ICSOA-DLPEC model was experimentally validated using a benchmark dataset, and the results were assessed under several dimensions. Extensive comparative studies were conducted, and the results confirmed the superior performance of the proposed ICSOA-DLPEC model over other existing approaches. The proposed model achieved a maximum accuracy of 99.72%."
The increasing number of cyber-attacks has become a global problem for companies, public institutions, even for governments and for each particular user. Cybercrime causes damage of about 750 billion EUR every year in Europe alone. Thus, ICT security is nowadays a major concern, increasing the demand for specialists in this domain. Currently, universities do not produce enough graduates with strong network security skills able to defend against complex cyber-attacks. Recently new EU approved ERASMUS+ project (DECAMP) addresses innovatively this educational aspect. DECAMP brings together within a framework of an international partnership 6 EU universities and 3 associated partners. The project is set up to create 6 online courses with integrated heterogeneous virtual hands-on lab environments covering ICT Security issues of various application areas. Each partner creates a course corresponding to its expertise. These courses can be accessed by all the students, professors and researchers of the universities within the DECAMP consortium as well as from other EU universities. The core of the DECAMP project is an online distributed virtual campus. The paper describes the procedure developed for controlling a secure access of various types of users to the platform of eLearning course materials. The solution complies on one hand with all the differences of enrollment procedures installed at each particular EU University to verify the type of user (student, teacher, researcher, etc.) requiring access to the institution's resources. On the other hand the developed mechanism allows the users to obtain a single sign on (SSO) account which supports their accesses to all distributed modules of the platform, placed at the corresponding universities which create and maintain them. A prototype system that has been deployed for testing the proposed solutions is also presented.
"Tourism is one of the most dynamically developing areas of the world economy, in which rural areas are getting more and more importance, since they are popular target areas of tourists. It is an exciting task to examine the link between civil lifestyle, tourism and security. The three major new types of challenges - economic recession, cybercrime and terrorism, which threaten tourism and the industries involved - affect many fields of science. All kinds of safety begin with the individual and family security. This is followed by social security, then followed by the protection of national security and the security of the world and humanity. It is scientifically proven that safety is one of the basic human needs; there is the need for protection. In our paper we review and analyze some of the pivotal problems associated with tourism safety. This paper is only the first part of our broad research, in which we will also study the experiences of the civil population and tourists in Hungary and their sense of security. Research questions will be: 1. The European migration crisis reduced the sense of security both in population and in tourists. 2. Increased security preparedness will give a greater sense of security for both the civilian population and tourists. 3. The emergence of European migration crisis affected the Hungarians differently than the foreigners in their free movement in regards of their decisions for tourism purposes. The aim of our research is to see how much safety issues influence the general living and specific tourism potentials of the countryside, whether these issues have such negative impacts that halt the sustainable and smart development of rural settlements. In this paper we intended to call the attention on the relationship between rural development and security."
We live in an era of unprecedented technology. Millions of users depend on information technology to carry out their daily lives and large-scale commercial and industrial operations are no exception. At the same time, the rapidly growing interconnectivity of IT systems and the surge in cybercrime since the pandemic have rendered industry-standard hardware and software components increasingly vulnerable to malicious attacks. Cyber defense is a coordinated act of resistance that intends to understand the capabilities and motives of attackers in order to secure our country's data and more importantly, the livelihoods of our citizens. This research aims to contribute to the progress of cybersecurity and defense technology as a whole by focusing on a dynamic aspect of malware: digital unwanted advertisements. It presents a novel approach to automating the analysis of malicious content on the internet by web scraping ads of the popular search engine Google to extract relevant data (URL, Company, Title, Product Desc.), building machine learning models (supervised & unsupervised) to classify and make predictions on that data, and creating a web application for end users to access. The results show that our tool can detect trends within the features with limited false positives, paving the way for us to make predictions on whether the advertisements are desirable or unwanted. The research concludes that in this time and age, it is extremely important to protect against fraud, especially by adhering to cybersecurity's best practices and to think about threats in more global terms. Our hope with this research is to prompt action to ensure society continues to improve in IT resilience.
The internet has had a major impact on how information is shared within supply chains, and in commerce in general. This has resulted in the establishment of information systems such as e-supply chains amongst others which integrate the internet and other information and communications technology (ICT) with traditional business processes for the swift transmission of information between trading partners. Many organisations have reaped the benefits of adopting the eSC model, but have also faced the challenges with which it comes. One such major challenge is information security. Digital forensic readiness is a relatively new exciting field which can prepare and prevent incidents from occurring within an eSC environment if implemented strategically. With the current state of cybercrime, tool developers are challenged with the task of developing cutting edge digital forensic readiness tools that can keep up with the current technological advancements, such as (eSCs), in the business world. Therefore, the problem addressed in this paper is that there are no DFR tools that are designed to support eSCs specifically. There are some general-purpose monitoring tools that have forensic readiness functionality, but currently there are no tools specifically designed to serve the eSC environment. Therefore, this paper discusses the limitations of current digital forensic readiness tools for the eSC environment and an architectural design for next-generation eSC DFR systems is proposed, along with the system requirements that such systems must satisfy. It is the view of the authors that the conclusions drawn from this paper can spearhead the development of cutting-edge next-generation digital forensic readiness tools, and bring attention to some of the shortcomings of current tools.
Stylometry is a method for identifying anonymous authors of anonymous texts by analyzing their writing style. While stylometric methods have produced impressive results in previous experiments, we wanted to explore their performance on a challenging dataset of particular interest to the security research community. Analysis of underground forums can provide key information about who controls a given bot network or sells a service, and the size and scope of the cybercrime underworld. Previous analyses have been accomplished primarily through analysis of limited structured metadata and painstaking manual analysis. However, the key challenge is to automate this process, since this labor intensive manual approach clearly does not scale. We consider two scenarios. The first involves text written by an unknown cybercriminal and a set of potential suspects. This is standard, supervised stylometry problem made more difficult by multilingual forums that mix 133t-speak conversations with data dumps. In the second scenario, you want to feed a forum into an analysis engine and have it output possible doppelgangers, or users with multiple accounts. While other researchers have explored this problem, we propose a method that produces good results on actual separate accounts, as opposed to data sets created by artificially splitting authors into multiple identities. For scenario 1, we achieve 77% to 84% accuracy on private messages. For scenario 2, we achieve 94% recall with 90% precision on blogs and 85.18% precision with 82.14% recall for underground forum users. We demonstrate the utility of our approach with a case study that includes applying our technique to the Carders forum and manual analysis to validate the results, enabling the discovery of previously undetected doppelganger accounts.
Vice President for Promoting our European Way of Life Margaritis Schinas, stated at the time of the adoption of Europol's amendment that Europol is a true example of where EU action helps protect us all. Today's agreement will give Europol the right tools and safeguard to support police forces in analysing big data to investigate crime and in developing pioneering methods to tackle cybercrime. While some characterized the changes as an achievement for the adaptability and operational role of Europol, others argued that it undermines fundamental rights and weakens data protection. This paper analyses the amendments made to the Regulation and explores Europol's increasing role of in national investigations and the associated dangers of it. The paper starts with a historical analysis of Europol's legal framework and role in national criminal investigations, before diving into the core of the Regulation. After 2022, Europol supports Member States' investigations in many ways. First, through the continuous retention of large and complex datasets, which was strongly criticized by NGOs and the EDPS. Second, through the transformation of Europol into the information hub and broker for the exchanges of data with private parties. Third, more indirectly, through Europol's support of research and innovation projects, for national authorities to use and explore new technologies in their work. However, these amendments are not without dangers. The Regulation of 2022 pushes the boundaries of Europol's competences further, by circumventing existing limits and questioning the legality of the operations. The stronger role of Europol lacks sufficient safeguards and efficient oversight. This is highly problematic considering the impact Europol may have on national investigations, and as a result on the situation of individuals.
"Along with advances in information technology, cybercrime techniques also increased. There are several forms of attacks on data and information, such as hackers, crackers, Trojans, etc. The Symantec Intelligence report edition on August 2012 indicated that the attacker selected the target of attacks. The type of data is valuable and confidential. The Hackers selected the target to attack or steal interest information the first and they did not just taking random from a large amount of data. This indication worried because hackers stealing the data more planned. Therefore, today many systems reinforced with various efforts to maintain data security and overcome these attacks. Necessary methods to secure electronic messages that do not fall on those who are not authorized. One alternative is steganography. Cryptography and Steganography are the two major techniques for secret communication. Cryptography converts information from its original form (plaintext) into unreadable form (cipher text); where as in steganography is the art of hiding messages within other data without changing the data to it attaches, so data before and after the process of hiding almost look like the same. There are many different techniques are available for cryptography and steganography. The cryptography suspicion against disguised message is easily recognizable, because of the message disguised by changing the original message becomes as if illegible. While further reduce suspicion steganography disguised as a message hidden in the file. The research designed the application of steganography using Least Significant Bit (LSB) in which the previous message is encrypted using the Advanced Encryption Standard algorithm (AES) and it can restore the previously hidden data. The messages in this form application and hidden text on media digital image so as not to arouse suspicion. The result of research shown the steganography is expected to hide the secret message, so the message is not easy to know other people who are not eligible."
The digital revolution we are witnessing nowadays goes hand in hand with a revolution in cybercrime. This irrefutable fact has been a major reason for making digital forensic (DF) a pressing and timely topic to investigate. Thanks to the file system which is a rich source of digital evidence that may prove or deny a digital crime. Yet, although there are many tools that can be used to extract potentially conclusive evidence from the file system, there is still a need to develop effective techniques for evaluating the extracted evidence and link it directly to a digital crime. Machine learning can be posed as a possible solution looming in the horizon. This article proposes an Enhanced Multiclass Support Vector Machine (EMSVM) model that aims to improve the classification performance. The EMSVM suggests a new technique in selecting the most effective set of parameters when building a SVM model. In addition, since the DF is considered a multiclass classification problem duo to the fact that a file system might be accecced by more than one application, the EMSVM enhances the class assignment mechanism by supporting multi-class classification. The article then investigates the applicability of the proposed model in analysing incriminating digital evidence by inspecting the historical activities of file systems to realize if a malicious program manipulated them. The results obtained from the proposed model were promising when compared to several machine-learning algorithms. (c) 2019 Production and hosting by Elsevier B.V. on behalf of King Saud University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).
"Issues related to information culture evoke a natural interest of Ukrainian and foreign researchers. In addition to purely technical aspects and the development of competency characteristics, the problem of the development of students' information culture and their personal characteristics is acquiring particular relevance. Virtual space has significant advantages in terms of the speed of information distribution, communication opportunities, and the exchange of scientific data. At the same time, threats are evident, in particular the growth of cybercrime, aggression, dependence, and manipulation. This is particularly obvious in the youth environment. The creation of national elite in modern universities, the openness of the information space, globalization, and the growth of competition in the labor market require operational information for specification of educational policy and adoption of optimal managerial decisions. It becomes increasingly clear that there is a contradiction between the low level of information and analytical skills of students and the high level of requirements for the organization of work with computer technology; sufficient level of theoretical and practical knowledge and low level of skills necessary for work with information technologies; sufficient level of professional training and low level of readiness to use computer technologies in educational activities. Information culture is considered as one of the facets of the universal culture associated with the social nature of man and is the product of its various creative abilities. The need for original specialized courses, which allow quick response to new challenges and improve the quality of professional training of students is constantly increasing. The article outlines the preconditions for improving students' information culture in view of the experience of foreign countries and Ukraine."
"The cyberspace depicts an increasing number of difficulties related to security, especially in healthcare. This is evident from how vulnerable critical infrastructures are to cyberattacks and are unprotected against cybercrime. Users, ideally, should maintain a good level of cyber hygiene, via regular software updates and the development of unique passwords, as an effective way to become resilient to cyberattacks. Cyber security breaches are a top priority, and most users are aware that their behaviours may put them at risk; however, they are not educated to follow best practices, such as protecting their passwords. Mass cyber education may serve as a means to offset poor cyber security behaviours; however, mandatory education becomes a questionable point if the content is not focused on human factors, using human-centric approaches and taking into account end users' behaviours, which is currently the case. The nature of the present paper is largely exploratory, and the purpose is two-fold: To present and explore the cyber hygiene definition, context and habits of end users in order to strengthen our understanding of users. Our paper reports the best practices that should be used by healthcare organisations and healthcare professionals to maintain good cyber hygiene and how these can be applied via a healthcare use case scenario to increase awareness related to data privacy and cybersecurity. This is an issue of great importance and urgency considering the rapid increase of cyberattacks in healthcare organisations, mainly due to human errors. Further to that, based on human-centric approaches, our long-term vision and future work involves facilitating the development of efficient practices and education associated with cybersecurity hygiene via a flexible, adaptable and practical framework."
Over the past few years, the number of crimes related to the worldwide diffusion of digital devices with large storage and broadband network connections has increased dramatically. In order to better address the problem, law enforcement specialists have developed new ideas and methods for retrieving evidence more effectively. In accordance with this trend, our research aims to add new pieces of information to the automated analysis of evidence according to Machine Learning-based post mortem triage. The scope consists of some copyright infringement court cases coming from the Italian Cybercrime Police Unit database. We draw our inspiration from this low level crime which is normally sat at the bottom of the forensic analyst's queue, behind higher priority cases and dealt with the lowest priority. The present work aims to bring order back in the analyst's queue by providing a method to rank each queued item, e. g. a seized device, before being analyzed in detail. The paper draws the guidelines for drive-under-triage classification (e. g. hard disk drive, thumb drive, solid state drive etc.), according to a list of crime-dependent features such as installed software, file statistics and browser history. The model, inspired by the theory of Data Mining and Machine Learning, is able to classify each exhibit by predicting the problem dependent variable (i.e. the class) according to the aforementioned crime-dependent features. In our research context the class variable identifies with the likelihood that a drive image may contain evidence concerning the crime and, thus, the associated item must receive an high (or low) ranking in the list.
This paper investigates the generation of personal biometric data, encryption of data, and the storage of such data in a cloud based database. The data generator simulates the movement and action of an active person for detailed analysis in other projects. Included is a report on the performance of the data generator implementing different data encryption algorithms and storage in NoSQL document databases. Additionally this paper contains results of experiments from tests on the writing performance of the NoSQL databases (MongoDB, Couchbase, Elasticsearch and RethinkDB) using the emulated data.
"Currently, the world of digital forensics analysis is dividing into multiple branches, with each branch needing its own specific set of skills and competences. A growing number of best practices for analysing the different media are reported in many publications, such as the Scientific Working Group on Digital Evidence website [1]. The forensics analysis of a video, for example, shares nothing in common (except scientific procedure) with the examination of a mobile content extraction. This ongoing fragmentation of the digital forensics specialisations impacts on the ability to acquire a comprehensive global overview of the entire investigation. The range of data is vast, and thus, selecting the right insight is vital, for example, using a known procedure or tested tool to conduct the investigation, as described in the Computer Forensic Tool Testing Program (CFTT NIST) documentation [2]. CDR (Call Detail Record) analysis is a great source of information, but is sometimes overlooked; the volume of the data seems overwhelming and the lack of a standard model could be discouraging. Combining this massive amount of useful data with other data sources, such as GPS positions, BTS log files, mobile extractions, highway and traffic logs, will provide the investigator with a complete picture of the events. CDR analysis can be the keystone of the digital forensics world, pulling all the elements together to effectively counteract anti-forensics."
Purpose The purpose of this paper is to critically analyse research surrounding mandate fraud and to devise a crime script identifying the crime commission process. It is hoped this will assist in standardising investigation of mandate fraud by law enforcement. Design/methodology/approach The design of this paper follows on from the approach taken by van Hardeveld et al. (2017) in identifying a cybercrime script for carding activity. The current environment surrounding the investigation of digital fraud in the UK is examined through a review of the existing literature. Analysis of the crime commission process for mandate fraud is then outlined in a potential crime script identifying possible intervention points for law enforcement. Findings UK law enforcement's current response to digital fraud is struggling to provide positive outcomes for victims. There is inconsistency in the investigative approach and practical issues surrounding how the digital fraud problem is tackled. Changing the emphasis of digital fraud investigation to focus on the money laundering networks based in the UK also needs to be considered. Standardising investigation into digital fraud by mapping out digital criminality using crime scripts and routine activity theory could be beneficial for law enforcement. The results of this process could also assist in effectively identifying where law enforcement resources may be best deployed to solve some of the practical issues highlighted. Originality/value There is little literature directly focused on a crime script for mandate fraud. This is despite it being a significant contributor to fraud losses in the UK. For law enforcement, both digital and financial investigation skill sets are required to investigate such offences. Therefore, mapping the crime commission process has significant value for clearly identifying investigative intervention points.
This study examines whether macro-level opportunity indicators affect cyber-theft victimization. Based on the arguments from criminal opportunity theory, exposure to risk is measured by state-level patterns of internet access (where users access the internet). Other structural characteristics of states were measured to determine if variation in social structure impacted cyber-victimization across states. The current study found that structural conditions such as unemployment and non-urban population are associated with where users access the internet. Also, this study found that the proportion of users who access the internet only at home was positively associated with state-level counts of cyber-theft victimization. The theoretical implications of these findings are discussed.
The prevalence of cybercrime has emerged as a critical issue in contemporary society because of its far-reaching financial, social, and psychological implications. The negative effects of cyber-attacks extend beyond financial losses and disrupt people's lives on social and psychological levels. Conventional practice involves cyber experts sourcing data from various outlets and applying personal discernment and rational inference to manually formulate cyber intelligence specific to a country. This traditional approach introduces personal bias towards the country-level cyber reports. However, this paper reports a novel approach where country-level cyber intelligence is automatically generated with artificial intelligence (AI), employing cyber-related social media posts and open-source cyber-attack statistics. Our innovative cyber threat intelligence solution examined 37,386 tweets from 30,706 users in 54 languages using sentiment analysis, translation, term frequency-inverse document frequency (TF-IDF), latent Dirichlet allocation (LDA), N-gram, and Porter stemming. Moreover, the presented study utilized 238,220 open-intelligence cyber-attack statistics from eight different web links, to create a historical cyber-attack dataset. Subsequently, AI-based algorithms, like convolutional neural network (CNN), and exponential smoothing were used for AI-driven insights. With the confluence of the voluminous Twitter-derived data and the array of open-intelligence cyber-attack statistics, orchestrated by the AI-driven algorithms, the presented approach generated seven-dimensional cyber intelligence for Australia and China in complete automation. Finally, the topic analysis on the cyber-related social media messages revealed seven main themes for both Australia and China. This methodology possesses the inherent capability to effortlessly engender cyber intelligence for any country, employing an autonomous modality within the realm of pervasive computational platforms.
"One of the important factors for popularity and addiction of Facebook is due to the daily installments and use of third party apps. The new way of cybercrime is introduced by hackers in online social networks (OSNs). Hackers identified various ideas to damage the computer systems and methods to forward spam messages for advertisement purpose in illegal way. As popularity of third-party apps platform and deploying malicious applications increases so hackers have started taking advantages. Online social media is considered as a rich source of information as it provides the good quality content; however consumption of poor quality content can degrade user experience, and have unsuitable impact in the real world. In this paper we proposed system which develop tool that can predict malicious application with better results as compare to existing system. Proposed system implements the classification technique. This technique is used for identifying malicious app. System added parameters like number of user rating and user review to generate better results. That is user rating and Description Content check-up. Every app has few reviews as a feedback from user side, and we consider user mention all related to that app which is as per its experience. We consider all that reviews for checking app is malicious or benign. Existing system says that malicious app doesn't give any description but what about such malicious app which gives description and such benign app which doesn't give description...?? Its completely opposite of accuracy result. So in this paper we consider these two points as new feature of FRAppE for improving result accuracy. By including these parameters the proposed framework produces better result prediction."
Advanced Persistent Threats (APTs) use sophisticated cyberattacks to disrupt a nation's critical infrastructure. Conventional cyberattack management is response-driven, which (while important) is ineffective, especially in managing APTs. There is an immediate need for anticipatory defense measures that reflect the adaptive nature of this new breed of adversaries. This paper identifies five main research areas that need immediate attention. Using a criminological framework and empirical evidence of observations and interviews done at Industrial Control Systems Computer Emergency Response Team's (ICS-CERT) Red/Blue cybersecurity training exercise held at Idaho National Laboratory, this paper argues that understanding how adversaries adapt at various points in the intrusion chain is crucial in profiling APTs and developing anticipatory cybersecurity measures. The paper offers recommendations for further research and the relevance of multidisciplinary collaboration.
Five decades ago advances in integrated circuits and time-sharing operating systems made interactive use of computers economically feasible. Visions of man-computer symbiosis and the augmentation of man's intellect became realistic research objectives. The initial focus was on facilitating interactivity through improved interface technology, and supporting its application through good practices based on experience and psychological principles. Within a decade technology advances made low cost personal computers commonly available in the home, office and industry, and these were rapidly enhanced with software that made them attractive in a wide range of applications from games to office automation and industrial process control. Within three decades the Internet enabled human-computer interaction to extend across local, national and international networks, and, within four, smartphones and tablets had made access to computers and networks almost ubiquitous to any person, at any time and any place. Banking, commerce, institutional and company operations, utility and government infrastructures, took advantage of, and became dependent on, interactive computer and networking capabilities to such an extent that they have now been assimilated in our society and are taken for granted. This hyperconnectivity has been a major economic driver in the current millennium, but it has also raised new problems as malevolent users anywhere in the world have become able to access and interfere with critical personal, commercial and national resources. A major issue for human-computer studies now is to maintain and enhance functionality, usability and likeability for legitimate users whilst protecting them from malevolent users. Understanding the issues involved requires a far broader consideration of socio-economic issues than was required five decades ago. This article reviews various models of the role of technology in human civilization that can provide insights into our current problematique.
Underground hacking has evolved from its early countercultural roots to become a complex and varied phenomenon. By combining a historical review of the literature with a content analysis of 30 years of underground hacker communication, we show that hacking has evolved in three waves to embrace learning and creativity, intrusion and crime, as well as politics and cyberwarfare. We uncover a paradoxical relationship between hackers and society at large where underground hacking is considered a digital crime while at the same time inspiring and driving corporate innovation, cybersecurity, and even cyberwarfare. The outcome of our research provides a nuanced picture of the hacker underground by highlighting differences between competing discursive themes across time. Moreover, by translating these themes into a set of six contrasting personas of IS professionals, we discuss how knowledge, technologies, and creative practices of underground hackers are being professionalized. We use this discussion to provide implications and a research agenda for IS studies in cybersecurity, innovation, and cyberwarfare.
Similarity searching has become widely utilized in many online services processing unstructured and complex data, e.g., Google Images. Metric spaces are often applied to model and organize such data by their mutual similarity. As top-k queries provide only a local view on data, a data analyst must pose multiple requests to observe the entire dataset. Thus, group-by operators for metric data have been proposed. These operators identify groups by respecting a given similarity constraint and produce a set of objects per group. The analyst can then tediously browse these sets directly, but representative members may provide better insight. In this paper, we focus on concise representations of metric datasets. We propose a novel concept of a metric hull which encompasses a given set by selecting a few objects. Testing an object to be part of the set is then made much faster. We verify this concept on synthetic Euclidean data and real-life image and text datasets and show its effectiveness and scalability. The metric hulls provide much faster and more compact representations when compared with commonly used ball representations. (c) 2021 Elsevier B.V. All rights reserved.
"Like notions of process equivalence, behavioural preorders on processes come in many flavours, ranging from fine-grained comparisons such as ready simulation to coarse-grained ones such as trace inclusion. Often, such behavioural preorders are characterized in terms of theory inclusion in dedicated characteristic logics; e.g. simulation is characterized by theory inclusion in the positive fragment of Hennessy-Milner logic. We introduce a unified semantic framework for behavioural preorders and their characteristic logics in which we parametrize the system type as a functor on the category Pos of partially ordered sets following the paradigm of universal coalgebra, while behavioural preorders are captured as graded monads on Pos, in generalization of a previous approach to notions of process equivalence. We show that graded monads on Pos are induced by a form of graded inequational theories that we introduce here. Moreover, we provide a general notion of modal logic compatible with a given graded behavioural preorder, along with a criterion for expressiveness, in the indicated sense of characterization of the behavioural preorder by theory inclusion. We illustrate our main result on various behavioural preorders on labelled transition systems and probabilistic transition systems."
World is moving rapidly towards the digital transformation. The internet across the world is growing rapidly which gives rise to many opportunities in every field including entertainment, finance, education, sports etc. As every coin has two aspects, the internet also has many advantages and disadvantages. We know about the advantages, but the only major disadvantage that everyone should be aware of is the increase in cyber-attacks. It is nothing, but the illegal activity committed on the internet. Many government websites and systems were hacked in the past few years and it has caused a huge loss to the nations like India, USA and China. These governments have already taken various steps to counter these crimes & attacks. But still, the attackers are coming up with new ways of attacking every time. There is need for some concrete solution which is not in the reach of the attackers. For developing such systems and tools we have done a deep analysis of various types of cybercrime and attacks happened in past along with existing solutions proposed by many researchers. Therefore, this paper presents a study on various cyber-attacks that were triggered in India and other countries in the past few years. The various prevention methods proposed in past to deal with these kinds of attacks. These prevention methods are based on machine learning algorithms like a random forest, k-means clustering, support vector machine and artificial neural network applied in order to prevent cyber-attacks. Also, in this paper, we have reported proposed intelligent system which is based supervised and unsupervised learning techniques to avoid these cyber-attacks. This proposed system might provide high efficiency with a minimum human intervention which can be implemented and used as a universal solution to most of the common cyber-attacks.
Two-factor authentication (2FA) is a protective technology designed to increase the security of online accounts. The enhanced security is achieved by using two layers of authentication to facilitate a login process so that should one layer become compromised (e.g. a password), the second layer will still ensure that the account is protected. Considering the prevalence of cybercrime and in particular, password attacks, it is important to examine the behaviour of individuals in terms of the effort they make to protect their online account(s). Studies surrounding 2FA have focused on the various technologies supporting it as well as issues concerning its usability and convenience. In general, users fail to protect themselves online due to the effort that is required. Enhanced security means increased effort and inconvenience, and although risks are present and perceived by Internet users, sufficient effort to protect their online accounts is not made. This study made use of a Protection Motivation Theory (PMT) approach in trying to understand the behaviour of Internet users surrounding their intentions to adopt 2FA to protect their online account(s). The PMT model is adapted to include an additional concept focusing on 'Technology Awareness'. Empirical data was collected using an online survey, with 209 responses analysed using Partial Least Squares Structural Equation Modelling (PLS-SEM). Results were in line with other protective technology literature in terms of the perception of online threat vulnerability and severity being less significant in determining behavioural intention than the perception of the technology itself. The results show that the perception of (1) the difficulty associated with using the 2FA technology (response costs) and (2) the effectiveness of the 2FA technology (response efficacy) prove to be significant in determining behavioural intention to adopt 2FA as protective technology. Lastly, awareness of online security issues and solutions was relevant in the adapted PMT model and significantly influenced intention.
Voice recognition technology is very important for prevention of identity crimes and cybercrimes because voice contains a lot of personal information. In this research, we propose a voice recognition system which consists of two stages of the feature extraction and identification. In feature extraction, noise directly affects the reason of feature extraction. Therefore, how to effectively remove noise is a special important study voice recognition system. Voice signal will be unavoidably affected by the interference of noise in the process of generation and transmission, thus resulting in a decline in the recognition rate of the system. When someone now enroll and use themselves voiceprint with noise to verify their identity by our presented system, the next time only use an app to access online services or phone, this security recognition system will help online system protect users account and reduce the chances of scammers accessing users account and information. The results of experiments, when SNR are -5, 0 and 5, the recognition rate of our proposed system can reach 80.1%, 86.2% and 90.4%, respectively. This research fits to the key approaches to prevent cybercrimes.
Under most circumstances, cyber criminals will commit fraudulent transactions using proxy services which hide their real IP address and physical location. This is done in an effort to avoid being tracked and prosecuted by law enforcement agencies. This paper presents the investigation of a proxy detection methodology and efforts to implement such technology into a business solution with the sole purpose of eliminating the majority of fraudulent transaction attempts. The approach, described in this paper identifies multiple proxy connectivity methods, and implements a multi-tiered detection technique. The result of the experiments demonstrates that the proxy methodology improves business security by identifying users who are utilizing proxies and to collect data that prevents potentially fraudulent activities.
The EU Commission has proposed a new Directive on combating sexual abuse, sexual exploitation of children and child pornography. The updated piece of legislation proposes to block access to child pornography websites. After months of negotiation, the Council, Parliament and Commission have agreed on a compromised text which generates more confusion and has been lambasted as meaningless. The compromised text is a result of powerful lobbying by Hollywood porno industry, Internet Service Providers and civil libertarians. The compromised text brings to highlight the tension between freedom of speech and protection of children. (C) 2011 Sylvia Kierkegaard. Published by Elsevier Ltd. All rights reserved.
Of all the types of cybercrime considered in this book, piracy, illegal file sharing and/or other types of copyright infringement are probably the offences that members of the general public are most likely to have committed. Yar (2007) indicates that piracy activity seems to be very widespread, including individuals from various social classes, although there seems to be a disproportionate number of young people engaging in the activity. This chapter aims to determine if those involved in piracy and online copyright infringement activities see themselves as criminals. It also aims to examine how such offenders justify their actions and how they can be dissuaded from such acts. Definitions of key terms in the area will be presented, along with some examples of real events relating to illegal file sharing. A description of some of the methods used during illegal file sharing and piracy will be provided, along with a historical view of how copyright infringement has developed over time. The known current prevalence rates and costs of offending will be considered, along with arguments presented from industry and academia regarding the effects of file sharing on legitimate sales. Similarly, the problem of trying to estimate the true cost of piracy and illegal file sharing will be highlighted. The psychology of offenders will be considered, and in particular, the phenomenon of the lack of insight of offenders into their own criminality will be investigated. In particular, the roles of self-control, social learning and justifications in illegal file sharing will be analysed. Some potential solutions for these crimes will be considered, including the determination of appropriate punishments and the development of suitable educational campaigns. Finally, potential future trends and research will be described.
The threat of Cyberterrorism is a sub-category of the overall threat environment that can emerge from cyberspace. In consequence military commanders have to take this into account for planning and executing operations when the strategic threat assessment indicates the presence of this specific threat to the operation. The article provides a practitioner's perspective along the military Operational Planning Process (OPP) and the recent developments to integrate cyber considerations in this process along the questions what ... if ... a commander of an (EU-led) Crisis Management Operation (CMO) has to respond inter alia to the (so far fictional) tasking, ... in cooperation with all relevant partners and stakeholders take appropriate means and actions to counter Cyberterrorism in the Area of Responsibility (AoR).... It provides considerations, which are the relevant planning aspects and factors that the commander should take into account in order to respond to the tasking. This includes the characteristics of cyberspace, the phenomenon of cyberterrorism as such, the type of the military operation, the mandate, the legal framework, the (cyber) situation in the theatre, the necessary information and capabilities, necessary processes/procedures, as well as the relationship with other Cyber Security Providers like Law Enforcement Agencies (LEA) in the AoR. It provides an assessment of the use of military capabilities in countering cyberterrorism in crisis management operations, and draws conclusions on the different options and limitations for a complementary use of LEAs and the Military in countering cyberterrorism as it occurs in current and future CMO outside of EU boundaries.
Cities all over the world are becoming increasingly smarter, including Dubai Smart City. The Smart Cities vision is to automate critical public services, make improvements for community interaction, and achieve significant levels of efficiency in a connected and technology-driven society, such as in traffic control. However, this has positioned Smart Cities to be a target of compromised Cyber Security, which can potentially undermine the information systems of local departments, general security, law enforcement, and transport systems. Dubai is a dynamic and emerging economy that has the potential to become a major economic center of the Middle East and North Africa (MENA) region owing to its strength in strategic information. The main purpose of this paper is to explore, evaluate, and discuss components of overall strategic security, innovation, management, and the development of Dubai in terms of Cyber Security. This study also showcases conducted survey results based on the Smart City Cyber Security program. Furthermore, it demonstrates the current and future challenges faced by Smart Cities from cyber-threats, including the development of strategic security plans to defend against sophisticated cyberattacks, and encourages citizens to take responsibility and implement awareness of how to proactively counter cyberattacks on Smart Cities. However, it is imperative to evaluate the strategies that are required to prepare Dubai's Electronic Security Center to align with Dubai Vision 2021 in order for Dubai to be one of the securest Smart Cities in terms of Cyber Security around the globe. This research aims to argue that to develop Dubai's Electronic Security Center, strategies should be a top priority for Dubai to ensure the security of the Smart City and to diminish cybercrime in Dubai.
The internet has become so integrated with users' daily activities, that cyber users interweave their daily activities automatically between the physical and cyberspace without noticing. Cyberspace can be referred to as a virtual computer world, which includes the connectivity of multiple networks. These connections between multiple networks within cyberspace form a global computer network to enable online communication between cyber users. Cyber users connect to cyberspace for socialising, work and educational purposes. The advantages of cyberspace are enormous and to a great benefit to all cyber users and cyber business. However, cyberspace opens the door to a number of possible cyber risks and cybercrimes that can affect cyber users. Cybercrimes and risks relate to either financial loss, disruption or damage to the reputation of a cyber user or organisation. These cybercrimes can include hacking, phishing or identity theft. Cyber users may not be aware of or knowledgeable regarding cyber risks and cybercrimes. The cyber user needs to be cyber safety conscious in order to be protected against cyber risks and cybercrimes. In many instances, cyber users within the industrial sector are being made aware of cyber risks through education and training programmes within their working environment. However, many cyber users within communities in South Africa are not working in industry and therefore do not have access to opportunities regarding cyber safety awareness. This research aims to investigate the level of cyber safety awareness within communities and propose a number of approaches that can be used to create and implement cyber-safety awareness programmes and material within different communities. Differently communities within South Africa have different needs that can range from different languages, learning approaches and community-defined processes and procedures. A quantitative research method and random sampling were used to obtain data about cyber safety awareness within communities. In the research, a survey with full ethical clearance was used.
The data generated by Internet of Things devices is increasingly being introduced as evidence in court. The first US case involving the introduction of medical data from a pacemaker as evidence of arson and insurance fraud was State of Ohio v Compton. The purpose of this article is three-fold. First, the article explores this case, looking in particular at the facts of the case and the charges brought against the defendant. Second, the article critically examines the decision of the trial court judge during the suppression hearing for the evidence from the pacemaker. In this hearing, the judge ruled that the search and seizure did not violate the Fourth Amendment rights of the defendant and allowed the pacemaker data to be entered as evidence against him. Third, the article considers the implications of this decision for future cases involving Internet-of-Things (IoT) medical data. Ultimately, the constitutional protections of IoT medical device data and the circumstances under which the data from these devices will be collected and used as evidence, are issues that currently demand the attention of legal and digital forensics professionals and warrant public debate.
Internet has brought a lot of security challenges on the interaction, activities, and transactions that occur online. These include pervasion of privacy of individuals, organizations, and other online actors. Relationships in real life get affected by online mischievous actors with intent to misrepresent or ruin the characters of innocent people, leading to damaged relationships. Proliferation of cybercrime has threatened the value and benefits of internet. Identity theft by fraudsters with intent to steal assets in real space or online has escalated. This study has developed a metrics model based on distance metrics in order to quantify the credential identity attributes used in online services and activities. This is to help address the digital identity challenges, bring confidence to online activities and ownership of assets. The application forms and identity tokens used in the various sectors to identify online users were used as the sources of the identity attributes in this paper. The corpus toolkits were used to mine and extract the identity attributes from the various forms of identity tokens. Term weighting schemes were used to compute the term weight of the identity attributes. Other methods used included Shannon Entropy and the Term Frequency-Inverse Document Frequency scheme (TF*IDF). Standardization of data using data normalization method has been applied. The results show that using the Cosine Similarity Measure, we can identify the identity attributes in any given identity token used to identify individuals and entities. This will help to attach the legitimate ownership to the digital identity attributes. The developed model can be used to uniquely identify an online identity claimant and help address the security challenge in identity management systems. The proposed model can also identify the key identity attributes that could be used to identify an entity in real or cyber spaces.
In today's digital age, the digital transformation is necessary for almost every competitive enterprise in terms of having access to the best resources and ensuring customer satisfaction. However, due to such rewards, these enterprises are facing key concerns around the risk of next-generation data security or cybercrime which is continually increasing issue due to the digital transformation four essential pillars- cloud computing, big data analytics, social and mobile computing. Data transformation-driven enterprises should ready to handle this next-generation data security problem, in particular, the compromised user credential (CUC). When an intruder or cybercriminal develops trust relationships as a legitimate account holder and then gain privileged access to the system for misuse. Many state-of-the-art risk mitigation tools are being developed, such as encrypted and secure password policy, authentication, and authorization mechanism. However, the CUC has become more complex and increasingly critical to the digital transformation process of the enterprise's database by a cybercriminal, we propose a novel technique that effectively detects CUC at the enterprise-level. The proposed technique is learning from the user's behavior and builds a knowledge base system (KBS) which observe changes in the user's operational behavior. For that reason, a series of experiments were carried out on the dataset that collected from a sensitive database. All empirical results are validated through well-known evaluation measures, such as (i) accuracy, (ii) sensitivity, (iii) specificity, (iv) prudence accuracy, (v) precision, (vi) f-measure, and (vii) error rate. The experiments show that the proposed approach obtained weighted accuracy up to 99% and overall error of about 1%. The results clearly demonstrate that the proposed model efficiently can detect CUC which may keep an organization safe from major damage in data through cyber-attacks. (C) 2018 Published by Elsevier B.V.
Purpose The aims of this paper are to explore the rise of cyberhate on the Zoom video conferencing platform at the outset of the coronavirus disease 2019 (COVID-19) pandemic and to examine victimized cases of Zoombombing where it was used as a cyberhate tool. The COVID-19 pandemic has not only impacted our lives and modes of working and studying, but also created new environments for cybercriminals to engage in cybercrime, cyberhate and hacking by exploiting tools such as Zoom. This new phenomenon called Zoombombing was first reported in mid-March 2020, when the social distancing and stay-at-home policies in the United States were nationally introduced. Design/methodology/approach This research conducted a news media content analysis on cases of Zoombombing. To conduct this analysis empirically, a dataset with all of the reported Zoombombing cases from March to April 2020 was created. Google Trends, news media and tweets were used to analyze Zoombombing as a form of cyberhate, particularly digital racism. Findings The results reveal prevalent Zoom-mediated racism toward Asian Americans, African Americans and Jewish Americans. This study understands Zoombombing from a sociopolitical/cultural perspective through news reporting of victimized cases and explores various ways that Zoombombing shapes, mediates, transforms and escalates racism. Originality/value This study is one of the very first studies to analyze Zoombombing in a way that builds upon an emerging body of literature on cyberhate. This paper considers Zoom as a space where curious young people, cybercriminals, extremists and hackers impose their ideologies and beliefs upon newly established online learning and working environments and engage in a struggle for identity recognition in the midst of increasingly accessible vulnerable software and cyberspace.
"Cloud computing has drastically altered the ways in which it is possible to deliver information technologies (ITs) to consumers as a service. In addition, the concept has given rise to multiple benefits for consumers and organizations. However, such a fast surge in the adoption of cloud computing has led to the emergence of the cloud as a new cybercrime environment, thus giving rise to fresh legal, technical and organizational challenges. In addition to the vast number of attacks that have had an impact on cloud computing and the fact that cloud-based data processing is carried out in a decentralized manner, many other concerns have been noted. Among these concerns are how to conduct a thorough digital investigation in cloud environments and how to be prepared to gather data ahead of time before the occurrence of an incident; indeed, this kind of preparation would reduce the amount of money, time and effort that is expended. As a number of cloud forensics challenges have not received enough attention, this study is motivated by a particular gap in research on the technical, legal and organizational factors that facilitate forensic readiness in organizations that utilize an Infrastructure as a Service (IaaS) model. This paper presents a framework with which to investigate the factors that facilitate the forensic readiness of organizations. This framework was identified by critically reviewing previous studies in the literature and by performing an in-depth examination of the relevant industrial standards. The factors were comprehensively studied and extracted from the literature; then, the factors were analysed, duplicates were removed, and the factors were categorized and synthesized to produce the framework. To obtain reliable results, the research method involved two steps: a literature review, followed by expert reviews. These techniques help us paint a comprehensive picture of the research topic and validate and confirm the results."
This paper discusses a new online tool for online gamers to share networking performance results and to produce aggregate networking performance reports generated from the data reported by all users. It is necessary to improve the quality of service and make online games faster. This will help improve latency sensitive game data collection. It will help to improve gaming performance through the global network of servers, optimizing the game connection from end to end. A Web API was developed to receive data in the form of a HTTP post from WTFast clients while they are playing their games. A data generator was also built to simulate thousands of concurrent users posting data to the web API. The web API stores the data in a Elasticsearch database. A web interface was built for WTFast clients to view their historical network data and to share it on forums or social media. The web interface shows a graph of their network performance with and without using the GPN (R). The complete system is anticipated to need to handle approximately 400,000 concurrent users sending network data every 2 seconds.
The Denial of Service (DoS) attacks is one of the main issues faced by cloud service providers due to their intricate nature. The main aim of this attack is to disrupt the services of authorized users by forwarding massive malicious requests to the victim system. Even though the modern Artificial Intelligence-powered intrusion detection system offers improved benefits, it suffers from analyzing the traces of intrusion in the massive network flow. This article presents a novel Chaos-based Henry Gas Solubility Optimization-Weight initialization based-rectified linear Unit (HGSO-WIB-ReLU) framework to identify different types of DoS attacks like HTTP flood attacks, DNS flood attacks, and ICMP flood attacks. The main aim of this technique is to identify the low rate of DoS attacks which the state-of-art techniques often based on statistical analysis and machine learning are incapable of accomplishing. Convolutional neural network architecture incorporates the WIB-ReLU activation function primarily to prevent vanishing gradient issues and provide efficient training. The WIB-ReLU framework's hyperparameters are primarily improved using the HGSO method by increasing the classifier's accuracy rate for detecting DoS attacks. In this way, the proposed model minimized the high false positive rate in identifying low-rate DoS attacks in large datasets. Different benchmark datasets such as BUET- DDoS2020, CIC DoS Attacks, and Low Rate DDoS are used to conduct the experiments and the efficiency of the model is verified in terms of accuracy, F1-measure, and precision score. When evaluated using the BUET-DDoS, CIC-DoS, and Low rate DDoS datasets, the proposed model offers an accuracy of 97%, 96.67%, and 96%, respectively. The experimental analysis conducted demonstrates the effectiveness of the proposed HGSO-WIB-ReLU framework when compared to the different state-of-art methodologies.
We review seminal social science theories of Trust & Control to consider how their application to Blockchain and Cryptocurrency (DLT, e.g., Bitcoin, Ethereum, Ripple) provide the potential for fresh criminal and information security challenges to traditional mechanisms of criminal detection and law enforcement. The social science theories of trust and control provide an accessible matrix to evaluate malicious behavior related to these new forms of money and currency. This foreshadows the ability for DLTs to become the 'poison of choice' for crime and security objectives or perhaps be avoided altogether by criminals. We argue that an understanding of DLTs is incomplete without a social science underpinning and framework which trust and control provide. The continued use of these technologies will require public and private institutions to rethink their approaches to crime prevention and information security for purely digital threats.
Network forensics can be an expansion associated with network security design which typically emphasizes avoidance and detection of community assaults. It covers the necessity for dedicated investigative abilities. When you look at the design, this indeed currently allows investigating harmful behavior in communities. It will help organizations to examine external and community this is undoubtedly around. It is also important for police force investigations. Network forensic techniques can be used to identify the source of the intrusion and the intruder's location. Forensics can resolve many cybercrime cases using the methods of network forensics. These methods can extract intruder's information, the nature of the intrusion, and how it can be prevented in the future. These techniques can also be used to avoid attacks in near future. Modern network forensic techniques face several challenges that must be resolved to improve the forensic methods. Some of the key challenges include high storage speed, the requirement of ample storage space, data integrity, data privacy, access to IP address, and location of data extraction. The details concerning these challenges are provided with potential solutions to these challenges. In general, the network forensic tools and techniques cannot be improved without addressing these challenges of the forensic network. This paper proposed a thematic taxonomy of classifications of network forensic techniques based on extensive. The classification has been carried out based on the target datasets and implementation techniques while performing forensic investigations. For this purpose, qualitative methods have been used to develop thematic taxonomy. The distinct objectives of this study include accessibility to the network infrastructure and artifacts and collection of evidence against the intruder using network forensic techniques to communicate the information related to network attacks with minimum false-negative results. It will help organizations to investigate external and internal causes of network security attacks.
TLS is crucial to network security, but TLS-related APIs have been repeatedly shown to be misused. While existing usable security research focuses on cryptographic primitives, the specifics of TLS interfaces seem to be under-researched. We thus set out to investigate the usability of TLS-related APIs in multiple libraries with a focus on identifying the specifics of TLS. We conducted a three-fold exploratory study with altogether 60 graduate students comparing the APIs of three popular security libraries in establishing TLS connections: OpenSSL, GnuTLS, and mbed TLS. We qualitatively analyzed submitted reports commenting on API usability and tested created source code. User satisfaction emerged as an interesting, potentially under-researched theme as all APIs received both positive and negative reviews. Abstraction level, error handling, entity naming, and documentation emerged as the most salient usability themes. Regarding functionality, checking for revoked certificates was especially complicated and other basic security checks seemed not easy as well. In summary, although there were conflicting opinions on both the interface and documentation of the libraries, several usability issues were shared among participants, forming a target for closer inspection and subsequent improvement.
Smartphones and tablets have become important in daily life as they provide a convenient and mobile means to seek, access and share information. This convenience carries a downside in the rising concern with mobile security. Conveying the importance of personal digital security is notoriously difficult with adult users and perhaps more so for users who are children. Children are avid consumers of novel apps, frequently install new programs, and regularly update existing ones. Since apps can have hidden actions or malicious behaviour, this places users at risk from malicious or modified apps and some teenage users avoid specific apps out of concern for privacy. For example, location information is considered sensitive, so thousands of mobile apps that access GPS data, might impact upon children's privacy and security. This paper reports a school-based survey of mobile usage as a perspective on issues in smartphone security. Our results suggest that males are more cautious than females in relation to installation of software applications on mobile devices and tend to be more aware of permission issues than their female counterparts. In contrast, a larger proportion of females than males received upsetting, attacking or bullying content from other people.
The development of the information and digital world determines the formation of new public requests, firstly, to ensure the security of cyberspace. The development of social tasks that can be solved using information technologies is constantly growing, therefore the number of cybercrimes is increasing, and their specifics are changing. An analysis of the development trend of cybersecurity and cybercrime showed that the number and scale of cyberattacks will continue to grow. Accordingly, the purpose of the article is to find the development directions of approaches to ensuring security in cyberspace in accordance with modern global challenges. The article determines that new global challenges have significantly changed the system of people's life conducting financial transactions, trade, receiving services (administrative, communal, social, and household, etc.), and providing communication systems. In the conditions of the pandemic, the processes of all spheres of human activity only spread. The analysis of digitalization models for ensuring cybersecurity is presented in the article but needs clarification and further development, which relates to new global challenges in the security architecture in the world and on the European continent. The article considers approaches to ensuring the effectiveness of cybersecurity in new social conditions. The article uses the methods of control charts to conduct a sociological survey to rank the factors of influence on the cause-and-effect relationships of cybersecurity. The put-forward hypothesis regarding the structuring of influencing factors allows us to systematize directions for further research. The projection of the trend line of the number of cyberattacks and the number of subjects subjected to cyberattacks in the analyzed period determines the scaling and complication of protection processes both at the company level and at the state level.
"Ransomware is a type of malicious software that locks out its victim from accessing functionality or data on their device, typically by encrypting files. To regain access, victims would typically need to make a ransom payment. Victims get notified that their device has been infected through a ransom note (splash screen) displayed on their device. Ransomware splash screens can be presented in many ways; the most common ones are via a text file or a graphical user interface (GUI). Splash screens may also include additional features, such as a countdown timer, as part of the ransomware operator's ploy to encourage their victims to pay. The main aim of this study was to gain valuable insights into how ransomware splash screens might affect victims' responses. Moreover, the study also investigated whether exposure to different splash screens would encourage participants to adopt good security behaviours. A controlled experiment was conducted by randomly assigning 538 participants into one of the three ransomware infection scenarios based on the splash screen type (Text -based, GUI or GUI + Timer). After watching a demonstration of a ransomware scenario, each participant was asked to complete a survey regarding their post-infection behaviour and their cybersecurity habits. The study concluded that ransomware's user interface elements do not have a notable effect on how victims would react, in terms of their willingness to pay or their reporting rates. Additionally, even though 60% of the participants would like to report a ransomware incident, they were not sure how to do this. This illustrates a lack of public awareness about cybercrime reporting. Lack of trust was the main reason why participants did not want to click on links offering cybersecurity advice after the exposure. This shows that more effective methods for encouraging cybersecurity behaviour are still needed."
"The people having a perpetrating mind and the facilitation in advanced technologies cause the criminogenic activities in cyberspace, thereby creating societal problems. Darknet is an internet-based technology that builds on an encrypted network. Darknet networks can be accessed using a specific software with a specific network configuration; its content does not index by any search engines. Since its beginning, Darknet has been used for criminogenic tasks and applauded primarily for cybercrime promotion, including arms and drug dealing. Few countries have control over digital media and are ruled by a suppressive government. They have formulated strict policies for freedom fighters and journalism, using the Darknet anonymously. Also, many people use it for illegal purposes. Therefore, we have both positive and negative impacts of the darknet on human society and just cannot be discarded. However, in this paper, our prime concern emanates from the darknet network detection from the network traffic data through the deep transfer learning model. To provide a more accurate result, we transform time-based features into a three-dimensional image and then feed it into a pretrained model for the extraction of promising features. In this study, we considered the Deepinsight method to transform the numerical features into image data. These features were then used in a proposed bi-level classification system to classify the input data into malicious activities. To identify the optimized pretrained network this paper utilized 10 pre-trained models: AlexNet, ResNetl8, ResNet50, ResNet101, DenseNet, GoogLeNet, VGG16, VGG19, Inceptionv3, and SqueezeNet with three different baseline classifiers, namely support vector machine, decision tree, and random forest. In addition to malicious activity prediction, the proposed model could also predict the type of traffic. The experiment results illustrate that the VGG19 based features along with random forest can classify the traffic data with 96% of accuracy. (C) 2021 Elsevier B.V. All rights reserved."
This paper describes our research on the insider threats of Social engineering. Social engineering is a method using interaction between humans to get the access of a system in an illegal way. Due to staff's lack of confidentiality, the confidentiality of records is compromised, data is stolen or financial damage is done. This is insider threat. Social engineering and insider threat are two of the most relevant subjects in cyber security today. This research summarizes and seeks solution for the drawback of Social engineering through analyzing the Insider Threat cases. The first stage is to introduce the importance of using social engineering to reduce internet crime by analyzing the past loss created by insider threats. The second test illustrates insider threats' hazards to network security are ongoing. The third part covers the situation of insider threats with the emphasis on the security side. The topic of security aspect is extended to the rest of internal control of system, data exchange, and management of employees and their communication content. Actually, by the time of this abstract, insider threats are still not being taken as seriously as it should be. Many companies and organizations have given little thought to the insider threat but have concentrated on keeping attackers outside the network. This research will directly focus on the insider threats of organizations and the ways hackers use social engineering with the latest analysis of technology involved and examples that are close to common cybercrime. We aim to reveal the importance of reducing insider threats in organizations. The further research will be focused on a group consisted of managers and engineers within a company and the communication means of staff to the outside world. The analysis of the related crime cases will help prevent similar tragedy and seek possible approaches.
Artificial intelligence (AI), and in particular machine learning(ML), deep learning (DL) has seen huge pace in recent years and is now set to really start influencing all aspects of community and occupations in which people are engaged. This growth has been charged the advancement in computing power, combined with headway in algorithms and cybersecurity is no exception. AI is an area of computer science that deals with giving machines the ability to look like they have natural brilliance. Systems which are based on AI, sometimes called as cognitive systems, are helping us automate many jobs and gear up difficulties which are more complex than most humans are capable of solving. New generations malware and cyber attacks can be difficult to detect with traditional cybersecurity procedures. They develop over time, so more vigorous approaches are necessary. Solutions for these problems in security rely on ML use data from earlier attacks to respond to newer. Another significant advantage of AI systems in cybersecurity is that they will free up a huge amount of time for IT employees. AI is most commonly used to detect threats and attacks. The systems are developed in a such a way that it must be able to act quickly to the situation on its own. And AI systems do not make errors in completing their tasks. So each threat is responded in the most effective and proper way. In future days, there will be a rapid increase of an international clash in cyberspace. This may include attacks on infrastructure and utilities, as well as damaging normal operations of government and financial bodies, traditional society institutions such as banks, press, law enforcement, and judicial. So taking the cybersecurity as an issue for this paper and let us see the challenges and what is the role of AI, ML, and DL in avoiding cybercrime in future.
Social ideas and affordable and global online communications. Many people use social networks to communicate and express their opinions in supporting or opposing different causes, with most of this user-generated content being textual information. As there are a lot of raw data of people posting real time messages about their opinions on a variety of topics in daily life, it is a worthwhile research endeavor to collect and analyze these data, which may be useful for government to make informed decisions or to monitor public opinion. Data available in social media is obviously only one type of information that can be of interest when trying to detect a possible terrorist or radical group, there are several cases for example in which the social media has been used by radical thinkers to act as influencers and encourage fanatics with the same radical views to take violent action.Therefore, in this paper, we propose a framework for opinion mining and extremist content detection in on line social media data. Social media data targeted in this work to analyze, is the public text post on Facebook, the most popular social networking site. With this framework, machines can learn how to automatically extract the set of messages from Facebook public pages, using API graph calls, filter out non opinion messages,determine their sentiment regarding the issue of interest directions (i.e. positive, negative) and detect violent or extremist content. The purpose of this model is to build a Big Data application that gets stream of public data from Facebook social network, which can help law enforcement and cybercrime analysts with analyzing and monitoring social media, in the search of digital trace of violence or radicalism, that can be exploited in further digital forensic investigation
The wide diffusion of mobile devices and the ability of users to customize their experience through applications (Apps) is opening to new problems related to privacy, security and data integrity for the mobile ecosystem. Smartphones, in general, and Android devices, in particular, are rapidly becoming emerging threat vectors of cybercrime activities. Unofficial Android markets, especially those with weak controls on published Apps, are the places where frauds may easily start and spread. Hence, the ability to identify and quickly shut down deceptive Apps is of paramount importance in the protection of users, services and infrastructures. Traditional approaches that aim at mitigating the presence of malicious Apps in unofficial markets, are based on crawlers for scanning stores and checking the words used in Apps' description. These methods works very well when the App's title, keywords and description match specific patterns that identify services to protect and the application owner or App's signature do not match expected ones. Unluckily, the performance of such methods reduce sharply when the store adopts a language that is not supported by the recognition system or the App publisher uses misleading words in the App's description. Nevertheless, App publishers always use a logo which is familiar to the user in order to highlight the application and increase the probability that the users install it. In this paper we presents a system that overcomes the limitation of traditional approaches including logo analysis in the process of App recognition. Our contribution is the definition and evaluation of a logo-based complementary system to be used in conjunction with traditional approaches based on word lists checking. The system and the performance of the proposed solution are presented and analyzed in the paper.
In this technological world one of the general method for user to save their data is cloud. Most of the cloud storage company provides some storage space as free to its users. Both individuals and corporate are storing their files in the cloud infrastructure so it becomes a problem for a forensics analyst to perform evidence acquisition and examination. One reason that makes evidence acquisition more difficult is user data always saved in remote computer on cloud. Various cloud companies available in the market serving storage as one of their services and everyone delivering different kinds of features and facilities in the storage technology. One area of difficulty is the acquisition of evidential data associated to a cybercrime stored in a different cloud company service. Due to lack of understanding about the location of evidence data regarding which place it is saved could also affect an analytical process and it take a long time to speak with all cloud service companies to find whether data is saved within their cloud. By analyzing two cloud service companies (IDrive and Mega cloud drive) this study elaborates the various steps involved in the activity of obtaining evidence on a user account through a browser and then via cloud software application on a Windows 7 machine. This paper will detail findings for both the Mega cloud drive and IDrive client software, to find the different evidence that IDrive and the mega cloud drive leaves behind on a user computer. By establishing the artifacts on a user machine will give an overall idea regarding kind of evidence residue in user computer for investigators. Key evidences discovered on this investigation comprises of RAM memory captures, registry files application logs, file time and date values and browser artifacts are acquired from these two cloud companies on a user windows machine.
There is an emerging international phenomenon of drugs that are sold without any control on online marketplaces. An example of a former online marketplace is Silk Road, best known as a platform for selling illegal drugs operated as a Tor hidden service. Silk Road was closed by FBI in 2013 but new alternatives have appeared since illicit substances is a big market. One problem with online marketplaces is that the sold substances have many different names and new substances are constantly developed. In this work we use semantic techniques to automatically detect new names of drugs. Our experiments are applied on data from a darknet marketplace, on which we use a set of known drug names and distributional statistics to find words that are semantically similar. The results show that semantic technologies work very well when it comes to detecting names of drugs on darknets.
Recently, well-known and established South African organisations have experienced cyberattacks. South African Bank Risk Information Centre (SABRIC) confirmed in October 2019 that the industry had been hit by a wave of Distributed Denial of Service (DDoS) attacks targeting multiple banks. This happened shortly after the website of City of Johannesburg (CoJ) succumb to a ransomware attack. These attacks are a wakeup call for South African organisations and underline the essential need for suitable detection mechanisms to prevent cyberattacks. The detection of cyberattacks relies not only on understanding existing attacks but also being able to identify emerging threats. The continuous and strategic collection of relevant and valuable cybersecurity data sets can offer insight into ongoing threats or cyberattacks, while also assisting with the combatting of cybercrime. Although various third-party providers, such as Shodan and Have I Been Pwned (HIBP), exist and do provide access to cybersecurity data sets, these providers have little to no presence in South Africa (SA). Most of the available cybersecurity data sets are heavily slanted towards the United States and the identified trends might not be relevant to the South African context. Therefore, this paper introduces the Lost Packet warehousing Service, a technological solution that will function as the primary source for cybersecurity data collection within South Africa. The Lost Packet Warehousing Service will allow for the continuous but passive collection of cybersecurity data sets. Examples of such data sets could include network telescope, honeypot and NetFlow collectors. Data analysis and processing techniques are then applied to the collected cybersecurity data sets to identify, infer, detect and predict emerging trends and cyberattacks. Also discussed in this paper is the steps taken to maintain the security and privacy of the collected cybersecurity sets. The paper concludes by discussing the various benefits offered by the Lost Packet Warehousing Service.
Transformations of data objects into the Hamming space are often exploited to speed-up the similarity search in metric spaces. Techniques applicable in generic metric spaces require expensive learning, e.g., selection of pivoting objects. However, when searching in common Euclidean space, the best performance is usually achieved by transformations specifically designed for this space. We propose a novel transformation technique that provides a good trade-off between the applicability and the quality of the space approximation. It uses the n-Simplex projection to transform metric objects into a low-dimensional Euclidean space, and then transform this space to the Hamming space. We compare our approach theoretically and experimentally with several techniques of the metric embedding into the Hamming space. We focus on the applicability, learning cost, and the quality of search space approximation.
As recent events have clearly demonstrated, the first years of the 21st century have been tenser than experts in the field would have estimated some time ago. This situation is largely due to significant developments in the global security environment, generated by newer (cybercrime) or older (terrorism, trans-border crime) risks and threats, which have reached unpredictable levels of intensity. Changes in the security environment will inevitably lead to a reconsideration of identity security and of the responsibility for the patrimonial, cultural and religious values of a state. We believe that besides representing a political project, the effort of managing Europe is or should also be a project for managing the European cultural space. Valorizing the heritage of every nation, safeguarding the masterpieces that have not yet been recognized as such, the monuments, the traditions and the small heritage of all the countries belonging to this cultural area (regardless of the political evolution of united Europe) is becoming the primary mission in a Europe of cultures. Noting the impact of cultural and religious determinations on either individual or collective personality, social psychology is called upon to analyze and describe the ways in which individuals and societies can influence culture, behavior and spirituality, but also the ways in which culture and religion can influence, in their turn, individuals and societies. Day after day, it is becoming increasingly clear that 21st-century Europe is a space of-often inadvertent-interference between religion and culture, the former having contributed to the development of culture and having, indeed, created culture. Aware of its contribution to the creation of culture, religion remains open to dialogue with the culture of the secular society, not in order to impose its own views, but to build interconnecting bridges.
This article considers the legal response to romance frauds. This is an issue that attracts increasing scholarship, as digital communication technologies not only make it easier for people to meet but also for scammers to prey on unsuspecting victims. Initially, these scams would take place solely online, and they could proceed very rapidly. While these still exist, they increasingly take place over a prolonged period and can be very sophisticated, including the establishment of a real relationship. Historically, the law responds to this crime through offences relating to fraud, but this arguably leaves parts of the behaviour unpunished. Using hypothetical case studies (vignettes), this article compares how the law in England and Wales, Scotland and Canada tackles romance fraud. It concludes by suggesting the need for a new offence where sexual activity is procured by deception.
A parallel neural joint model algorithm is proposed for the analysis and detection of malicious Uniform Resource Locator (URL). By detecting and analyzing malicious URL's characteristics, the semantic and visual information will be extracted. First, a visualization algorithm is used to realize the visualization of the URL mapping to a gray image with texture characteristics. Second, the lexical feature and character feature of URL are extracted and further processed through word vector technology. These extracted features are transformed into lexical embedding vectors and character embedding vectors. To combine the texture features with text features, a parallel joint neural network combining capsule network (CapsNet) and independent recurrent neural network (IndRNN) is utilized to capture multi-modal vectors of visual and semantic information synchronously. The last layer utilizes the attention mechanism to further filter the deep features extracted from the overall network while concentrating on effective features improving the classification accuracy and analyzing and detect malicious URLs. Based on the experimental results, it is demonstrated that this algorithm has higher accuracy compared to the traditional algorithms.
"Concerns about cyberwar, cyberespionage, and cybercrime have burst into focus in recent years. The United States and China have traded accusations about cyber intrusions, and a December 2012 U.N. conference broke down over disagreements about cyberspace governance. These events show the increased risk of cyberconflict and the corresponding need for basic agreement between states about governing cyberspace. States agree that something must be done, but they disagree about almost everything else. Two competing visions of cyberspace have emerged so far: Russia and China advocate a sovereignty-based model of cyber governance that prioritizes state control, while the United States, United Kingdom, and their allies argue that cyberspace should not be governed by states alone. Prior academic writing has focused on cyber issues related to states' regulation of their citizens, but this Article addresses the now-pressing state-to-state issues. A limited analogy to existing legal regimes for the high seas, outer space, and Antarctica shows that global governance of cyberspace is possible. Moreover, these existing regimes provide a menu of options for governance and establish a baseline against which cyber governance can be assessed. The Article examines three fundamental questions that states have answered for the other domains and must now answer for cyber: (I) what role, if any, private parties should play in governance; (2) how the domain should be governed (no governance system, treaty, or norms); and (3) whether and how to regulate military activities in the domain. The answers for the old domains were similar multilateral governance, governance by treaty, and some level of demilitarization. But cyber differs from the old domains in important ways that suggest the answers for cyber should be different. This Article argues for multistakeholder governance, governance through norms, and regulated militarization."
The emerging dynamic architectures of autonomous digital ecosystems raise new challenges in the process of assuring trust and safety. In particular, the admission of software smart agents into autonomous dynamic ecosystems will become a significant future topic. In this work we propose the concept of predictive simulation, which elevates from the concept of virtual Hardware-in-the-Loop (vHiL) testbed, to support rapid runtime evaluation of software smart agents in autonomous digital ecosystems. Based on this testbed, we introduce a novel strategy for building trust in software components that enter an ecosystem as black boxes without executing their behavior which can be potentially malicious, but by executing corresponding digital twins which are abstract models fed with real-time data.
Robust cyber-resilience depends on sound technical controls and testing of those controls in combination with rigorous cyber-security policies and practices. Increasingly, corporations and other organizations are seeking to test all of these, using methods more sophisticated than mere network penetration testing or other technical audit operations. More sophisticated organizations are also conducting so-called Red Team exercises, in which the organization tasks a small team of highly skilled and trained individuals to try to gain unauthorized access to physical and logical company assets and information. While such operations can have real value, they must be planned and conducted with great care in order to avoid violating the law or creating undue risk and reputational harm to the organization. This article explores these sometimes tricky issues, and offers practical risk-based guidance for organizations contemplating these types of exercises. (C) 2018 Joseph V. DeMarco. Published by Elsevier Ltd. All rights reserved.
Smartphones have become increasingly popular, and, nowadays, thanks to the use of 3G networks, the need for connectivity in a business environment is significant. Smartphones provide access to a tremendous amount of sensitive information related to business, such as customer contacts, financial data and Intranet networks. If any of this information were to fall into the hands of hackers, it would be devastating for the company. In this paper, we propose a cluster-based approach to detecting abnormal behaviour in smartphone applications. First we carry out various robust clustering techniques that help to identify and regroup applications that exhibit similar behaviour. The clustering results are then used to define a cluster-based outlier factor for each application, which in turn identifies the top n malware applications. Initial results of the experiments prove the efficiency and accuracy of cluster-based approaches in detecting abnormal smartphone applications and those with a low false alert rate.
Effective criminal investigation depends on reliable access to evidence. With the extensive use of cloud computing in various forms, electronic evidence of criminal activity may no longer be found with criminals or their associates themselves. Rather, the evidence resides with cloud providers, oftentimes on servers outside of the territory of the investigating law enforcement authorities (LEAs). Thus, even in otherwise completely domestic criminal investigations of crime committed domestically against a domestic victim, relevant electronic evidence may be stored in a cloud arrangement in another country. Obtaining the evidence in those situations may be difficult. In this article, we identify 16 variables and a number of fundamental and non fundamental constraints that must be taken into account by anyone setting out to construct a framework facilitating appropriate LEA access to evidence via direct contact with cloud providers, while safeguarding the rights and interest of individuals, as well as the rights and interest of the provider, and those of other States. (C) 2016 Dan Jerker B Svantesson & Lodewijk van Zwieten. Published by Elsevier Ltd. All rights reserved.
Many methods in image forensics are sensitive to varying amounts of JPEG compression. To mitigate this issue, it is either possible to a) build detectors that better generalize to unknown JPEG settings, or to b) train multiple detectors, where each is specialized to a narrow range of JPEG qualities. While the first approach is currently an open challenge, the second approach may silently fail, even for only slight mismatches in training and testing distributions. To alleviate this challenge, we propose a forensic detector that is able to express uncertainty in its predictions. This allows detecting test samples for which the training distribution is not representative. More specifically, we propose Bayesian logistic regression as an instance of an infinite ensemble of classifiers. The ensemble agrees in its predictions from test samples similar to the training data but its predictions diverge for unknown test samples. The applicability of the proposed method is evaluated on the task of detecting JPEG double compression. The detector achieves high performance on two goals simultaneously: It accurately detects double-JPEG compression, and it accurately indicates when the test data is not covered by the training data. We assert that the proposed method can assist a forensic analyst in assessing detector reliability and in anticipating failure cases for specific inputs.
Modern industries widely rely upon software and IT services, in a context where cybercrime is rapidly spreading in more and more sectors. Unfortunately, despite greater general awareness of security risks and the availability of security tools that can help to cope with those risks, many organizations (especially medium/small-size ones) still lag when it comes to building security into their services. This is mainly due to the limited security skills of common developers/IT project managers and to the typically high costs of security procedures. In fact, while automated tools exist to perform code analysis, vulnerability scanning, or security testing, the manual intervention of security experts is still required not only for security analysis and design, but also to configure and elaborate the output of the security testing tools. In this paper, we propose a novel secure software development methodology aimed at supporting developers from security design to security testing, suitable for integration within modern DevOps pipelines according to a DevSecOps (or SecDevOps) approach. The proposed methodology leverages a model-based process that enables identifying existing threats, selecting appropriate countermeasures to enforce, and verify their mitigation effectiveness through both static assessment procedures and targeted security tests. To demonstrate our approach's feasibility and concretely illustrate the devised activities, we provide a step-by-step description of the whole process concerning a containerized microservice-based application case study. In addition, we discuss the application of the proposed methodology, in its threat modeling and security testing phases, to a well-known vulnerable web application widely used for security training purposes, to illustrate that we can identify most of the existing vulnerabilities and determine appropriate test plans to assess and mitigate such vulnerabilities.
Phishing leverages people's tendency to share personal information online. Phishing attacks often begin with an email and can be used for a variety of purposes. The cybercriminal will employ social engineering techniques to get the target to click on the link in the phishing email, which will take them to the infected website. These attacks become more complex as hackers personalize their fraud and provide convincing messages. Phishing with a malicious URL is an advanced kind of cybercrime. It might be challenging even for cautious users to spot phishing URLs. The researchers displayed different techniques to address this challenge. Machine learning models improve detection by using URLs, web page content and external features. This article presents the findings of an experimental study that attempted to enhance the performance of machine learning models to obtain improved accuracy for the two phishing datasets that are used the most commonly. Three distinct types of tuning factors are utilized, including data balancing, hyper-parameter optimization and feature selection. The experiment utilizes the eight most prevalent machine learning methods and two distinct datasets obtained from online sources, such as the UCI repository and the Mendeley repository. The result demonstrates that data balance improves accuracy marginally, whereas hyperparameter adjustment and feature selection improve accuracy significantly. The performance of machine learning algorithms is improved by combining all fine-tuned factors, outperforming existing research works. The result shows that tuning factors enhance the efficiency of machine learning algorithms. For Dataset-1, Random Forest (RF) and Gradient Boosting (XGB) achieve accuracy rates of 97.44% and 97.47%, respectively. Gradient Boosting (GB) and Extreme Gradient Boosting (XGB) achieve accuracy values of 98.27% and 98.21%, respectively, for Dataset-2.
This paper examines two recent Canadian legislative initiatives intended in whole or in part to combat cyberbullying: the Cyber-Safety Act of Nova Scotia and the Protecting Canadians from Online Crime Act. Both pieces of legislation were responses to the suicide deaths of female teenagers Rehtaeh Parsons in Nova Scotia and Amanda Todd in British Columbia. It is questionable whether either suicide was directly caused by cyberbullying. Todd was primarily a victim of an adult capper living in the Netherlands, who persuaded her to expose her breasts via web cam, and then attempted to use the captured images to extort money and more images. Parsons was filmed engaging in what police concluded was consensual sex with several teenage boys at a house party. The cell phone video was posted on the Internet, making Parsons a victim of sexting. In 2014, the Supreme Court of Canada fired a shot across the bow of Bill C-30, the immediate predecessor to the Protecting Canadians from Online Crime Act, ruling that a police search (permissible under Bill C-30) in a child pornography case infringed upon constitutional protection against unreasonable search and seizure, and upon the right to privacy in cyberspace. In 2015, the Cyber-Safety Act of Nova Scotia was struck down by the Supreme Court of Nova Scotia because it violated constitutional rights to freedom of expression and to life, liberty and security of person. In a criminal harassment case involving Twitter, the Ontario Court of Justice ruled in 2016 that Twitter is like a public square, where freedom of expression is protected, and where there is no reasonable expectation of privacy. These three court decisions can be expected to limit the extent to which Canadian legislators and law enforcement agencies can go in their efforts to pursue and prosecute cyberbullies and/or cyber criminals.
Within a digital ecosystem, systems and actors form coalitions for achieving common and individual goals. In a constant motion of collaborative and competitive forces and faced with the risk of malicious attacks, ecosystem participants require strong guarantees of their collaborators' trustworthiness. Evidence of trustworthy behavior derived from runtime executions can provide these trust guarantees, given that clear definition and delimitation of trust concerns exist. Without them, a base for negotiating expectations, quantifying achievements and identifying strategical attacks cannot be established and attainment of strategic benefits relies solely on vulnerable collaborations. In this paper we examine the relationship between goals and trust and we introduce a formalism for goal representation. We delimit the trust concerns with anti-goals. The anti-goals set the boundaries within which we structure the trust analysis and build up evidence for motivated attacks.
A nation cannot sustain a highly productive and efficient population without smart cities. Due to their significant reliance on digital technologies, these cities require a high level of cybercrime protection. Cryptocurrencies have gained significant attention due to their secure and reliable infrastructure. The decentralised cryptocurrency operates in a trust-less environment known as the blockchain, where each network participant has a ledger copy of all transactions. Blockchain technology employs a proven consensus mechanism without requiring establishment of a central authority. But the consensus mechanism requires miner to solve a cryptographic problem by generating random hashes until one of them matches the desired one. This procedure is energy-intensive, and when thousands of miners repeat it to verify a single transaction, a substantial amount of electricity is consumed. Moreover, electricity produces a significant amount of carbon footprint. Patch methodology utilises the data of all hashes created per year and the efficiency of mining hardware over a 10-year period to calculate the Bitcoins energy consumption. Due to a large number of unknown and uncertain factors involved, it is difficult to precisely calculate a single value for electricity consumption and carbon footprint as reported by Patch methodology. The proposed method extends the Patch methodology by adding a maximum and minimum limit to the hardware efficiency as well as the sources of power generation, which can help refine estimates of electricity consumption and carbon emissions for a more accurate picture. Using the proposed methodology, it was estimated that Bitcoin consumed between 38.495 and 120.72 terawatt hours of electricity in 2021 and released between 2.12 and 45.37 million metric tonnes of carbon dioxide. To address the issue of excessive energy consumption and carbon emissions, a significant number of individual miners and mining pools are relocating to energy-intensive regions, such as aluminium mining sites that rely on hydroelectricity for energy generation.
The article aims to highlight the usefulness of composite indicators applications as an analytical tool in guiding national security policies related to multidimensional phenomena revealing its role in measuring hybrid threats - as part of global risks.The qualitative and quantitative research shows that the use of composite indicators in measuring hybrid threats at country level is a relatively new topic, that has enjoyed increased interest since 2021.The originality lies in the way it is putted in evidence the link between the composite nature of hybrid threats, and the composite indicators in order the argue the role using this modern tool in the process of forecasting the risks with national impact, by using SWOT analysis.To highlight the relevance of composite indicators in measuring hybrid threats to peace and security, we have chosen to compare the results of measurement made by Normandia Index. The outcomes demonstrate upward trend in hybrid threat values (cybercrime, economic crises, disinformation, energy insecurity and terrorism) at the UE-27 level (as one country), comparative to the world level situation, between 2019-2022.We concluded that using composite indicators in measuring hybrid threats represents an analytical strategical level method that could play a role as an early warning engine to support policy makers and, also as a facilitator instrument of the planning process country level to avoid strategic surprises and prevent security risks.Considering the geopolitical clashes of power poles and the unpredictable situation of the socioeconomic environment, we have concluded that future steps of the research should be related with the dynamics of hybrid threats at the global level. An important contribution could be brought by the development of new systems for interpreting measurement by specialized software, and by the identification of appropriate calculation formulas to reduce misinterpretation due to the lack of real data provided.
Although the number of cloud projects has dramatically increased over the last few years, ensuring the availability and security of project data, services, and resources is still a crucial and challenging research issue. Distributed denial of service (DDoS) attacks are the second most prevalent cybercrime attacks after information theft. DDoS TCP flood attacks can exhaust the cloud's resources, consume most of its bandwidth, and damage an entire cloud project within a short period of time. The timely detection and prevention of such attacks in cloud projects are therefore vital, especially for eHealth clouds. In this paper, we present a new classifier system for detecting and preventing DDoS TCP flood attacks (CS_DDoS) in public clouds. The proposed CS_DDoS system offers a solution to securing stored records by classifying the incoming packets and making a decision based on the classification results. During the detection phase, the CS_DDOS identifies and determines whether a packet is normal or originates from an attacker. During the prevention phase, packets, which are classified as malicious, will be denied to access the cloud service and the source IP will be blacklisted. The performance of the CS_DDoS system is compared using the different classifiers of the least squares support vector machine (LS-SVM), naive Bayes, K-nearest, and multilayer perceptron. The results show that CS_DDoS yields the best performance when the LS-SVM classifier is adopted. It can detect DDoS TCP flood attacks with about 97% accuracy and with a Kappa coefficient of 0.89 when under attack from a single source, and 94% accuracy with a Kappa coefficient of 0.9 when under attack from multiple attackers. Finally, the results are discussed in terms of accuracy and time complexity, and validated using a K-fold cross-validation model.
The emerging trend of ubiquitous and pervasive computing aims at embedding everyday devices such as wristwatches, smart phones, home video systems, autofocus cameras, intelligent vehicles, musical instruments, kitchen appliances etc. with microprocessors and imparts them with wireless communication capability. This advanced computing paradigm, also known as the Internet of Things or cyber-physical computing, leads internet and computing to appear everywhere and anywhere using any device and location. With maximum appreciation and due regards to the evolutionary arc, depth and scope of ceaseless internet utilities, it is equally necessary to envisage the security and data confidentiality challenges posed by the free and ubiquitous availability of internet. Wireless communication, by virtue of a plethora of networked devices, is severely prone to illegal use, unauthorized access, protocol tunneling, eavesdropping, and denial of service attacks as these devices are unknowingly exposed to illegal access from undefined locations. Amidst the rapidly expanding arena of cybercrime, banks, stock exchanges, business transactions and shopping firms such as Amazon and eBay are heavily dependent on internet. The freedom offered by wireless and 3G based internet communication and its open character has led to many incidences of abuse of technology. Unrestricted accessibility to internet has intensified the likelihood of sophisticated attacks, malicious intrusions and malware, capable of inflicting widespread damage on modern human life and economy. The classical intrusion detection systems have been found to be less equipped to handle the magnitude and complexity of wireless networks due to enormous user activities and constantly varying behavior patterns. This paper analyses the role of computational intelligence techniques to design adaptive and cognitive intrusion detection systems that can efficiently detect malicious network activities and proposes novel three-tier architecture for designing intelligent intrusion detection systems for wireless networks.
Companies and individuals are becoming more dependant on technology, automated processes, the Internet of Things (IoT) and daily use of internet, mobile devices, and other tools that the technological revolution has created. But against the back-drop of rapid technological progress, cyber-threats have become a serious challenge that require immediate, continuous action. As cyber-crime poses an ever-present and growing threat, corporate and individual users of the cyber-space are constantly struggling to ensure an acceptable level of security with respect to their assets. Based on an analysis of 4,785 attacks deployed world-wide in recent years, this paper outlines the correlations and patterns identified, with the final objective of defining security countermeasures that organisations from certain business sectors could implement in order to focus their limited resources and budget on mitigating the right risks.
The Internet and social media allow people to spread their views rapidly to a large group of people. While the right to freely express one's ideas and views is a cornerstone in a democratic society, in some cases the Internet can serve as a breeding ground for violent extremism and terrorism. Hence, in order to protect democracy, effective techniques of Internet surveillance are needed. Previous research has shown that there is a connection between word use and psychological states. The text analysis tool Linguistic Inquiry and Word Count (LIWC) counts words in psychologically meaningful categories. Based on the relative frequency of words from the different categories, conclusions can be drawn about the author of for instance a blog text. In this work, we have explored the characteristics of written communication produced by ten different lone offenders prior to their engagement in targeted violence. We found eight possible indicators of the drives and emotions that preceded their attacks.
The increasing use of the internet by minors has made public the risk they run of becoming victims of serious cyber-crime. This awareness has led to the adoption of binding and soft-law instruments in the field of International and EU law aimed at combating new forms of crime and protecting minors in the cyberspace. This paper, after having critically analysed the main acts adopted at International and EU level, focuses on their implementation in the Italian legal system. In doing so, cybercrimes committed by adults against children and those committed among peers will be separately considered. The study shows that, despite the legislator's attempt to carry out the obligations deriving from International and EU Law, there are still several critical profiles in the national law.
Identifying the model of a camera that has captured an image can be an important task in criminal investigations. Many methods assume that the image under analysis originates from a given set of known camera models. In practice, however, a photo can come from an unknown camera model, or its appearance could have been altered by unknown post-processing. In such a case, forensic detectors are prone to fail silently. One way to mitigate silent failures is to use a rejection mechanism for unknown examples. In this work, we propose Gaussian processes (GPs), which intrinsically provide such a rejection mechanism. This makes GPs a potentially powerful tool in multimedia forensics, where forensic analysts regularly work on images from unknown origins. We demonstrate that GPs scale well to the task of camera model identification. Probabilistic predictions from a GP classifier achieve high classification accuracy for known camera models while providing reliable uncertainty estimates. The built-in uncertainty estimates effectively tackle open-set camera model identification, outperforming two state-of-the-art methods.
FPGAs offer fast and reliable near-data processing and are therefore suitable candidates for implementing IoT and edge computing systems. As they are usually deployed in exposed locations, they are vulnerable to physical attacks, especially Side-Channel Analysis (SCA). In this paper, we characterize side-channels and how they can be exploited for SCA on FPGA-based off-the-shelf boards, i.e. without having to make any modifications to the board, hardware, or software. The basic requirement for any kind of SCA is that the individual Cryptographic Operations (COs) in the side-channel traces can be detected. To this end, we apply a SCA for semi-automatic CO detection that can be generically applied off-the-shelf to a wide variety of boards. Additionally, we introduce a new metric called Signal of COs to Noise Ratio (SCONR), that allows to quantify the pronouncedness of COs versus noise in a side channel. We then evaluate side channels measured on three different boards containing Xilinx 7 series FPGAs. We further investigate the influence of other sources of noise and how much they affect the attackability of a system. Our results show that FPGAs have a high vulnerability to SCA in general and that even noise from an operating system will not hinder the recording and finding of COs in an automated fashion as long as there are no countermeasures in place. Finally, SCONR converges after fewer recorded traces and gives a clearer indication whether a side channel is susceptible to this type of automated attack than leakage assessment techniques such as TVLA.
We introduce a new measure on regular languages: their non-deterministic syntactic complexity. It is the least degree of any extension of the 'canonical boolean representation' of the syntactic monoid. Equivalently, it is the least number of states of any subatomic nondeterministic acceptor. It turns out that essentially all previous structural work on non-deterministic state-minimality computes this measure. Our approach rests on an algebraic interpretation of nondeterministic finite automata as deterministic finite automata endowed with semilattice structure. Crucially, the latter form a self-dual category.
Cybersecurity incident analysis is an exploratory, data-driven process over records and logs from network monitoring tools. The process is rarely linear and frequently breaks down into multiple investigation branches. Analysts document all the steps and lessons learned and suggest mitigations. However, current tools provide only limited support for analytical provenance. As a result, analysts have to record all the details regarding the performed steps and notes in separate documents. Such a procedure increases their cognitive demands and is naturally error-prone. This paper proposes a conceptual design of the analytical tool implementing means of analytical provenance in cybersecurity incident analysis workflows. We identified the user requirements and designed and implemented a proof of concept prototype application Incident Analyzer. Qualitative feedback from four domain experts confirmed that our approach is promising and can significantly improve current cybersecurity and network incident analysis practices.
"The WTFast's Gamers Private Network (GPN (R)) is a client/server solution that makes online games faster. GPN (R) connects online video-game players with a common game service across a wide-area network. Online games are interactive competitions by individual players who compete in a virtual environment. Response time, latency and its predictability are keys to GPN (R) success and runs against the vast complexity of internet-wide systems. We have built an experimental network of virtualized GPN (R) components so as to carefully measure the statistics of latency for distributed Minecraft games and to do so in a controlled laboratory environment. This has led to a better understanding of the coupling between parameters such as: the number of players, the subset of players that are idle or active, the volume of packets exchanged, the size of packets, latency to and from the game servers, and time-series for most of those parameters. In this paper we present a mathematical model of those system game network parameters and show how it leads to: (1) realistic simulation of each of those network or game parameters, without relying on the experimental setup; (2) very large-scale numerical simulation of the game setup so as to explore various internet-wide performance scenarios that: (a) are impossible to isolate from internet noise in their real environment and; (b) would require vast supercomputing resources if they were to be simulated exhaustively. We motivate all elements of our mathematical model and estimate the savings in computational costs they will bring for very large-scale simulation of the GPN (R). Such simulations will improve quality of service for GPN (R) systems and their reliability."
Social networking sites have billions of users who communicate and share their personal information every day. Social engineering is considered one of the biggest threats to information security nowadays. Social engineering is an attacker technique to manipulate and deceive users in order to access or gain privileged information. Such attacks are continuously developed to deceive a high number of potential victims. The number of social engineering attacks has risen dramatically in the past few years, causing unpleasant damage both to organizations and individuals. Yet little research has discussed social engineering in the virtual environments of social networks. One approach to counter these exploits is through research that aims to understand why people fall victim to such attacks. Previous social engineering and deception research have not satisfactory identified the factors that influence the users' ability to detect attacks. Characteristics that influence users' vulnerability must be investigated to address this issue and help to build a profile for vulnerable users in order to focus on increasing the training programs and education for those users. In this context, the present study proposes a user-centric framework to understand the user's susceptibility, relevant factors and dimensions.
Webpages with terrorist and extremist content are key factors in the recruitment and radicalization of disaffected young adults who may then engage in terrorist activities at home or fight alongside terrorist groups abroad. This paper reports on advances in techniques for classifying data collected by the Terrorism and Extremism Network Extractor (TENE) web-crawler, a custom-written program that browses the World Wide Web, collecting vast amounts of data, retrieving the pages it visits, analyzing them, and recursively following the links out of those pages. The textual content is subjected to enhanced classification through software analysis, using the Posit textual analysis toolset, generating a detailed frequency analysis of the syntax, including multi-word units and associated part-of-speech components. Results are then deployed in a knowledge extraction process using knowledge extraction algorithms, e.g., from the WEKA system. Indications are that the use of the data enrichment through application of Posit analysis affords a greater degree of match between automatic and manual classification than previously attained. Furthermore, the incorporation and deployment of these technologies promises to provide public safety officials with techniques that can help to detect terrorist webpages, gauge the intensity of their content, discriminate between webpages that do or do not require a concerted response, and take appropriate action where warranted.
Ensuring the security of built-in apparatuses has become an important task because of advancements in Internet of Things (IoT) technology. Therefore, lightweight ciphers that are available in built-in apparatuses have attracted the attention of many researchers. However, the danger of electromagnetic analysis attacks against cryptographic circuits has been pointed out. Electromagnetic analysis attacks illegally analyze confidential information using the electromagnetic waves that are generated during the operation of a cryptographic circuit. Many studies have reported on power analysis attacks against the advanced encryption standard (AES). However, as far as we know, no study has reported on electromagnetic analysis attacks against PRINCE, which is one of the most popular lightweight ciphers. The present study proposes a method for electromagnetic analysis attacks against PRINCE in order to evaluate the tamper resistance of PRINCE. The present study also verifies the validity of the proposed method by performing an evaluation experiment using a field-programmable gate array (FPGA).
As cybercriminals scale up their operations to increase their profits or inflict greater harm, we argue that there is an equal need to respond to their threats by scaling up cyber-security. To achieve this goal, we have to develop a co-productive approach towards data collection and sharing by overcoming the cybersecurity data sharing paradox. This is where we all agree on the definition of the problem and end goal (improving cybersecurity and getting rid of cybercrime), but we disagree about how to achieve it and fail to work together efficiently. At the core of this paradox is the observation that public interests differ from private interests. As a result, industry and law enforcement take different approaches to the cybersecurity problem as they seek to resolve incidents in their own interests, which manifests in different data sharing practices between both and also other interested parties, such as cybersecurity researchers. The big question we ask is can these interests be reconciled to develop an interdisciplinary approach towards co-operation and sharing data. In essence, all three will have to co-own the problem in order to co-produce a solution. We argue that a few operational models with good practices exist that provide guides to a possible solution, especially multiple third-party ownership organisations which consolidate, anonymise and analyse data. To take this forward, we suggest the practical solution of organising coproductive data collection on a sectoral basis, but acknowledge that common standards for data collection will also have to be developed and agreed upon. We propose an initial set of best practices for building collaborations and sharing data and argue that these best practices need to be developed and standardised in order to mitigate the paradox.
One of the main challenges in digital forensics is the increasing volume of data that needs to be analyzed. This problem has become even more pronounced with the emergence of big data and calls for a rethink on the way digital forensics investigations have been handled over the past years. This paper briefly discusses the challenges and needs of digital forensics in the face of the current trends and requirements of different investigations. A digital forensics analysis framework that puts into consideration the existing techniques as well as the current challenges is proposed. The purpose of the framework is to reassess the various stages of the digital forensics examination process and introduce into each stage the required techniques to enhance better collection, analysis, preservation and presentation in the face of big data and other challenges facing digital forensics.
Terrorism research has begun to focus on the issue of radicalization, or the acceptance of ideological belief systems that lead toward violence. There has been particular attention paid to the role of the Internet in the exposure to and promotion of radical ideas. There is, however, minimal work that attempts to model the ways that messages are spread or how individual participation in radical on-line communities operates. In this paper, we present a stochastic linear system to represent the evolution of contribution to a sample of 126 threads in an on-line forum where individuals discuss radical belief systems. To estimate or predict the time-varying contributions of agents for given online-forum data, each agent's contribution has been modeled as a state variable. We then use the expectation-maximization (EM) algorithm to identify the model parameters including the adjacency matrix of the graph constructed among participating agents along with measurement and system uncertainty levels in online-postings. Our approach reveals the identified dynamical influences among agents in the time-varying shaping of the contribution in a data-driven fashion. We use the real-world data from online-postings to demonstrate the usefulness of our approach, and its application toward on-line radicalization.
As the smartphone and the services it provides are becoming targets of cybercrime, it is critical to secure smartphones. However, it is important security controls are designed to provide continuous and userfriendly security. Amongst the most important of these is user authentication, where users have experienced a significant rise in the need to authenticate to the device and individually to the numerous apps that it contains. Gait authentication has gained attention as a mean of non-intrusive or transparent authentication on mobile devices, capturing the information required to verify the authenticity of the user whilst the person is walking. Whilst prior research in this field has shown promise with good levels of recognition performance, the results are constrained by the gait datasets utilised being based upon highly controlled laboratory-based experiments which lack the variability of real-life environments. This paper introduces an advanced real-world smartphone-based gait recognition system that recognises the subject within real-world unconstrained environments. The proposed model is applied to the uncontrolled gait dataset, which consists of 44 users over a 7-10 day capture - where users were merely asked to go about their daily activities. No conditions, controls or expectations of particular activities were placed upon the participants. The experiment has modelled four types of motion normal walking, fast walking and down and upstairs for each of the users. The evaluation of the proposed model has achieved an equal error rate of 11.38%, 11.32%, 24.52%, 27.33% and 15.08% for the normal, fast, down and upstairs and all activities respectively. The results illustrate, within an appropriate framework, that gait recognition is a viable technique for real-world use. (C) 2021 The Author(s). Published by Elsevier Ltd.
As cyber threats endanger everyone, from regular users to computing professionals, spreading cybersecurity awareness becomes increasingly critical. Therefore, our university designed an innovative cybersecurity awareness course that is freely available online for students, employees, and the general public. The course offers simple, actionable steps that anyone can use to implement defensive countermeasures. Compared to other resources, the course not only suggests learners what to do, but explains why and how to do it. To measure the course impact, we administered it to 138 computer science undergraduates within a compulsory information security and cryptography course. They completed the course as a part of their homework and filled out a questionnaire after each lesson. Analysis of the questionnaire responses revealed that the students valued the course highly. They reported new learning, perspective changes, and transfer to practice. Moreover, they suggested suitable improvements to the course. Based on the results, we have distilled specific insights to help security educators design similar courses. Lessons learned from this study are relevant for cybersecurity instructors, course designers, and educational managers.
Tampered multimedia content is being increasingly used in a broad range of cybercrime activities. The spread of fake news, misinformation, digital kidnapping, and ransomware-related crimes are amongst the most recurrent crimes in which manipulated digital photos and videos are the perpetrating and disseminating medium. Criminal investigation has been challenged in applying machine learning techniques to automatically distinguish between fake and genuine seized photos and videos. Despite the pertinent need for manual validation, easy-to-use platforms for digital forensics are essential to automate and facilitate the detection of tampered content and to help criminal investigators with their work. This paper presents a machine learning Support Vector Machines (SVM) based method to distinguish between genuine and fake multimedia files, namely digital photos and videos, which may indicate the presence of deepfake content. The method was implemented in Python and integrated as new modules in the widely used digital forensics application Autopsy. The implemented approach extracts a set of simple features resulting from the application of a Discrete Fourier Transform (DFT) to digital photos and video frames. The model was evaluated with a large dataset of classified multimedia files containing both legitimate and fake photos and frames extracted from videos. Regarding deepfake detection in videos, the Celeb-DFv1 dataset was used, featuring 590 original videos collected from YouTube, and covering different subjects. The results obtained with the 5-fold cross-validation outperformed those SVM-based methods documented in the literature, by achieving an average F1-score of 99.53%, 79.55%, and 89.10%, respectively for photos, videos, and a mixture of both types of content. A benchmark with state-of-the-art methods was also done, by comparing the proposed SVM method with deep learning approaches, namely Convolutional Neural Networks (CNN). Despite CNN having outperformed the proposed DFT-SVM compound method, the competitiveness of the results attained by DFT-SVM and the substantially reduced processing time make it appropriate to be implemented and embedded into Autopsy modules, by predicting the level of fakeness calculated for each analyzed multimedia file.
"Terrorist organizations pursue political goals, seeking to achieve them through violence. At the same time, communication remains one of the main aspects of their activities. Their propaganda, recruitment and searches for funding are not just in the digital arena, but also involve the use of a wide range of sophisticated technologies-new encryption tools, crypto currencies, operations in the darknet, etc. At the same time, more and more crimes are committed with the help of social engineering tools (psychological manipulation in order to induce a person to perform certain actions or share confidential information). Given the importance for terrorists of influencing the public consciousness, as well as the convergence of terrorism and cybercrime, terrorist organizations and lone-wolf terrorists can actively use the mechanisms of social engineering in their psychological operations. This threat to the psychological security of society (and in some cases, its physical security) is already a reality. It can become even more relevant due to the development and spread of AI technologies, which (if used maliciously) can facilitate the tasks of social engineering even for criminals without special technical knowledge. Much more dangerous may be a turn to terrorism by specialists who are able to develop AI tools that will initially be designed for psychological manipulation. This could be as simple as AI that offers a user content on certain topics in social media designed to surround the victim of recruitment with content that glorifies terrorism and persuade them to transfer money to a terrorist organization. This paper aims to find out the answers to four main research questions: 1) what are the current and future factors in the turn to terrorism of AI specialists; 2) how can existing and (possibly) future AI technologies make it easier for criminals to perform social engineering tasks; 3) how can terrorist organizations and lonewolf terrorists use social engineering tools through AI to achieve their goals; and 4) what means of countering this threat already exist and can be created in the future?"
The exposure of students to the internet and the digital world has increased many folds in the last two years because of the pandemic situations. A complete shift to online education and giving gadgets to every little hand was an emergency response to the world-wide lockdown situation.By choice or due to having no choice, the internet and virtual world have become an inseparable part of our lives now.The world is developing rapidly as a hub of technology, and it has affected out lives in exciting ways, but it has given rise to new problems also. We read news of misuse of social networking websites, cyber bullying, and online scams. There are cases of credit card scams, illegal downloads and copying, addiction of games to harmful level, viruses, pornography, and hate sites.With the increased exposure of children to the internet and the digital world, the cybercrimes against children are also increasing at an alarming rate. According to the latest National Crime Record Bureau data, there has been an increase of 400 percent in cybercrime against children in 2020 in comparison to 2019 in India. Now, it's time for us to think about what should be the do's and don'ts of this newly evolving world or in other words we can say that how to become digital citizens.Someone who has a place of its own in the digital world is called as a Digital Citizen. A digital citizenship comes with digital rights and responsibilities, which are for the purpose of protection of digital citizens.In this world full of technology, it is crucial to teach students about Digital Citizenship. In a world where students have devices like mobiles and tablets in their hands from infancy, it becomesvery important to train students for digital citizenship accordingly. This paper talks about the digital role and responsibilities of every digital citizen. This paper emphasises on the importance of the teacher's role to prepare digital citizens of tomorrow. It also talks about the content and methodology to teach digital citizenship.
Until recently, systems and networks have been designed to implement established actions within known contexts. However, gaining thehumantrust in system behavior requires development of artificial ethical agents proactively acting outside fixed context boundaries for mitigating dangerous situations in which other interacting entities find themselves. A proactive altruistic behavior oriented towards removing danger needs to rely on predictive awareness of a dangerous situation. Different that current approaches for designing cognitive architectures, in this paper, we introduce a method that enables the creation of artificial altruistic trusted behavior together with an architecture of the framework that enables its implementation.
Cyberattacks perpetrated by insiders are difficult to prevent using traditional security approaches. Often, such attackers misuse legitimate access to the system to conduct an attack, or an external attacker manipulates or masquerades as an insider to gain access, bypassing the security controls. A possible solution to this problem are forensic-ready software systems that support the eventual forensic investigation. For example, assuring that appropriate evidence of an attack would be generated and assessable if needed. While not primarily aimed at prevention, the controls of forensic-ready systems can be used to ensure reliable post-incident investigation in the case of an insider attack. Currently, however, there is a gap in adequate methods for identifying requirements and assessment of such systems. Therefore, we propose FR-ISSRM, a risk management approach to derive the forensic readiness requirements addressing insider attacks. The requirements, once implemented, assist in the reliable uncovering culprit, root cause, damage of the attack, and overall improvement of security posture. The approach is then demonstrated in three cases covering typical insider attacks.
Purpose Phishing attacks have evolved in recent years due to high-tech-enabled economic growth worldwide. The rise in all types of fraud loss in 2019 has been attributed to the increase in deception scams and impersonation, as well as to sophisticated online attacks such as phishing. The global impact of phishing attacks will continue to intensify, and thus, a more efficient phishing detection method is required to protect online user activities. To address this need, this study focussed on the design and development of a deep learning-based phishing detection solution that leveraged the universal resource locator and website content such as images, text and frames. Design/methodology/approach Deep learning techniques are efficient for natural language and image classification. In this study, the convolutional neural network (CNN) and the long short-term memory (LSTM) algorithm were used to build a hybrid classification model named the intelligent phishing detection system (IPDS). To build the proposed model, the CNN and LSTM classifier were trained by using 1m universal resource locators and over 10,000 images. Then, the sensitivity of the proposed model was determined by considering various factors such as the type of feature, number of misclassifications and split issues. Findings An extensive experimental analysis was conducted to evaluate and compare the effectiveness of the IPDS in detecting phishing web pages and phishing attacks when applied to large data sets. The results showed that the model achieved an accuracy rate of 93.28% and an average detection time of 25 s. Originality/value The hybrid approach using deep learning algorithm of both the CNN and LSTM methods was used in this research work. On the one hand, the combination of both CNN and LSTM was used to resolve the problem of a large data set and higher classifier prediction performance. Hence, combining the two methods leads to a better result with less training time for LSTM and CNN architecture, while using the image, frame and text features as a hybrid for our model detection. The hybrid features and IPDS classifier for phishing detection were the novelty of this study to the best of the authors' knowledge.
Phishers sometimes exploit users' trust of a known website's appearance by using a similar page that looks like the legitimate site. In recent times, researchers have tried to identify and classify the issues that can contribute to the detection of phishing websites. This study focuses on design and development of a deep learning based phishing detection solution that leverages the Universal Resource Locator and website content such as images and frame elements. A Convolutional Neural Network (CNN) and the Long Short-Term Memory (LSTM) algorithm were used to build a classification model. The experimental results showed that the proposed model achieved an accuracy rate of 93.28%.
Purpose Computer games that teach cybersecurity concepts have been developed to help both individuals and organizations shore up their defence against cybercrimes. Evidence of the effectiveness of these games has been rather weak, however. This paper aims to guide the design and testing of more effective cybersecurity educational games by developing a theoretical framework. Design/methodology/approach A review of the literature is conducted to explore the dependent variable of this research stream, learning outcomes and its relationship with four independent variables, game characteristics, game context, learning theory and user characteristics. Findings The dependent variable can be measured by five learning outcomes: information, content, strategic knowledge, eagerness to learn/time spent and behavioral change. Game characteristics refer to features that contribute to a game's usefulness, interactivity, playfulness or attractiveness. Game context pertains to factors that determine how a game is used, including the target audience, the skill involved and the story. Learning theory explains how learning takes place and can be classified as behaviorism, cognitivism, humanism, social learning or constructivism. User characteristics including gender, age, computer experience, knowledge and perception, are attributes that can impact users' susceptibility to cybercrimes and hence learning outcomes. Originality/value The framework facilitates taking stock of past research and guiding future research. The use of the framework is illustrated in a critique of two research streams. Multiple research directions are discussed for continued research into the design and testing of next-generation cybersecurity computer games.
Security breaches due to attacks by malicious software (malware) continue to escalate posing a major security concern in this digital age. With many computer users, corporations, and governments affected due to an exponential growth in malware attacks, malware detection continues to be a hot research topic. Current malware detection solutions that adopt the static and dynamic analysis of malware signatures and behavior patterns are time consuming and have proven to be ineffective in identifying unknown malwares in real-time. Recent malwares use polymorphic, metamorphic, and other evasive techniques to change the malware behaviors quickly and to generate a large number of new malwares. Such new malwares are predominantly variants of existing malwares, and machine learning algorithms (MLAs) are being employed recently to conduct an effective malware analysis. However, such approaches are time consuming as they require extensive feature engineering, feature learning, and feature representation. By using the advanced MLAs such as deep learning, the feature engineering phase can be completely avoided. Recently reported research studies in this direction show the performance of their algorithms with a biased training data, which limits their practical use in real-time situations. There is a compelling need to mitigate bias and evaluate these methods independently in order to arrive at a new enhanced method for effective zero-day malware detection. To fill the gap in the literature, this paper, first, evaluates the classical MLAs and deep learning architectures for malware detection, classification, and categorization using different public and private datasets. Second, we remove all the dataset bias removed in the experimental analysis by having different splits of the public and private datasets to train and test the model in a disjoint way using different timescales. Third, our major contribution is in proposing a novel image processing technique with optimal parameters for MLAs and deep learning architectures to arrive at an effective zero-day malware detection model. A comprehensive comparative study of our model demonstrates that our proposed deep learning architectures outperform classical MLAs. Our novelty in combining visualization and deep learning architectures for static, dynamic, and image processing-based hybrid approach applied in a big data environment is the first of its kind toward achieving robust intelligent zero-day malware detection. Overall, this paper paves way for an effective visual detection of malware using a scalable and hybrid deep learning framework for real-time deployments.
"<< SISMI-Telecom scandal >>, as it is known in Italy, is illegal phone tapping by some people in charge of security at the Telecom Italia Company. The case became common knowledge in September 2006 because 34 people were indicted and there were 21 provisional arrests among many employees at Telecom Italia, among national police and members of the Carabinieri Corps (paramilitary police of the Italian Armed Forces) and of the Guardia di Finanza (Inland Revenue Police)>>. In 2010, some journalists revealed what was happening and stated that thousands of people had been secretly put under unauthorised surveillance by Telecom Italia and illegal dossiers had been created. People's lives were monitored as well as their bank accounts; even the data banks of the Italian Ministry of the Interior were accessed. Ultimately, the SISMI-Telecom trial ended with very mild sentences between settlements and state secrets. Yet, in the information age, the SISMI-Telecom scandal can be considered dead only from a procedural point of view. According to the interpretation of some experts on the case, the famous law that has destroyed and ordered the destruction of those dossiers, is a suicide law because those files were all in electronic format, and of course continue to circulate and keep producing poisons."
Cybercrime in the past decade has experienced an all-time high due to the inclusion of so-called smart devices in our daily lives. These tiny devices with brittle security features are often dubbed as the Internet of Things (IoT). Their inclusion is not only limited to our daily lives but also in different fields, for example, healthcare, smart-industries, aviation, and smart-cities. Although IoT devices make our lives easy and perform our jobs in a smart way, but their fragile security mechanisms pose a severe challenge regarding safety and privacy of its users. Attacks like Stuxnet, and Mirai-botnet are the key examples of the damages that can be caused by maliciously controlling these devices. One effective tool to identify a malicious entity at a network device is to perform Remote Attestation (RA). However, performing RA over a large, heterogeneous IoT network is difficult tasks due to resource constrain nature of these networks. To this end, we propose a novel scheme called SARP, which is an attestation-assisted secure and scalable routing protocol for IoT networks. SARP performs attestation in large scale IoT networks by using Routing Protocol for Low Power and Lossy Networks (RPL) framework and exploiting the inbuilt features of RPL. In particular, SARP uses attestation technique that not only secures the network from internal attacks, but it also provides security to RPL's data communication process, which helps to improve the overall network performance. Moreover, SARP supports network mobility, device heterogeneity, and network scalability, while it does not sacrifice the key requirements of IoT networks such as low energy and memory consumption, and low network overhead. The simulation results obtained in different IoT scenarios in presence of various types of attacks show the effectiveness of SARP, concerning energy consumption, packet delivery ratio, network overhead, data integrity, and communication security. (C) 2019 Elsevier B.V. All rights reserved.
Introduction. Current trends in the development of socio-economic processes aim at building the digital economy and society, but the spread of COVID-19 has exposed the major problems related to these processes. Problem Statement. The pandemic has defined a new fundamental and applied problem regarding the need to study crisis phenomena at the intersection of the number of research areas and their impact on economy and society. Purpose. The purpose is to analyze the state of the development and implementation of digital services in the country under the influence of quarantine restrictions. Materials and Methods. State legislative acts and statistical information have been selected as the information base. A number of methods of scientific inquiry have been applied: theoretical generalization, empirical analysis, comparison, abstraction, etc. Results. It has been defined that majority of the priority areas of digital economy development is being formed and not ready for their massive use. In particular, there still exists a significant inequality of access to digital services, the digital competency training system remains unregulated, the digital job creation scheme in the real economy is almost undeveloped, there are significant gaps in the digital public security services, the problem of the national digital educational platforms has not been resolved yet, the system of provision of remote healthcare services has not been formed, and the procedures for estimating tourist migration and for remote financial services have not been settled. Several problems that aggravated during the pandemic have been studied: the imperfection of social protection procedures, the existence of hidden unemployment, the activation of cybercrime, the aggravation of individual psychological problems, extremely high level of dependence on reliability of the network services. The directions for increasing financial results in the pandemic conditions have been identified. Conclusions. The pandemic has identified bottlenecks in the processes of the digital economy and society formation and allowed determining the priority areas in research and practical actions in order to adapt rapidly to new conditions.
"Image forgery is the intentional alteration of digital images, either manually using image editors or through deep fake techniques, for the purpose of disseminating fake information. We propose a forgery detection approach that efficiently detects copy-move and splicing attacks of varying scales in digital images. Our goal is to identify the homogeneous region(s) inconsistent with the rest of the image. This region property has been typically employed in object detection and classification, while we exploit this property to detect forgery in images. Thus, we generate the deep-derived features from the existing hand-crafted features in forgery detection as input to the VGG16, a deep learning method, trained for object classification. We use a binary class SVM trained on the obtained deep-derived features to determine whether an image is real or fake. We perform extensive experiments on three publicly available image manipulation datasets, DVMM, Casia and Korus to validate the effectiveness of the proposed methodology. The results show a better accuracy compared to the state-of-the-art methods.& COPY; 2023 Elsevier B.V. All rights reserved."
"Background: Health care data breaches are the most rapidly increasing type of cybercrime; however, the predictors of health care data breaches are uncertain. Objective: This quantitative study aims to develop a predictive model to explain the number of hospital data breaches at the county level. Methods: This study evaluated data consolidated at the county level from 1032 short-term acute care hospitals. We considered the association between data breach occurrence (a dichotomous variable), predictors based on county demographics, and socioeconomics, average hospital workload, facility type, and average performance on several hospital financial metrics using 3 model types: logistic regression, perceptron, and support vector machine. Results: The model coefficient performance metrics indicated convergent validity across the 3 model types for all variables except bad debt and the factor level accounting for counties with >20% and up to 40% Hispanic populations, both of which had mixed coefficient directionality. The support vector machine model performed the classification task best based on all metrics (accuracy, precision, recall, F1-score). All the 3 models performed the classification task well with directional congruence of weights. From the logistic regression model, the top 5 odds ratios (indicating a higher risk of breach) included inpatient workload, medical center status, pediatric trauma center status, accounts receivable, and the number of outpatient visits, in high to low order. The bottom 5 odds ratios (indicating the lowest odds of experiencing a data breach) occurred for counties with Black populations of >20% and <40%, >80% and <100%, and >40% but <60%, as well as counties with <= 20% Asian or between 80% and 100% Hispanic individuals. Our results are in line with those of other studies that determined that patient workload, facility type, and financial outcomes were associated with the likelihood of health care data breach occurrence. Conclusions: The results of this study provide a predictive model for health care data breaches that may guide health care managers to reduce the risk of data breaches by raising awareness of the risk factors."
Dynamic changes within the cyberspace are greatly impacting human lives and our societies. Emerging evidence indicates that without an ethical overlook on technological progress, intelligent solutions created to improve and enhance our lives can easily be turned against humankind. In complex AI-socio-technical ecosystems where humans, AI (Artificial Intelligence) and systems interact without a common language for building trust, this paper introduces a methodological concept of Ethical Digital Identities for supporting the ethical evaluation of intelligent digital assets.
The dynamic forces that transit back and forth traditional boundaries of system development have led to the emergence of digital ecosystems. Within these, business gains are achieved through the development of intelligent control that requires a continuous design and runtime co-engineering process endangered by malicious attacks. The possibility of inserting specially crafted faults capable to exploit the nature of unknown evolving intelligent behavior raises the necessity of malicious behavior detection at runtime. Adjusting to the needs and opportunities of fast AI development within digital ecosystems, in this paper, we envision a novel method and framework for runtime predictive evaluation of intelligent robots' behavior for assuring a cooperative safe adjustment.
"Internet technology keeps transforming the nature and processes of organizational goals because of its capacity to assist and enhance operational and managerial performance in both the business and non-business industries. However, the presence of cybersecurity threat and privacy issues have become rampant nowadays, considering the magnitude of digital-related transactions, particularly on internet banking systems. The harmful impact perceived from the said threats negatively affects the confidentiality, integrity, and availability of information to financial institutions (bank) and its customers. Again, the perception formed out of this displeasure (cybersecurity threat) interferes with bank customer's decision process regarding the e-banking adoption (or engagement) and retention. In view of this, the present article seeks to; (1) review empirical constructs (cybersecurity threats) in the previous work regarding customer's perception of cybersecurity threats that impedes e-banking adoption and retention and (2) propose a conceptual framework depicting the direct and indirect relationship among the constructs studied. Since the present study is entirely based on a conceptual study, we deployed both document and content analysis as a composite technique for executing the general aim of the study. Findings show that knowledge of perceived identity theft, perceived impersonation, and perceived account hijacked are regarded as cybersecurity threats from the customer's viewpoint that impedes their e-banking adoption and retention. The review discovered that victims (bank customers) of such perceived threats are apparently skeptical in their quest to engage in e-banking adoption and retention. Similarly, the cybersecurity threat-related information, however, poses a danger to users and a sense of unwillingness to engage or retain in e- banking transaction space. Practically, the survival of every business largely depends on its customer base (retention), hence this study propels the financial institutions as a reminder to strategically strengthen their security and privacy concern inasmuch as cybercrime in the banking sector is concerned. In theory, the paper adds up to the concept of cybersecurity repercussion especially in the scope of internet banking. Limitations of this present study are highlighted in the concluding part of the entire paper."
The security of computer systems often relies upon decisions and actions of end users. In this paper, we set out to investigate users' susceptibility to cybercriminal attacks by concentrating at the most fundamental component governing user behavior-the human brain. We introduce a novel neuroscience-based study methodology to inform the design of user-centered security systems as it relates to cybercrime. In particular, we report on an functional magnetic resonance imaging study measuring users' security performance and underlying neural activity with respect to two critical security tasks: 1) distinguishing between a legitimate and a phishing website and 2) heeding security (malware) warnings. We identify the neural markers that might be controlling users' performance in these tasks, and establish relationships between brain activity and behavioral performance as well as between users' personality traits and security behavior. Our results provide a largely positive perspective on users' capability and performance vis-a-vis these crucial security tasks. First, we show that users exhibit significant brain activity in key regions associated with decision-making, attention, and problem-solving (phishing and malware warnings) as well as language comprehension and reading (malware warnings), which means that users are actively engaged in these security tasks. Second, we demonstrate that certain individual traits, such as impulsivity measured via an established questionnaire, are associated with a significant negative effect on brain activation in these tasks. Third, we discover a high degree of correlation in brain activity (in decision-making regions) across phishing detection and malware warnings tasks, which implies that users' behavior in one task may potentially be predicted by their behavior in the other. Fourth, we discover high functional connectivity among the core regions of the brain, while users performed the phishing detection task. Finally, we discuss the broader impacts and implications of our work on the field of user-centered security, including the domain of security education, targeted security training, and security screening.
"New century turns out the intensive scientific revolution, which leads to the extension the digital technologies. The research is devoted to the analysis the opportunities and prospects for the implementation of the artificial intelligence in the legal system. The urgency of the study predetermines by the large-scale digital revolution that affects all spheres of society, including the area of legal activity (for example, the initiative to use lawyers-robots in the legal corporations; the idea to automate legal activity; the appearance of smart contracts, blockchain technologies, cryptocurrency, which are not regulated by the Russian law yet; increase if the cybercrime and others). The purpose of the study is to analyze the advantages and disadvantages, opportunities and limits of introducing the digital technologies into the legal environment. Research methods are analysis, synthesis and comparative law. The study contains several positions. Firstly, the analysis the points of view about the theme of research among the national and foreign scientists are presented. Secondly, the comprehensive assessment of the artificial intelligence influence on the legal sphere is given. Thirdly, the opportunities for regulation changed relationships, connected with digital technologies, in the current Russian legislation, and the advantages and disadvantages of fixing new categories in the Civil Code of the Russian Federation are researched. Fourthly, examples of the negative impact of legal vacuum on the law enforcement practice and the ways for its overcoming are given. By the way, specific decisions of the courts given as the arguments. Fifthly, the analysis of current trends in the introduction of digital technologies in the legal sphere in the Russian Federation and in the over countries is carried out. There are some contradictory opinions of scientists and practitioners regarding the possibility of using robotic technologies in the legal system. Sixthly, it presents own conclusion based on the conducted research, which is to substantiate the positive trend towards digitalization in the legal system, but the negativity of the possible effects excessive interference of the artificial intelligence in the legal activity."
Phishing is a social engineering cyberattack where criminals deceive users to obtain their credentials through a login form that submits the data to a malicious server. In this paper, we compare machine learning and deep learning techniques to present a method capable of detecting phishing websites through URL analysis. In most current state-of-the-art solutions dealing with phishing detection, the legitimate class is made up of homepages without including login forms. On the contrary, we use URLs from the login page in both classes because we consider it is much more representative of a real case scenario and we demonstrate that existing techniques obtain a high false-positive rate when tested with URLs from legitimate login pages. Additionally, we use datasets from different years to show how models decrease their accuracy over time by training a base model with old datasets and testing it with recent URLs. Also, we perform a frequency analysis over current phishing domains to identify different techniques carried out by phishers in their campaigns. To prove these statements, we have created a new dataset named Phishing Index Login URL (PILU-90K), which is composed of 60K legitimate URLs, including index and login websites, and 30K phishing URLs. Finally, we present a Logistic Regression model which, combined with Term Frequency - Inverse Document Frequency (TF-IDF) feature extraction, obtains 96.50% accuracy on the introduced login URL dataset.
"Nowadays, the speed up development and use of digital devices such as smartphones have put people at risk of internet crimes. The evidence of present crimes in a computer file can be easily unreachable by changing the prefix of a file or other algorithms. In more complex cases, either file divided into different parts or the parts of a file that has information about the file type are deleted, where the file fragment recognition issue is discussed. The known files are divided into different fragments, and different classification algorithms are used to solve the problems of file fragment recognition. A confusion matrix measures the accuracy of type recognition. The issue of identifying the type of file fragment due to its importance in cybercrime issues as well as antivirus has been highly emphasized and has been addressed in many articles. Increasing the accuracy in this field on the types of widely used files due to the sensitivity of the subject of recognizing the type of file under study is the main goal of researchers in this field. Failure to identify the correct type of file will lead to deviations of the results and evidence from the main issue or failure to conclude. In this paper, first, the file is divided into different fragments. Then, the file fragment features, which are obtained from Binary Frequency Distribution (BFD), are reduced by 2 feature reduction algorithms; Sequential Forward Selection algorithm (SFS) as well as Sequential Floating Forward Selection algorithm (SFFS) to delete sparse features that result in increased accuracy and speed. Finally, the reduced features are given to 3 Multiclass classifier algorithms, Multilayer Perceptron (MLP), Support Vector Machines (SVM), and K-Nearest Neighbor (KNN) for classification and comparison of the results. The proposed recognition algorithm can recognize 6 types of useful files (PDF, TXT, JPG, DOC, HTML, EXE) and may distinguish a type of file fragments with higher accuracy than the similar works done."
Cross-Site Scripting (XSS) - around fourteen years old vulnerability is still on the rise and a continuous threat to the web applications. Only last year, 150505 defacements (this is a least, an XSS can do) have been reported and archived in Zone-H (a cybercrime archive) (http://www.zone-h.org/). The online WYSIWYG (What You See Is What You Get) or rich-text editors are now a days an essential component of the web applications. They allow users of web applications to edit and enter HTML rich text (i.e., formatted text, images, links and videos etc.) inside the web browser window. The web applications use WYSIWYG editors as a part of comment functionality, private messaging among users of applications, blogs, notes, forums post, spellcheck as-you-type, ticketing feature, and other online services. The XSS in WYSIWYG editors is considered more dangerous and exploitable because the user-supplied rich-text contents (may be dangerous) are viewable by other users of web applications. In this paper, we present a security analysis of twenty (20) popular WYSIWYG editors powering thousands of web sites. The analysis includes WYSIWYG editors like Enterprise TinyMCE, EditLive, Lithium, Jive, TinyMCE, PHP HTML Editor, markItUp! universal markup jQuery editor, FreeTextBox (popular ASP. NET editor), Froala Editor, elRTE, and CKEditor. At the same time, we also analyze rich-text editors available on very popular sites like Twitter, Yahoo Mail, Amazon, GitHub and Magento and many more. In order to analyze online WYSIWYG editors, this paper also present a systematic and WYSIWYG editors's specific XSS attack methodology. We apply the XSS attack methodology on online WYSIWYG editors and found XSS is all of them. We show XSS bypasses for old and modern browsers. We have responsibly reported our findings to the respective developers of editors and our suggestions have been added. In the end, we also point out some recommendations for the developers of web applications and WYSIWYG editors.
In current business practices, majority of organizations rely heavily on digital devices such as computers, generic media, cell phones, network systems, and the internet to operate and improve their business. Thus, a large amount of information is produced, accumulated, and distributed via electronic means. Consequently, government and company interests in cyberspace and private networks become vulnerable to cyberspace threats. The investigation of crimes involving the use of digital devices is classified under digital forensics which involves adoption of practical frameworks and methods to recover data for analysis which can serve as evidence in court. However, cybercrime has advanced to the stage where criminals try to cover their tracks through the use of anti-forensic strategies such as data overwriting and data hiding. Research into anti-forensics has given rise to the concept of 'live' forensics which comprises proactive forensics approaches capable of digitally investigating an incident as it occurs. However, information exchange using ICT facilities has reduced the world into a global village without eliminating the linguistic diversity on the planet. Moreover, existing digital forensics frameworks have assumed the language of stored information. If such assumption turns out to be wrong, semantic interpretation of extracted text would also be wrong leading to wrong conclusions. We propose incorporation of language identification (LID) in digital forensics investigation (DFI) models in order to help law enforcement to be a step ahead of criminals. In this chapter, we outline issues of language identification in DFI frameworks and propose a new framework with language identification component. The LID component is to carry out digital surveillance by scrutinizing emails, SMS, and text file transfers, in and out of the system of interest. The collected text is then subjected to language identification. Determining the language of the text would help to decide if the communication is regular and safe or suspicious and should be subjected to further forensic analysis. Finally we discuss results from a simple language identification scheme that can be easily and quickly integrated to a DFI model yielding very high accuracy without compromising speed performance.
This work addresses the challenge of discerning non-exact or non-obvious similarities between cybercrimes, proposing a new approach to finding linkages and repetitions across cases in a cyber-investigation context using near similarity calculation of distinctive digital traces. A prototype system was developed to test the proposed approach, and the system was evaluated using digital traces collected during actual cyber-investigations. The prototype system also links cases on the basis of exact similarity between technical characteristics. This work found that the introduction of near similarity helps to confirm already existing links, and exposes additional linkages between cases. Automatic detection of near similarities across cybercrimes gives digital investigators a better understanding of the criminal context and the actual phenomenon, and can reveal a series of related offenses. Using case data from 207 cyber-investigations, this study evaluated the effectiveness of computing similarity between cases by applying string similarity algorithms to email addresses. The Levenshtein algorithm was selected as the best algorithm to segregate similar email addresses from non-similar ones. This work can be extended to other digital traces common in cybercrimes such as URLs and domain names. In addition to finding linkages between related cybercrime at a technical level, similarities in patterns across cases provided insights at a behavioral level such as modus operandi (MO). This work also addresses the step that comes after the similarity computation, which is the linkage verification and the hypothesis formation. For forensic purposes, it is necessary to confirm that a near match with the similarity algorithm actually corresponds to a real relation between observed characteristics, and it is important to evaluate the likelihood that the disclosed similarity supports the hypothesis of the link between cases. This work recommends additional information, including certain technical, contextual and behavioral characteristics that could be collected routinely in cyber-investigations to support similarity computation and link evaluation. (c) 2018 The Author(s). Published by Elsevier Ltd on behalf of DFRWS.
The Center for Strategic and International Studies estimates the annual cost from cybercrime to be more than $400 billion. Most notable is the recent digital identity thefts that compromised millions of accounts. These attacks emphasize the security problems of using clonable static information. One possible solution is the use of a physical device known as a Physically Unclonable Function (PUF). PUFs can be used to create encryption keys, generate random numbers, or authenticate devices. While the concept shows promise, current PUF implementations are inherently problematic: inconsistent behavior, expensive, susceptible to modeling attacks, and permanent. Therefore, we propose a new solution by which an unclonable, dynamic digital identity is created between two communication endpoints such as mobile devices. This Physically Unclonable Digital ID (PUDID) is created by injecting a data scrambling PUF device at the data origin point that corresponds to a unique and matching descrambler/hardware authentication at the receiving end. This device is designed using macroscopic, intentional anomalies, making them inexpensive to produce. PUDID is resistant to cryptanalysis due to the separation of the challenge-response pair and a series of hash functions. PUDID is also unique in that by combining the PUF device identity with a dynamic human identity, we can create true two-factor authentication. We also propose an alternative solution that eliminates the need for a PUF mechanism altogether by combining tamper resistant capabilities with a series of hash functions. This tamper resistant device, referred to as a Quasi-PUDID (Q-PUDID), modifies input data, using a black-box mechanism, in an unpredictable way. By mimicking PUF attributes, Q-PUDID is able to avoid traditional PUF challenges thereby providing high-performing physical identity assurance with or without a low performing PUF mechanism. Three different application scenarios with mobile devices for PUDID and Q-PUDID have been analyzed to show their unique advantages over traditional PUFs and outline the potential for placement in a host of applications.
Cyber defense exercises (CDXs) represent an effective way to train cybersecurity experts. However, their development is lengthy and expensive. The reason lies in current practice where the CDX life cycle is not sufficiently mapped and formalized, and then exercises are developed ad-hoc. However, the CDX development shares many aspects with software development, especially with ERP systems. This paper presents a generic CDX development method that has been derived from existing CDX life cycles using the SPEM standard meta-model. The analysis of the method revealed bottlenecks in the CDX development process. Observations made from the analysis and discussed in the paper indicate that the organization of CDXs can be significantly optimized by applying a balanced mixed approach with agile preparation and plan-driven disciplined evaluation.
Software ecosystems are considered the natural evolution of software product lines. A software ecosystem provides a (software) product within a particular business and organizational context that supports the exchange of activities and services within a domain. However, the increasing degree of autonomy demanded by software ecosystems is elevating the system response to end users, while the existing software ecosystem architectures are not well prepared to deal with the dynamicity of context changes and autonomous behavior needs. In order to provide a transition toward an increased level of autonomy, in this article, we introduce the notion of autonomous dynamic ecosystems as representative of those software ecosystems able to support dynamic, smart, and autonomous features demanded by modern software systems. In this work, we further investigate and provide evidence of four industrial examples that have fully embodied the principles of autonomous dynamic ecosystems, and we characterize the main features and technology requirements of this kind of new ecosystems.
Purpose: Knowledge risks are increasingly becoming a great challenge to a variety of organizations. At the same time, academic research on such types of risks, their consequences, and potential ways of overcoming them is still scarce and fragmented. To fill this gap, the paper aims to find out do companies manage their knowledge risks, what are the possible knowledge risks they face and have they observed an increase of knowledge risks during the COVID-19 pandemic. The paper is aimed to present insights on different types of knowledge risks that organizations face, and the ways organizations handle them. The paper also proposes some potential countermeasures organizations might use to mitigate the consequences of knowledge risks. Methodology: The study presents the results of a quantitative survey performed among 60 professionals dealing with management and knowledge risks in organizations. In the study, the authors also have examined what tools and methods are used to manage these risks. The study also explores the level of readiness organizations have to address potential knowledge risks. Findings: The theoretical study has allowed us to identify a variety of knowledge risks, which can bear severe consequences for organizations, such as knowledge loss, knowledge leaking, knowledge hiding, or risks related to cybercrime. All these risks may potentially reduce the productivity in organizations, thus leading to the degradation of organizational performance. Research limitations: Research results are limited to the convenience sample that was selected for the study and thus may not give a comprehensive overview of the state of the art. Practical implications: The study provides useful insights for managers and owners of organizations in need of dealing with the knowledge risks in their organizations. The paper is enriched with a number of sample solutions that they may apply for the sake of their organization. Originality/value: The paper lays the ground for a better understanding of the knowledge risks that organizations need to face nowadays. As such, the paper offers food for thought for researchers dealing with the topic of knowledge risks, knowledge management, and organizational risk management in general.
Cybersecurity training is a key endeavour for ensuring that the IT workforce possess the knowledge and practical skills required to counter the ever-increasing cybersecurity threats that our society is faced with. While some related systems, such as Capture The Flag platforms, have been available for almost one decade, platforms that support full-fledged cybersecurity training exercises have only been released as open source in recent years. Given the complexity of such cybersecurity training platforms, the ques-tion that arises is how to meaningfully evaluate and compare their capabilities in order to identify the most suitable solution for a given type of organization and/or training activity. In this paper, we intro-duce a capability assessment methodology for cybersecurity training platforms that focuses on the three key aspects of training: content representation, environment management, and training facilitation. The assessment tool that we developed is used to evaluate two open-source cybersecurity training platforms, CyTrONE and ICYPO. We then conduct a comparative analysis of these two platforms based on our first-hand developer experience with them, and discuss the lessons learned from implementing, deploying and using these platforms. The assessment tool and the detailed technical comparative analysis that we conducted are intended as instruments and references for anyone who plans to deploy or develop cyber-security training platforms. (R) 2023 Elsevier Ltd. All rights reserved.
Hands-on training is an effective way to practice theoretical cybersecurity concepts and increase participants' skills. In this article, we discuss the application of visual analytics principles to the design, execution, and evaluation of training sessions. We propose a conceptual model employing visual analytics that supports the sensemaking activities of users involved in various phases of the training life cycle. The model emerged from our long-term experience in designing and organizing diverse hands-on cybersecurity training sessions. It provides a classification of visualizations and can be used as a framework for developing novel visualization tools supporting phases of the training life-cycle. We demonstrate the model application on examples covering two types of cybersecurity training programs.
Trust is a fundamental aspect in enabling selfadaptation of intelligent systems and in paving the way towards a smooth adoption of technological innovations in our societies. While Artificial Intelligence (AI) is capable to uplift the human contribution to our societies while protecting environmental resources, its ethical and technical trust dimensions bring significant challenges for a sustainable self-adaptive evolution in the domain of safety-critical systems. Inspired from the safety assurance case, in this paper we introduce the concept of trust assurance case together with the implementation of its ethical and technical principles directed towards assuring a trustworthy sustainable evolution of safety-critical AI-controlled systems.
The multi-criteria decision-making (MCDM) tool is a robust decision-making technique utilized in several fields like networking, risk management, digital analysis, cybercrime investigation, artificial intelligence, waste management enterprises and many other selection criteria. Complex SFS (CSFS) is a new edition of the spherical fuzzy set (SFS) that offers substantial information about any item in terms of amplitude and phase terms in a wider range of real terms. Complex SFS (CSFS) can be an extension of the spherical fuzzy set (SFS). The Aczel-Alsina aggregation tools are more appropriate aggregation operators (AOs), and they are used to conquer the impact of inconsistent and uncertain data. In this paper, we reveal some new approaches based on AczelAlsina aggregation tools under consideration of Complex Spherical Fuzzy (CSF) information. These new approaches include the CSF Aczel-Alsina weighted average (CSFAWA) operator, and the CSF Aczel-Alsina ordered weighted average (CSFOWA) operator. In addition to this, we also introduce a list of novel techniques by making use of the theory of Aczel-Alsina aggregation tools such as CSF Aczel-Alsina weighted geometric (CSFAWG) and CSF Aczel-Alsina ordered weighted geometric (CSFOWG) operators. To demonstrate the resilience and efficacy of the approaches that have been mentioned, we will examine a few exceptional examples and remarkable properties of the methodology that we have devised. In addition, a characterization is provided for an approach to the MCDM issue using the CPF information system. We use the example of electric automobiles as a case study to illustrate the uniformity and dependability of the methodology that we have established. This example was chosen because of the high cost of fuel and the present economic challenges that are being encountered by families in the middle class. An empirical case study is also constructed to determine an electric car that is desirable based on the techniques that we have proposed. To evaluate the correctness and superiority of the established strategies, we compare the outcomes of previously used techniques with the AOs currently being provided.
Cybercrime has become more pervasive and sophisticated over the years. Cyber ranges have emerged as a solution to keep pace with the rapid evolution of cybersecurity threats and attacks. Cyber ranges have evolved to virtual environments that allow various IT and network infrastructures to be simulated to conduct cybersecurity exercises in a secure, flexible, and scalable manner. With these training environments, organizations or individuals can increase their preparedness and proficiency in cybersecurity-related tasks while helping to maintain a high level of situational awareness. SPIDER is an innovative cyber range as a Service (CRaaS) platform for 5G networks that offer infrastructure emulation, training, and decision support for cybersecurity-related tasks. In this paper, we present the integration in SPIDER of defensive exercises based on the utilization of machine learning models as key components of attack detectors. Two recently appeared network attacks, cryptomining using botnets of compromised devices and vulnerability exploit of the DoH protocol (DNS over HTTP), are used as the support use cases for the proposed exercises in order to exemplify the way in which other attacks and the corresponding ML-based detectors can be integrated into SPIDER defensive exercises. The two attacks were emulated, respectively, to appear in the control and data planes of a 5G network. The exercises use realistic 5G network traffic generated in a new environment based on a fully virtualized 5G network. We provide an in-depth explanation of the integration and deployment of these exercises and a complete walkthrough of them and their results. The machine learning models that act as attack detectors are deployed using container technology and standard interfaces in a new component called Smart Traffic Analyzer (STA). We propose a solution to integrate STAs in a standardized way in SPIDER for the use of trainees in exercises. Finally, this work proposes the application of Generative Adversarial Networks (GANs) to obtain on-demand synthetic flow-based network traffic that can be seamlessly integrated into SPIDER exercises to be used instead of real traffic and attacks.
"Whereas the use of technology in organizations has many advantages, it also provides employees with new outlets to engage in technology-enabled forms of workplace deviance. This chapter examines various forms of cyber misbehavior through the lens of the four primary dimensions of workplace deviance put forth by Robinson and Bennett (1995). We also expand on Robinson and Bennett's (1995) classic model of workplace deviance by discussing features unique to cyberdeviance engagement. A review of both individual- and organization-directed forms of cyberdeviance is provided, including cyberbullying, online incivility, cyberloafing, and cybercrime. A brief discussion of the known antecedents and outcomes for each construct, as well as recommendations for applied practice and future research are provided. Whereas the introduction of new technology provides many opportunities for organizational growth and development, it also presents new means for employees to engage in various types of workplace misbehavior. Workplace cyberdeviance, which is employee misbehavior conducted using electronic means, is important for organizations to address, as it can result in a number of negative outcomes for both organizations and employees. Notably, because of differences in online and offline contexts, cyberdeviance may not simply be the manifestation of offline workplace deviance in an online realm. For instance, online interpersonal communication differs from offline communication in several key ways: (a) messages are text-based, (b) there can be multiple recipients, (c) communication can be easily forwarded to others, (d) there may be a greater risk for misinterpretation due to missing paralinguistic information, and (e) source anonymity may be possible (Kowalski, Limber, & Agatston, 2008; Kruger, Epley, Parker, & Ng, 2005; Privitera & Campbell, 2009). Likewise, non-communicative online behaviors can differ from non-communicative offline behaviors in that online misbehavior can occur at other geographic locations without the perpetrator being present, and some forms of online misbehavior require a high level of technical expertise. Thus, although workplace cyberdeviance shares many commonalities with offline workplace deviance, a special consideration of the unique features of cyberdeviance is warranted. In this chapter, we will first introduce the topic of workplace deviance, followed by a more detailed discussion of workplace cyberdeviance."
Zero-rating is a practice of electronic communication network operators whereby they offer certain apps and services that do not count toward a consumer's monthly data allowance. Over the past few years, the legality of this practice has become one of the most disputed when it comes to the greater regulatory regime. Even though significant case law has emerged, and regulators have made meaningful efforts, there have been unexpected developments in established approaches to such regulation. Consequently, there is uncertainty as to the proper regulatory regime for this practice. Average consumers might be quick to say that zero-rating is a great benefit, but there are other important factors that must be evaluated before giving the final word about the regulation of zero-rating practices. Zero-rating practices reflect the interdisciplinary nature of the Internet and the digital economy. Therefore, it must be analyzed from various perspectives. First, zero-rating can be examined within the net neutrality debate and from the competition law perspective. However, there are also implications in other areas like consumer protection and content diversity, and even human rights. This paper attempts to highlight major controversies surrounding the zero-rating debate in order to provide a regulatory proposal that could address this practice.
"Machine learning-based (ML) malware detectors have been shown to be susceptible to adversarial malware examples. Given the vulnerability of deep learning detectors to small changes on the input file, we propose a practical and certifiable defense against patch and append attacks on malware detection. Our defense is inspired by the concept of (de)randomized smoothing, a certifiable defense against patch attacks on image classifiers, which we adapt by: (1) presenting a novel chunk-based smoothing scheme that operates on subsequences of bytes within an executable; (2) deriving a certificate that measures the robustness against patch attacks and append attacks. Our approach works as follows: (i) during the training phase, a base classifier is trained to make classifications on a subset of contiguous bytes or chunk of bytes from an executable; (ii) at test time, an executable is divided into non-overlapping chunks of fixed size and our detection system classifies the original executable as the majority vote over the predicted classes of the chunks. Leveraging the fact that patch and append attacks can only influence a certain number of chunks, we derive meaningful large robustness certificates against both attacks. To demonstrate the suitability of our approach we have trained a classifier with our chunk-based scheme on the BODMAS dataset. We show that the proposed chunk-based smoothed classifier is more robust against the benign injection attack and state-of-the-art evasion attacks in comparison to a non-smoothed classifier."
Trust is a major aspect in the relationship between humans and autonomous safety-critical systems, such as autonomous vehicles. Although human errors may cause higher risks, failures of autonomous systems are more strongly perceived by the general population, which hinders the adoption of autonomous safety-critical systems. It is therefore necessary to devise approaches for systematically building trust in autonomous functions and thereby facilitate the adoption process. In this paper, we introduce a method and a framework for incrementally building trust in the context of autonomous driving. Within the envisioned solution, we employ the psychological narrative behind trust building through the formation of new habits and introduce a method where trust is established gradually for both the human and the autonomous safety-critical system via reputation building and step-by-step integration of smart software agents replacing human actions.
Achieving situational awareness is a challenging process in current HTTPS-dominant web traffic. In this paper, we propose a new approach to encrypted web traffic monitoring. First, we design a method for correlating host-based and network monitoring data based on their common features and a correlation time-window. Then we analyze the correlation results in detail to identify configurations of web servers and monitoring infrastructure that negatively affect the correlation. We describe these properties and possible data preprocessing techniques to minimize their impact on correlation performance. Furthermore, to test the correlation method's behavior in different web server setups and for recent encryption protocols, we modify it by adapting the correlation features to TLS 13 and QUIC. Finally, we evaluate the correlation method on a dataset collected from a campus network. The results show that while the correlation requires monitoring of custom event and flow features, it remains feasible even when using encryption protocols designed for the near future.
Storage resources are usually organized in abstraction layers in computing systems where higher level storage (e.g. files or file systems) is constructed from lower level storage (e.g. disk volumes). Many forensic storage reconstruction techniques exist that gather data at lower layers and interpret this data to reconstruct higher layers. On the one hand, there are metadata-based reconstruction techniques that interpret metadata structures to precisely reconstruct upper layer content. On the other hand, there are pattern-based techniques (carving) that focus mainly on deleted files that cannot be reconstructed by other methods. Instances resembling the former approach are Carrier's The Sleuth Kit (TSK) as well as many commercial tools, while the latter approach is used by file carvers like Foremost and Scalpel. Based on a formalization of storage abstraction layers, we show that all these techniques can be unified within a modular reconstruction framework. We define composition operators that allow to precisely express complex reconstruction tasks that involve both metadata-based and pattern-based techniques and allow to combine their respective strengths seamlessly in forensic analysis. We present LAYR, an implementation of our approach and show that it can automatically and reliably combine different reconstruction approaches. (C) 2020 The Author(s). Published by Elsevier Ltd on behalf of DFRWS. All rights reserved.
With the alarmingly increasing rate of cybercrimes worldwide, there is a dire need to combat cybercrimes timely and effectively. Cyberattacks on computing machines leave certain artifacts on target device storage that can reveal the identity and behavior of cyber-criminals if processed and analyzed intelligently. Forensic agencies and law enforcement departments use several digital forensic toolkits, both commercial and open-source, to examine digital evidence. The proposed research survey focuses on identifying the current state-of-the-art digital forensics concepts in existing research, sheds light on research gaps, presents a detailed introduction of different computer forensic domains and forensic toolkits used for computer forensics in the current era. The proposed survey also presents a comparative analysis based on the tool's characteristics to facilitate investigators in tool selection during the forensics process. Finally, the proposed survey identifies and derives current challenges and future research directions in computer forensics.
In our paper, we describe the landscape that has led to the realization from the nineties of the last century that cyber is a social good: Cyber is a social good, said Cybersecurity and Infrastructure Security Agency Director Jen Easterly. It's about societal resilience. And my last message (at CES 2023) is that we need to fundamentally change the relationship between government and industry. This realization is build upon the belief that trust can be reinvented on three levels: that of data chains in devices, information chains in the supply chain (can I trust my supplier, my client), and trust in the realness, the 'reality' level of the contexts evoked by these chains in an age of deep fakes, Chat GPT and the Metaverse. We argue that there is a crisis of trust on all levels, a crisis which inevitability is part of the digital turn itself. As we move, as Mark Weiser wrote in his seminal text The Computer for the 21st century, to a form of computing that will disappear into the fabric of everyday life, and will only succeed as a success when it disappears fully from the experience of humans. It is the infrastructure itself that acquires a new layer and becomes 'smart'. It has become an integral part of society that was before governed by rules of the kinetic realities of the world. These rules were built with certain threats in mind. The hybrid reality, layers of analogue/kinetic that interact sometimes, leads to new everyday practices that become social behavior. Leveling new threats then indeed becomes a social good. We argue that this is especially the case for small and medium-sized enterprises (SMEs), who by forming 99% of all business in Europe, not only pose a large fragmented threat vector, but also they are fighting cybercrime in isolation. We purpose a novel solution to exchange cybersecurity risk information with context among SMEs in a peer to peer mesh network. Additionally, a graph based risk analysis and prioritization method which takes into account the context information of assets and their environment.
Information theft or data exfiltration, whether personal or corporate, is now a lucrative mainstay of cybercrime activity. Recent security reports have suggested that while information, such as credit card data is still a prime target, other data such as corporate secrets, employee files and intellectual property are increasingly sought after on the black market. Malicious actors that are intent on exfiltrating valuable data, usually employ some form of Advanced Persistent Threat (APT) in order to exfiltrate large amounts of data over a long period of time with a high degree of covertness. Botnet's are prime examples of APTs that are usually established on targeted systems through malware or exploit kits that leverage system vulnerabilities. Once established, Botnet's rely on covert command and control (C&C) communications with a central server, this allows a malicious actor to keep track of compromised systems and to send out instructions for compromised systems to do their biding. Covert channels provide an ideal mechanism for data exfiltration and the exchange of command and control messages that are essential to a Botnet's effectiveness. Our work focuses on one particular form of covert channel that enables communication of hidden messages over normal Domain Name Server (DNS) network traffic. Covert channels based on DNS traffic are of particular interest, as DNS requests are an essential part of most Internet traffic and as a result are rarely filtered or blocked by firewalls. As part of our work we have created a test bed system that uses a covert DNS channel to exfiltrate data from a compromised host. Using this system we have carried out network traffic analysis that uses baseline comparisons as a means to fingerprint covert DNS activity. Even though detection of covert DNS activity is relatively straightforward, there is anecdotal evidence to suggest that most organizations do not filter or pay enough attention to DNS traffic and are therefore susceptible to data exfiltration attacks once a host on their network has been compromised. Our work shows that freely available covert DNS tools have particular traffic signatures that can be detected in order to mitigate data exfiltration and C&C traffic.
In this paper we present an attack, which allows fraudulent transactions to be collected from EMV contactless credit and debit cards without the knowledge of the cardholder. The attack exploits a previously unreported vulnerability in EMV protocol, which allows EMV contactless cards to approve unlimited value transactions without the cardholder's PIN when the transaction is carried out in a foreign currency. For example, we have found that Visa credit cards will approve foreign currency transactions for any amount up to (sic)999,999.99 without the cardholder's PIN, this side-steps the 20 pound contactless transaction limit in the UK. This paper outlines our analysis methodology that identified the flaw in the EMV protocol, and presents a scenario in which fraudulent transaction details are transmitted over the Internet to a rogue merchant who then uses the transaction data to take money from the victim's account. In reality, the criminals would choose a value between (sic)100 and (sic)200, which is low enough to be within the victim's balance and not to raise suspicion, but high enough to make each attack worthwhile. The attack is novel in that it could be operated on a large scale with multiple attackers collecting fraudulent transactions for a central rogue merchant which can be located anywhere in the world where EMV payments are accepted.
Recognizing the need for proactive analysis of cyber adversary behavior, this paper presents a new event-driven simulation model and implementation to reveal the efforts needed by attackers who have various entry points into a network. Unlike previous models which focus on the impact of attackers' actions on the defender's infrastructure, this work focuses on the attackers' strategies and actions. By operating on a request-response session level, our model provides an abstraction of how the network infrastructure reacts to access credentials the adversary might have obtained through a variety of strategies. We present the current capabilities of the simulator by showing three variants of Bronze Butler APT on a network with different user access levels.
"BackgroundIncreased digitalization of healthcare comes along with the cost of cybercrime proliferation. This results to patients' and healthcare providers' skepticism to adopt Health Information Technologies (HIT). In Europe, this shortcoming hampers efficient cross-border health data exchange, which requires a holistic, secure and interoperable framework. This study aimed to provide the foundations for designing a secure and interoperable toolkit for cross-border health data exchange within the European Union (EU), conducted in the scope of the KONFIDO project. Particularly, we present our user requirements engineering methodology and the obtained results, driving the technical design of the KONFIDO toolkit.MethodsOur methodology relied on four pillars: (a) a gap analysis study, reviewing a range of relevant projects/initiatives, technologies as well as cybersecurity strategies for HIT interoperability and cybersecurity; (b) the definition of user scenarios with major focus on cross-border health data exchange in the threepilot countries of the project; (c) a user requirements elicitation phase containing a threat analysis of the business processes entailed in the user scenarios, and (d) surveying and discussing with key stakeholders, aiming to validate the obtained outcomes and identify barriers and facilitators for HIT adoption linked with cybersecurity and interoperability.ResultsAccording to the gap analysis outcomes, full adherence with information security standards is currently not universally met. Sustainability plans shall be defined for adapting existing/evolving frameworks to the state-of-the-art. Overall, lack of integration in a holistic security approach was clearly identified. For each user scenario, we concluded with a comprehensive workflow, highlighting challenges and open issues for their application in our pilot sites. The threat analysis resulted in a set of 30 user goals in total, documented in detail. Finally, indicative barriers of HIT acceptance include lack of awareness regarding HIT risks and legislations, lack of a security-oriented culture and management commitment, as well as usability constraints, while important facilitators concern the adoption of standards and current efforts for a common EU legislation framework.ConclusionsOur study provides important insights to address secure and interoperable health data exchange, while our methodological framework constitutes a paradigm for investigating diverse cybersecurity-related risks in the health sector."
As one of the most pervasive current modes of communication, email needs to be fast and reliable. However, spammers and attackers use it as a primary channel to conduct illegal activities. Although many approaches have been developed and evaluated for spam detection, they do not provide sufficient accuracy. This deficiency results in significant economic losses for organizations. In this article, we first propose a framework for creating novel spam filters using Keras to combine a Convolutional Neural Network (CNN) with Long Short-Term Memory (LSTM) classification models. We then use this framework to introduce a specific solution applicable to realistic scenarios involving dynamic incoming email data in real-time. This solution takes the form of a real-time content-based spam classifier. We evaluate its performance concerning accuracy, precision, recall, false-positive, and false-negative rates. Our experimental results show that our approach can significantly outperform existing solutions for real-time spam detection.
Contemporary challenges for efficient similarity search include complex similarity functions, the curse of dimensionality, and large sizes of descriptive features of data objects. This article reports our experience with a database of protein chains which form (almost) metric space and demonstrate the following extreme properties. Evaluation of the pairwise similarity of protein chains can take even tens of minutes, and has a variance of six orders of magnitude. The minimisation of a number of similarity comparisons is thus crucial, so we propose a generic three stage search engine to solve it. We improve the median searching time 73 times in comparison with the search engine currently employed for the protein database in practice.
Data breaches remain a common occurrence affecting both companies and individuals alike, despite promulgated data protection legislation worldwide. It is unlikely that factors causing data breaches such as incorrect device configuration or negligence will stop unless effective enforcement of relevant legislation is applied. While several information privacy regulators exist, the dominant norm is to respond reactively on reported incidents. Reactive response is useful for cleaning up detected breaches but does not provide a clear indication of the level of personal information available on the internet since only reported incidents are taken into account. The possibility of pro-active automated breach detection has previously been discussed as a capability augmentation for existing privacy regulators. By pro-actively detecting leaked information, detection times can potentially be reduced to limit the exposure time of Personal Identifiable Information (PII) on publicly accessible networks. At present the average time for data breach detection is in excess of three months internationally and breach discovery it most often not by the data owner but an external third party increasing exposure of leaked information. The duration of time that data is exposed on the internet has severe negative implications since a significant portion of information disclosed in data breaches have been proven to be used for cybercrime activities. It could then be argued that any reduction of data breach exposure time should directly reduce the opportunity for associated cyber-crime. While pro-active breach detection has been proven as potentially viable in previous work, numerous aspects of such a system remain in question. Aspects such as legality, detection accuracy and communication with affected parties and alignment with privacy regulator operating procedures are all unexplored. The research presented in this paper considers the results obtained from two iterations of such an experimental system that was conducted on the South African. co.za domain. The first iteration conducted in early 2014 was used as a baseline for the second iteration that was conducted one year later in 2015. While the experiment was conducted on the South African cyber domain, the concepts are applicable to the international environment.
A robust and accurate forecast of the Quality of Service (QoS) attributes is essential for effective web service recommendation, enhanced user experience, and service management. Deep learning methods, especially Long Short-Term Memory Neural Networks (LSTM NN), have proven to be worthy for sequence forecasting in various domains recently. In this paper, we pilot an experimental application of LSTM NN in the domain of QoS forecasting. We develop a LSTM NN model for QoS prediction and compare its forecast performance with existing approaches for QoS attribute forecasting - ARIMA and Holt-Winters models. The approaches are compared on two real-world QoS attribute datasets created using centralized passive QoS attribute collection technique. Our results show that LSTM NN improves the accuracy of QoS forecast for attributes collected with high granularity while maintaining a reasonable computation time.
In 1967, the Supreme Court decided the landmark case of United States v. Katz, which engineered a paradigm shift in Fourth Amendment law: instead of focusing solely on property interests in determining whether or not a search' had occurred, the Court broadened the scope of the Amendment's protection to include any activity in which an individual has a reasonable expectation of privacy. However, subsequent decisions by lower courts-as well as recent decisions by the Supreme Court itself-have shown a continuing tension in the Katz legacy. This Article argues that an accurate application of the Katz test considers only the result of the search-the type of information that was acquired-and disregards altogether the method of the search-the action or conduct of the agent conducting the search. Although this is a broader view of the test than some courts have adopted, this view is supported by a close textual reading of Justice Harlan's concurring opinion in Katz (which set out the reasonable expectations test) and is also consistent with much of the reasoning in case law from the hundred years prior to Katz. Furthermore, application of a purely results-based test is all the more necessary given the advances in technology that have occurred since that landmark case was decided over thirty years ago. This is especially true in the fields of computer crime, sense-enhancing technology, and binary or content-discriminating searches. Part I of this Article analyzes the reasonable expectation of privacy test set out in the Katz case by briefly tracing the history behind the test and then examining how the Court has applied the test since Katz. Part 11 presents a results-based interpretation of the Katz test and places it in the context of the evolving case law on the subject. Part III examines three specific methods of investigation: computer-assisted investigations (whether of cybercrime or more traditional crimes), sense-enhancing technology, and binary or content-discriminating searches, and demonstrate the ways in which improper applications of the Katz standard have resulted in holdings that are inconsistent with Katz's original purpose-and incompatible with technology in today's world.
Purpose Within a larger mandate of reviewing the key global trends concerning consumer protection in the electronic commerce (e-commerce) literature, this study aims to study the legal framework concerning e-commerce and consumer protection in the Sultanate of Oman and to analyse the current regulations concerning e-commerce and consumer protection. Design/methodology/approach This study followed the normative legal research approach and resorted to the desk research process to facilitate content analysis of literature containing consumer protection legislation and regulatory provisions in Oman in particular and the rest of the world in general. Findings The study reveals that consumer protection initiatives in Oman are well entrenched for offline transactions, but are relatively new and limited for e-commerce. In spite of the promulgation of consumer protection laws, electronic transaction law and cybercrime law, consumer protection measures for e-commerce in Oman do not address a large number of the global concerns necessary to build consumer confidence and trust in the online environment. Research limitations/implications There is a dearth of information concerning Oman on this topic in the extant literature. The research also witnessed the lack of empirical data on the issue of consumer protection and e-commerce in Oman that offer a detailed database of consumer complaints and associated outcomes. Practical implications The mechanism of consumer protection in electronic transactions is not robust in many countries. Because of the lack of comprehensive and robust legislation, consumers remain vulnerable in the online contractual purchase process. Moving beyond the fragmented legislation, many countries are currently mulling an all-comprehensive e-commerce law, implications of this paper will help the policymakers in identifying the focus areas. Social implications Consumer protection is a burning global issue in this era of consumerism. It is important to build consumer trust, transparency and integrity of transactions to reduce the risk and uncertainties of purchase. Originality/value Consumer protection studies conducted in the context of Oman, hitherto, deal more with data protection and dispute resolution mechanisms, and less with legal provisions, regulations and consumer confidence. The study shares newer insights based on a systematic review of legal and business databases. It is the first study of its kind in the context of Oman and the Middle East in general.
Terrorist organisations have already stated to make use of the Internet. Possible activities include cyber-attacks, fundraising, training, recruitment, secret communication, data mining, propaganda and radicalisation. 2 The following article summarises the result of a research undertaken by the author for the Counter Terrorism Implementation Task Force (CTITF) and provides an overview about legal response to terrorist use of the Internet.
Through the enhanced connectivity of physical devices, the Internet of Things (IoT) brings improved efficiency to the lives of consumers when on-the-go and in the home. However, it also introduces new potential security threats and risks. These include threats that range from the direct hacking of devices that could undermine the security, privacy and safety of its users, to the enslaving of IoT devices to commit cybercrime at scale, such as Denial of Service attacks. The IoT is recognized as being widely insecure, in large part, due to the lack of security features built into devices. Additionally, consumers do not always actively use security features when available. More disconcerting is that we lack market surveillance on whether manufacturers ship products with good security features or how the importance of user-controlled security features is explained to IoT users. Our study seeks to address this gap. To do this, we compiled a database of 270 consumer IoT devices produced by 220 different manufacturers on sale at the time of the study. The user manuals and associated support pages for these devices were then analysed to provide a 'consumer eye' view of the security features they provide and the cyber hygiene advice that is communicated to users. The security features identified were then mapped to the UK Government's Secure by Design Code of Practice for IoT devices to examine the extent to which devices currently on the market appear to conform to it. Our findings suggest that manufacturers provide too little publicly available information about the security features of their devices, which makes market surveillance challenging and provides consumers with little information about the security of devices prior to their purchase. On average, there was discussion of around four security features, with account management and software updates being the most frequently mentioned. Advice to consumers on cyber hygiene was rarely provided. Finally, we found a lack of standardization in the communication of security-related information for IoT devices among our sample. We argue for government intervention in this space to provide assurances around device security, whether this is provided in a centralized or decentralized manner.
A world association acknowledges that dynamics of the development of the shadow sector becomes a global threat economic security, in this connection with what from the states acceptance of the concerted measures of fight is required against this publicly dangerous phenomenon both on national and on international levels. To deal with these sources, the appropriate mechanisms for regulating the level of development of the shadow economy sector from the state side need to be upgraded. Nevertheless, the development of shadow processes in the economy is due to the fact that their volumes actually do not lead to a clear, systematic control, which leads to the need to improve the priority areas of regulatory control in some kind of cybercrime. Higher education provides an opportunity to consider the investigated issue as an actuality of nautical education, which has important scientific and practical knowledge for introducing changes into the theory and practice of mechanisms of regulatory regulation, namely, the effectiveness of the functioning of state instruments in counteracting the development of shadowing in Ukraine. In the article, the identification of views of both domestic and foreign scientists concerning the content of the contents and the main components of the mechanisms of effective regulation in the field of counteraction to the shadow economy are conducted. It was carried out an analysis of the current level of the shadow sector in Ukraine and identified the basic factors of the domestic economy in the educational stage of the state's development. There are a number of factors that affected the scale of the shadow economy in Ukraine. There was a proposed scientific and methodological approach to the formation of an effective integration mechanism for counteracting the shadow economy in Ukraine. It was the necessity to unify and consolidate formally obligatory inclusion of the coordination and advisory bodies to prevent the shadow of the country members of the public, which in turn will contribute to the balancing of public power and public foundations in the context of establishing state mechanisms of combating the shadow economy. The revision of the conducted report makes it possible to apply the innovative recombination on the effective functioning of the mechanisms of the effective regulation of the opposition to the development of the shadow economy sector.
Purpose Drawing from the argument that mobile money services have a significant potential to provide a wide range of affordable, convenient and secure financial services, there have been rampant frauds on consumers of financial products over the digital financial platform. Thus, this study aims to establish the mediating effect of digital consumer protection in the relationship between mobile money adoption and usage and financial inclusion with data collected from micro small and medium enterprises (MSMEs) in northern Uganda. Design/methodology/approach To achieve the main objective of this study, a research model was developed to test for the mediating effect of digital consumer protection in the relationship between mobile money adoption and usage and financial inclusion. The data were collected from MSMEs and structural equation modelling in partial least square (PLS) combined with bootstrap was applied to analyze and test the hypotheses of this study. The direct and indirect effect of mobile money adoption and usage on financial inclusion was tested through digital consumer protection as a mediator variable. Findings The findings from the PLS-structural equation modelling (SEM) showed that mobile money adoption and usage has both direct and indirect effect on financial inclusion. Moreover, financial inclusion is influenced by both mobile money adoption and usage and digital consumer protection. Research limitations/implications The study used partial least square (PLS-SEM) combined with bootstrap confidence intervals through a formative approach to establish the mediating effect of the mediator variable. Hence, it ignored the use of covariance-based SEM and the MedGraph programme. Furthermore, data were collected from samples located in Gulu district, northern Uganda and specifically from MSMEs. This limits generalization of the study findings to other population who also use mobile money services. Practical implications Promoters of digital financial services, managers of telecommunication companies, and financial inclusion advocates should consider strengthening the existing digital consumer protection laws on the mobile money platform. A collaborative approach between the mobile network operators, financial institutions and regulators should tighten the existing laws against mobile money fraudsters and an efficient mechanism for recourse, compensation and remedy should be set up to benefit the victims of frauds and cybercrime on the Fintech ecosystem. Originality/value The current study gives a useful insight into the critical mediating role of digital consumer protection as a cushion for promoting financial inclusion through mobile phones over the Fintech that face great threat and risk from cyber insecurity.
"Image forensic analysis becomes a major role in the field of digital image security due to tampering and forgery. The image forgery violates the authenticity and ownership of digital images. Copy-move forgery considers a significant kind of image forensic analysis algorithm. In this kind, the forger copies a part of an original image and then pastes it into the selected position from the same image. The purpose of forgery is to hide or highlight a specific region of the original image. To detect copy-move forgery, there are two traditional techniques: block-based and keypoint-based. The main drawback of the keypoint-based technique is the insufficient features for the small and flat regions, which causes undetected forgery. In contrast, the block-based technique has intensive processing. Therefore, this paper proposes a robust scheme that overcomes the drawbacks of the above techniques and maintains their advantages. This scheme adopts three connected stages, the first detects the initial duplicated regions using the SURF-HOG detector and descriptor. Subsequently, the second stage localizes the primary matched regions by SLIC segmentation and then selects the suspicious neighbor regions to be combined with primary regions to obtain the active regions. In the third stage, the block-based technique adopts overlapping Zernike moments to extract sufficient key points from the produced active regions. In the final stage, the duplicated regions are classified into authentic or forged regions. The proposed scheme provides not only forgery detection but also localization and recognition for the duplicated regions. The experimental results show that the proposed scheme is fast and has high accuracy for forgery detection and localization, at least 93.75, and 7.25 in terms of True Positive and False Positive Rates. Moreover, the scheme has high robustness under various conditions and attacks such as geometric transformation attacks and compound photometric attacks. The proposed scheme can be used in sensitive applications such as cybercrime detection and adopted as evidence in the courts.& COPY; 2022 The Authors. Published by Elsevier B.V. on behalf of King Saud University. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/)."
This article is about the manifestations of similarities between two seemingly distinct groups of Nigerians: cybercriminals and politicians. Which linguistic strategies do Twitter users use to express their opinions on cybercriminals and politicians? The study undertakes a qualitative analysis of 'engaged' tweets of an elite law enforcement agency in West Africa. We analyzed and coded over 100,000 'engaged' tweets based on a component of mechanisms of moral disengagement (i.e., advantageous comparison), a linguistic device. The results reveal how respondents defend the actions of online fraudsters (the deviant group) by strategically comparing them to the wrongful acts of corrupt politicians (the respectable group). Similarly, the results show how respondents positioned this linguistic strategy to compare the powerless group (online fraudsters) and the powerful group (politicians) in society. Indeed, tweet responses suggest that the Economic and Financial Crimes Commission (EFCC) generally looks downwards for culprits (i.e., online fraudsters) while ignoring fraudulent politicians. We conclude that the process by which some actions are interpreted as a crime compared to others is a moral enterprise.
We consider the nearest-neighbor search on large-scale high-dimensional datasets that cannot fit in the main memory. Sketches are bit strings that compactly express data points. Although it is usually thought that wide sketches are needed for high-precision searches, we use relatively narrow sketches such as 22-bit or 24-bit, to select a small set of candidates for the search. We use an asymmetric distance between data points and sketches as the criteria for candidate selection, instead of traditionally used Hamming distance. It can be considered a distance partially restoring quantization error. We utilize an efficient one-by-one sketch enumeration in the order of the partially restored distance to realize a fast candidate selection. We use two datasets to demonstrate the effectiveness of the method: YFCC100M-HNfc6 consisting of about 100 million 4,096 dimensional image descriptors and DEEP1B consisting of 1 billion 96 dimensional vectors. Using a standard desktop computer, we conducted a nearest-neighbor search for a query on datasets stored on SSD, where vectors are represented by 8-bit integers. The proposed method executes the search in 5.8 seconds for the 400GB dataset YFCC100M, and 0.24 seconds for the 100GB dataset DEEP1B, while keeping the recall of 90%.
Asset identification plays a vital role in situational awareness building. However, the current trends in communication encryption and the emerging new protocols turn the well-known methods into a decline as they lose the necessary data to work correctly. In this paper, we examine the traffic patterns of the TLS protocol and its changes introduced in version 1.3. We train a machine learning model on TLS handshake parameters to identify the operating system of the client device and compare its results to well-known identification methods. We test the proposed method in a large wireless network. Our results show that precise operating system identification can be achieved in encrypted traffic of mobile devices and notebooks connected to the wireless network.
The development of modem computer technology and changes in federal legislation introduced in recent years, have caused the authors to identify the main trends in the development of criminal liability for crimes against the security of computer information: four areas that encourage the need to increase criminal liability for acts committed with computer technology. The paper also gives a historical overview devoted to the development of computer technologies and a general description of crimes in the field of computer information, taking into account changes introduced into criminal legislation by Federal Law No. 194-FZ dated July 26, 2017.
Network community detection often relies on optimizing partition quality functions, like modularity. This optimization appears to be a complex problem traditionally relying on discrete heuristics. And although the problem could be reformulated as continuous optimization, direct application of the standard optimization methods has limited efficiency in overcoming the numerous local extrema. However, the rise of deep learning and its applications to graphs offers new opportunities. And while graph neural networks have been used for supervised and unsupervised learning on networks, their application to modularity optimization has not been explored yet. This paper proposes a new variant of the recurrent graph neural network algorithm for unsupervised network community detection through modularity optimization. The new algorithm's performance is compared against the state-of-the-art methods. The approach also serves as a proof-of-concept for the broader application of recurrent graph neural networks to unsupervised network optimization.
"Online Identity Theft (ID theft) is a significant problem in our modern knowledge-based and social-driven computing era. This type of cybercrime can be achieved in a number of different ways; and more of the point, various statistical figures suggest it is on the increase. The target is individual privacy and self-assurance, while efforts and measures for increased security and protection appear inadequate to prevent it. While personal identities are increasingly being stored and shared on digital media in virtualised environments, the threat of personal and private information that is used fraudulently cannot be eliminated. This trend in crime can result in complex investigations that involve virtualised information technologies, both as a medium for analysis and as evidence at the same time. Fraudsters are obtaining more sophisticated technological ways and increase their capability not only for committing but also for concealing their crimes. It is believed that fraudsters of this kind of crime are not acting individually, but rather they operate in an organised and well-structured manner. Indeed ID theft is nowadays directly linked to drug trafficking, money laundering and terrorism. ID theft, like almost all different types of crime, involves two parts, at least one victim and at least one fraudster. We argue that the differentiation of the investigation procedure between the victim's and the fraudster's side, depends on the ownership and control of the digital media involved in the crime, and can provide results on a more crime-focused basis. In addition it provides information gathering, understanding and knowledge about the way the fraudster acts and could potentially assist in future investigations. Different pieces of evidence can be discovered on each side (victim-fraudster) concerning the techniques that have been used to perpetrate the crime. The online ID theft techniques can leave evidence on both the victim's and the fraudster's system. However, the evidence tends to contain different elements on each side that can reveal information about the fraudster and eventually profile him in relation to the attack. There is an approach of profiling the ID theft fraudster based on the findings thatarise during the forensic investigation process in this paper. We discuss the extent of ID theft as a problem and the role of the fraudster in different ID theft techniques. We aim to demonstrate processes that could assist the profiling of the fraudster under the forensic investigation of ID theft."
In the context of Smart Ecosystems, systems engage in dynamic cooperation with other systems to achieve their goals. Expedient operation is only possible when all systems cooperate as expected. This requires a level of trust between the components of the ecosystem. New systems that join the ecosystem therefore first need to build up a level of trust. Humans derive trust from behavioral reputation in key situations. In Smart Ecosystems (SES), the reputation of a system or system component can also be based on observation of its behavior. In this paper, we introduce a method and a test platform that support virtual evaluation of decisions at runtime, thereby supporting trust building within SES. The key idea behind the platform is that it employs and evaluates Digital Twins, which are executable models of system components, to learn about component behavior in observed situations. The trust in the Digital Twin then builds up over time based on the behavioral compliance of the real system component with its Digital Twin. In this paper, we use the context of automotive ecosystems and examine the concepts for building up reputation on control algorithms of smart agents dynamically downloaded at runtime to individual autonomous vehicles within the ecosystem.
"Thirty three of the 55 African countries have enacted data privacy laws since 2001. This paper analyses two types of potential legal influences on these national laws: standards and obligations originating from outside Africa (international); and those developed within Africa, both at the continental (African Union) level, and at the level of Regional Economic Communities (RECs).& nbsp;Analysis commences with which countries have laws or Bills, and constitutional require-ments International influences are shown to be indirect, with only slight UN influences, but with accessions by African countries to data protection Convention 108 significant in con-verting it into a global Convention. EU influences are also indirect (aspirational) with both the DPD and the GDPR perceived as standards to be emulated in African instruments. Of African multilateral agreements, the African Union's data protection and cybercrime Con-vention (2014) is yet of limited influence because it is not in force. In contrast, the ECOWAS Supplementary Act (2010) has been of considerable practical influence in West Africa. The HIPPSA `Model Acts' with higher standards (particularly SADC Model Law, 2013) remain of potential influence in other regions of sub-Saharan Africa.& nbsp;European instruments are presented as typifying `three generations' of development of data privacy laws (exemplified by (i) Convention 108, 1980; (ii) EU DPD, 1995; and (iii) EU GDPR, 2016 plus Convention 108 + , 2018). The three main African multilateral instruments (ECOWAS Act, AU Convention, SADC Model Law) are compared with them, as well as be-tween themselves.& nbsp;These comparisons enable conclusions to be drawn, some of which are as follows.& nbsp;The African regional framework does not display any Africa-specific approach to data protection. Less individualist and more communitarian African culture or human rights discourse is absent from the texts of these laws.& nbsp;Drafters of the African instruments seem to accept, tacitly or expressly the necessity to be consistent with other international texts, in particular European instruments. They have adopted many `2nd generation' DPD elements but have anticipated relatively few of the `third generation' GDPR standards.& nbsp;Sub-regional agreements (ECOWAS Act and model laws) have provided a means to move forward despite the lack of progress in bring the regional AU convention into force. (C)& nbsp;2021 Graham Greenleaf and Bertil Cottier. Published by Elsevier Ltd. All rights reserved."
Purpose This study aims to identify weaknesses in current internal control systems in protecting customer data and the drivers that motivate employees to steal customer data and the impact of customer data theft on the organization. Design/methodology/approach A case study approach was taken to investigate and analyze internal control system weaknesses. One organization that involved investor and treasury services was selected as a case study in this research. A mixed method of data collection, specifically survey questionnaires and observations, was used. Findings This study revealed that employees are aware of the policy to protect customer data in their organization. Ironically, customer data theft still occurred despite the company having an internal control system. The main concern was the attitude of the employees to adhere to the policies in place, which becomes the major cause of internal control violation. Employees tend to ignore policies and standard operating procedures, providing opportunities for data theft and fraud to occur, although they realize this will result in a severe impact on the reputation of a company. Research limitations/implications - The results provide further confirmation of the fraud triangle theory, i. e. opportunity on the possible causes of the data theft and fraud, supporting prior empirical research and surveys conducted by researchers and global professional firms on fraud. This study, however, was conducted on only one organization with limited participation from employees because of the sensitivity of the nature of the topic. Practical implications - This study provided recommendations that can be a reference for companies and regulatory bodies in preventing customer data theft cases, such as regular training and awareness campaigns to the staff, stringent recruitment policies, close monitoring on the accessibility of customer data and continuous use of advanced technology to prevent a data breach. Originality/value - This study is original, as it focuses on an organization that operates in the financial services industry, which is one of the most attacked sectors for data theft and cybercrime activity globally. Furthermore, this kind of research is rare in fraud literature, particularly in developing markets such as Malaysia. The findings of this study are inferred from the direct observation of the organizational and employee work environments, activities and behaviors, which are private and confidential and difficult to access by researchers for publication in academic journals.
Recent studies have shown that phishers are using phishing kits to deploy phishing attacks faster, easier and more massive. Detecting phishing kits in deployed websites might help to detect phishing campaigns earlier. To the best of our knowledge, there are no datasets providing a set of phishing kits that are used in websites that were attacked by phishing. In this work, we propose PhiKitA, a novel dataset that contains phishing kits and also phishing websites generated using these kits. We have applied MD5 hashes, fingerprints, and graph representation DOM algorithms to obtain baseline results in PhiKitA in three experiments: familiarity analysis of phishing kit samples, phishing website detection and identifying the source of a phishing website. In the familiarity analysis, we find evidence of different types of phishing kits and a small phishing campaign. In the binary classification problem for phishing detection, the graph representation algorithm achieved an accuracy of 92.50%, showing that the phishing kit data contain useful information to classify phishing. Finally, the MD5 hash representation obtained a 39.54% F1 score, which means that this algorithm does not extract enough information to distinguish phishing websites and their phishing kit sources properly.
Internet of Things (IoT) devices are constantly growing in numbers, forecasted to reach 27 billion in 2025. With such a large number of connected devices, energy consumption concerns are a major priority for the upcoming years. Cloud / Edge / Fog Computing are critically associated with IoT devices as enablers for data communication and coordination among devices. In this paper, we look at the distribution of Semantic Reasoning between IoT devices and define a new class of reasoning, multi-step reasoning, that can be associated at the level of the edge or fog node in the context of IoT devices. We conduct an experiment based on synthetic datasets to evaluate the performance of multi-step reasoning in terms of power consumption, memory, and CPU usage. Overall we found that multi-step reasoning can help in reducing computation time and energy consumption on IoT devices in the presence of larger datasets.
With the rapid advancements in digital technologies and the exponential growth of digital artifacts, automated filtering of cybercrime data for digital investigation from a variety of resources has become the need of the hour. Many techniques primarily based on the Approximate Matching  approach have been proposed in the literature to address this challenging task. In the year 2019, Chang et al. proposed one such algorithm -FbHash: A New Similarity Hashing Scheme for Digital Forensics that was shown to produce the best correlation results compared to other existing techniques and also resist active adversary attack, unlike others. However, no performance analysis of the tool was given. In this work, we show that the current design structure of FbHash is slower and memory intensive compared to its peers. We then propose a novel Bloom filter based efficient version, i.e., FbHash-E that has a much lower memory footprint and is computationally faster compared to FbHash. While the speed of FbHash-E is comparable to other state-of-the-art tools, it is resistant (like its predecessor) to inten-tional/intelligent modifications that can fool the tool  attacks, unlike its peers. Our version thus renders FbHash-E fit for practical use-cases. We perform various modification tests to evaluate the security and correctness of FbHash-E. Our experiment results show that our scheme is secure against active attacks and detects similarity with 87% accuracy. Compared to FbHash, there is only 3% drop in accuracy results. We demonstrate the sensitivity and robustness of our proposed scheme by performing a variety of containment and resemblance tests. We show that FbHash-E can correlate files with up to 10% random-noise with 100% detection rate and is able to detect commonality as small as 1% between the two documents with an appropriate similarity score. We also show that our proposed scheme performs best to identify similarities between different versions of software or program files. We also introduce a new test, i.e., consistency test, and exhibit that our tool produces consistent results across all files under a fixed category with very low standard deviation, unlike other tools where standard deviation under a fixed test varies significantly. This shows that our tool is more robust and stable against different modifications. (C) 2022 Elsevier Ltd. All rights reserved.
Cybersecurity and digital forensics are closely related to current and even more to future criminal proceedings due to the fact that digital evidence is more and more dominating the body of evidence in criminal trials. It is also fair to say that digital forensics to a large extent performs in cybersecurity as a production of digital evidence. Not only in cybercrime but also in ordinary crimes like car theft, drug related crimes and forms of traditional organized crimes judicial decision-making and reasoning at trials and finally in judgments are more and more based on references to digital traces and data which are provided by forensic IT experts as exhibits. At the first glance providing digital data as evidence to criminal courts appears to be a straightforward more technical process. This paper claims that turning digital traces into digital evidence is anything but a simple and linear technical process just having to respect state-of-the-art technical standards and following rules for the chain of custody. Instead it will demonstrate that digital forensics is one part in a more complex social construction process where standards and methods of IT forensics of the 21st century meet evidentiary procedural rules in criminal justice of the 19th century, hence applied by a judiciary of the 20th century. This state of asynchrony leads to basic conflicts between digital forensics on the one side, rule of law, to process and fair trial on the other. The paper will reflect upon the central question if and how this conflict between digital forensics and traditional due process in the transition from analog to digital evidence in criminal justice proceedings can be turned into an integrated and complementary approach for the final sake of justice and security in the society. While the current situation is leading to a gap between data and judicial decision (digital divide), new standards for producing procedural truth in a comprehensible data-to-decision-cycle covering both the forensic acquisition, preparation and analysis at the pretrial stage, as well as presenting, testing (verifying/falsifying) and interpreting digital evidence at trial by judges, prosecutors and defense attorneys. Only if both sides, the forensic and judicial, are in balance the digital divide in today's criminal justice practice will fade away.
"E-banking, also known as electronic or Internet banking, has become a prevalent mode for online and internet-based transactions. However, modern databases, online information and knowledge sharing, and increased access points for e-banking transactions opened opportunities for sophisticated fraudsters to perpetrate and abuse customers in their social, cyber and physical worlds. The intensification in cases of fraud presents tremendous challenges for the banking sector to caution, educate and inform customers on cybercrime because it reflects the synthetic and integrative use of the interaction between resources which include the fraudster's intelligence abuse in the social world, the abuse of knowledge resources and technology in the cyber world and the abuse of resources and trading tools in the physical world. Most studies focus on the detection of fraud patterns through tapping in data warehouses of third-party or by using data mining programs to identify fraud patterns, specifically credit cards, computer intrusion and mobile communication or first-party fraud when a legitimate customer knowingly betrays the bank. This study focuses on the prevention of fraud that falls into two main categories, namely: phishing/vishing/SMishing and malware practices (any activity of payment fraud where fraudsters gain access and uses customer' accounts for their own unlawful financial benefit); and identity theft (gaining access to or opening new accounts in the customer's name). The main premises of the research problem are built on the fact that a lack of studies exists to investigate the use of proactive communication through the three typologies of knowledge management to create awareness and educate customers on e-security measures and prevention of e-banking fraud where the move towards (co)liability should not impede but enhance customer relationship building. The research is conducted through a qualitative research methodology and the subject under study was the website of the South African Banking Risk Information Centre (SABRIC) purposefully sampled. The three concurrent cyclical flow of activity of the data analysis interactive model was used in the research. An abductive approach was used to report on the findings based on descriptions and interpretive comments relating it to and drawing on the theoretical thrusts identified. Main findings suggest, inter alia, the importance to proactively educate customers on how to protect themselves before they fall, victim, the importance of visibility on security measures, methods and standards for e-banking, and the move towards a (co)liability policy and shared responsibility process."
Purpose The purpose of this paper is to show how global regulation of cryptocurrencies and other cybercurrencies can assist in addressing the challenges of attribution when investigating ransomware attacks and other types of cybercrime using these payment methods. Design/methodology/approach A literature review, looking at current academic research and discourse on the topic cryptocurrency regulation, is conducted to highlight current thinking and perceived difficulties in implanting a global regulatory framework. In addition, the research explores how governments have addressed the risks posed by cryptocurrencies and how regulation has been implemented. The research focuses on the regulatory approaches of Australia, Europe and the Americas to determine whether they could feasibly address the risks posed by cryptocurrencies and be implemented on a global scale. Findings To date, few sustained efforts have been made to regulate Bitcoin or other cybercurrencies. Where regulation has been introduced, it has often proven too costly to implement, thereby, stifling Bitcoin industry growth, or too ad hoc to function effectively. These regulatory pitfalls are substantiated by the continuing difficulty faced by law enforcement agencies, in identifying individual Bitcoin users and separating those that are using them for nefarious purposes from those that are using them for legitimate ones. These challenges appear to grow exponentially when it comes to prosecuting criminals for Bitcoin-related offences, due to the enormous lack of agreement within the justice system of most countries as to the appropriate legal definition for Bitcoin. This research highlights three characteristics that will be vital to the success of any global regulatory framework. These are consistency, clarity and cost-effective implementation. A regulatory framework for Bitcoin that lacks any one of these elements will fail to meet the requirements of every stakeholder in the regulatory process. A framework that is too costly to implement will stifle fintech innovation, subsequently depriving national economies of the multitude of potential benefits promised by fostering fintech entrepreneurship. Equally, a framework that is inconsistent will hamper the global cooperation necessary to combat Bitcoin-related crime. Originality/value This research evaluates research, discourse and regulatory responses from academic and governmental sources and discusses how a global response to cryptocurrency regulation will help address the growing problem of attribution when it comes to ransomware attacks, which has experienced a considerable spike in recent months.
For every finitary monad T on sets and every endofunctor F on the category of T-algebras, we introduce the concept of an ffg-Elgot algebra for F, i.e. an algebra admitting coherent solutions for finite systems of recursive equations with effects represented by the monad T. The goal is to study the existence and construction of free ffg-Elgot algebras. To this end, we investigate the locally ffg fixed point phi(F), i.e. the colimit of all F-coalgebras with free finitely generated carrier, which is shown to be the initial ffg-Elgot algebra. This is the technical foundation for our main result: the category of ffg-Elgot algebras is monadic over the category of T-algebras.
"This work is a companion reproducible paper of a previous paper (Antol et al., 2021) in which we presented an alternative to the traditional paradigm of similarity searching in metric spaces called the Learned Metric Index. Inspired by the advance in learned indexing of structured data, we used machine learning models to replace index pivots, thus posing similarity search as a classification problem. This implementation proved to be more than competitive with the conventional methods in terms of speed and recall, proving the concept as viable. The aim of this publication is to make our source code, datasets, and experiments publicly available. For this purpose, we create a collection of Python3 software libraries, YAML reproducible experiment files, and JSON ground-truth files, all bundled in a Docker image - the Learned Metric Index Framework (LMIF) - which can be run using any Docker-compatible operating system on a CPU with Advanced vector extensions (AVX). We introduce a reproducibility protocol for our experiments using LMIF and provide a closer look at the experimental process. We introduce new experimental results by running the reproducibility protocol introduced herein and discussing the differences with the results reported in our primary work (Antol et al., 2021). Finally, we make an argument that these results can be considered weakly reproducible (in both of the performance metrics), since they point to the same conclusions derived in the primary paper.& COPY; 2023 Elsevier Ltd. All rights reserved."
This paper discusses the increasing significance of smart water management, within the context of the fourth industrial revolution and the associated cybersecurity risks, particularly in Hungary and Central Europe. By examining the current state of smart water management and analyzing the various cybersecurity threats, this study seeks to raise awareness around the need for enhanced security measures, in this critical sector. The research methodology is primarily based on a through literature review and secondarily, on related data analysis. The paper identifies several cybersecurity challenges and potential solutions for smart water management and finally suggests future research directions, to ensure the safe and sustainable development of this critical infrastructure.
Formation of digital ecosystems enables a multitude of actors like organisations, users and developers to achieve different goals in cooperations. The achievement of operational, tactical and strategic goals of involved actors relies on trustworthy cooperation of systems that operate in open environments and might meet for the first time at runtime. Thus the evaluation of a potential collaborator's trustworthiness also needs to be performed at runtime. In this paper, we enhance our work on trust prediction in digital ecosystems, based on digital twins evaluation, with a supporting reference architecture that enables the creation of such a digital twin for automatic computation of trust.
Emerging evidence shows that safety-critical systems are evolving towards operating in uncertain context while integrating intelligent software that evolves over time as well. Such behavior is considered to be unknown at every moment in time because when faced with a similar situation, these systems are expected to display an improved behavior based on artificial learning. Yet, a correct learning and knowledge-building process for the non-deterministic nature of an intelligent evolution is still not guaranteed and consequently safety of these systems cannot be assured. In this context, the approach of predictive simulation enables runtime predictive evaluation of a system behavior and provision of quantified evidence of trust that enables a system to react safety in case malicious deviations, in a timely manner. For enabling the evaluation of timing behavior in a predictive simulation setting, in this paper we introduce a general timing model that enables the virtual execution of a system's timing behavior. The predictive evaluation of the timing behavior can be used to evaluate a system's synchronization capabilities and in case of delays, trigger a safe fail-over behavior. We iterate our concept over an use case from the automotive domain by considering two safety critical situations.
Cross-border data flows not only involve cross- border trade issues, but also severely challenge personal information protection, national data security, and the jurisdiction of justice and enforcement. As the current digital trade negotiations could not accommodate these challenges, China has initiated the concept of secure cross-border data flow and has launched a dual-track multi-level regulatory system, including control system for overseas transfer of important data, system of crossborder provision of personal information, and system of cross-border data request for justice and enforcement. To explore a global regulatory framework for cross-border data flows, legitimate and controllable cross-border data flows should be promoted, supervision should be categorized based on risk concerned, and the rule of law should be coordinated at home and abroad to promote system compatibility. To this end, the key is to build a compatible regulatory framework, which includes clarifying the scope of important data to define the Negative List for preventing national security risks, improving the cross-border accountability for protecting personal information rights and interests to ease pre-supervision pressure, and focusing on data access rights instead of data localization for upholding the jurisdiction of justice and enforcement.
Camera model identification is a standard task in digital image forensics. Learning-based approaches achieve state-of-the-art performance, but they are sensitive to so-called out-of-distribution (OOD) data due to a mismatch between the training and testing distribution. This may result in a significant reduction in classifier performance that is, unfortunately, not easy to anticipate for a forensic analyst. In this work, we investigate possibilities for adding reliability measures to the task of camera model identification. We leverage learning architectures that include an uncertainty measure with every prediction that can be reported back to an analyst. To this end, we investigate deep ensembles and Bayesian neural networks (BNNs). We compare both methods against a standard CNN with softmax statistics as uncertainty metric. We demonstrate in several experiments that both probabilistic approaches provide simultaneously state-of-the-art classification performance and reliable uncertainty estimates on OOD data. The uncertainty of deep ensembles is more accurate on OOD camera models, while BNN uncertainties are more accurate on OOD post-processing.
"This paper presents DIALOG ( Digital Investigation Ontology); a framework for the management, reuse, and analysis of Digital Investigation knowledge. DIALOG provides a general, application independent vocabulary that can be used to describe an investigation at different levels of detail. DIALOG is defined to encapsulate all concepts of the digital forensics field and the relationships between them. In particular, we concentrate on the Windows Registry, where registry keys are modeled in terms of both their structure and function. Registry analysis software tools are modeled in a similar manner and we illustrate how the interpretation of their results can be done using the reasoning capabilities of ontology. (C) 2009 Digital Forensic Research workshop. Published by Elsevier Ltd. All rights reserved."
"Importance Anecdotal evidence suggests that health care delivery organizations face a growing threat from ransomware attacks that are designed to disrupt care delivery and may consequently threaten patient outcomes. Objective To quantify the frequency and characteristics of ransomware attacks on health care delivery organizations. Design, Setting, and Participants This cohort study used data from the Tracking Healthcare Ransomware Events and Traits database to examine the number and characteristics of ransomware attacks on health care delivery organizations from 2016 to 2021. Logistic and negative binomial regression quantified changes over time in the characteristics of ransomware attacks that affected health care delivery organizations. Main Outcomes and Measures Date of ransomware attack, public reporting of ransomware attacks, personal health information (PHI) exposure, status of encrypted/stolen data following the attack, type of health care delivery organization affected, and operational disruption during the ransomware attack. Results From January 2016 to December 2021, 374 ransomware attacks on US health care delivery organizations exposed the PHI of nearly 42 million patients. From 2016 to 2021, the annual number of ransomware attacks more than doubled from 43 to 91. Almost half (166 [44.4%]) of ransomware attacks disrupted the delivery of health care, with common disruptions including electronic system downtime (156 [41.7%]), cancellations of scheduled care (38 [10.2%]), and ambulance diversion (16 [4.3%]). From 2016 to 2021, ransomware attacks on health care delivery organizations increasingly affected large organizations with multiple facilities (annual marginal effect [ME], 0.08; 95% CI, 0.05-0.10; P<.001), exposed the PHI of more patients (ME, 66385.8; 95% CI, 3400.5-129371.2; P=.04), were less likely to be restored from data backups (ME, -0.04; 95% CI, -0.06 to -0.01; P=.002), were more likely to exceed mandatory reporting timelines (ME, 0.06; 95% CI, 0.03-0.08; P<.001), and increasingly were associated with delays or cancellations of scheduled care (ME, 0.02; 95% CI, 0-0.05; P=.02). Conclusions and Relevance This cohort study of ransomware attacks documented growth in their frequency and sophistication. Ransomware attacks disrupt care delivery and jeopardize information integrity. Current monitoring/reporting efforts provide limited information and could be expanded to potentially yield a more complete view of how this growing form of cybercrime affects the delivery of health care. This cohort study examines the frequency and characteristics of ransomware attacks on health care delivery organizations."
Online child sexual abuse is a relatively new form of crime against children which requires a more concerted action on a global scale. Intricate organizational, technical, legal and socio-psychological issues surrounding this phenomenon make the current structure and possible solutions to organizational problems extremely complicated to analyse, evaluate and discuss. This commentary focuses on the organizational aspect by dividing the complex global response into four simple layers: governments and inter-governmental agencies, police and judicial bodies, non-governmental organizations and the private sector. Undoubtedly, developing more effective global policies for the fight against online child sexual abuse relies on deeper understanding of the current structure and the roles of each layer.
Timed automata (TA) have been widely adopted as a suitable formalism to model time-critical systems. Furthermore, contemporary model-checking tools allow the designer to check whether a TA complies with a system specification. However, the exact timing constants are often uncertain during the design phase. Consequently, the designer is often able to build a TA with a correct structure, however, the timing constants need to be tuned to satisfy the specification. Moreover, even if the TA initially satisfies the specification, it can be the case that just a slight perturbation during the implementation causes a violation of the specification. Unfortunately, model-checking tools are usually not able to provide any reasonable guidance on how to fix the model in such situations. In this paper, we propose several concepts and techniques to cope with the above mentioned design phase issues when dealing with reachability and safety specifications.
Profinite equations are an indispensable tool for the algebraic classification of formal languages. Reiterman's theorem states that they precisely specify pseudovarieties, i.e., classes of finite algebras closed under finite products, subalgebras and quotients. In this article, Reiterman's theorem is generalized to finite EilenbergMoore algebras for a monad T on a category D: we prove that a class of finite T-algebras is a pseudovariety iff it is presentable by profinite equations. As a key technical tool, we introduce the concept of a profinite monad (T) over cap associated to the monad T, which gives a categorical view of the construction of the space of profinite terms.
Sketches are compact bit strings that are considered as products of an LSH for high-dimensional data. We use them in filtering for narrowing down solution candidates in similarity search. We propose a pivot selection method for narrow sketches with a length such as 16-bits by optimization algorithms with the accuracy of filtering itself as the objective function.
This Full Paper in the Research-To-Practice Category presents a long-term study about the effects of a student-centered course on communication and professional skills on students' thoughts, attitudes, and behavior. The course is offered at a European university as part of a computer science master's program. This paper shares the design and challenges of a longitudinal study that reaches ten years behind and employs a mixed-methods approach. Besides presenting and interpreting the findings, we shed light on which features tend to stay on students' minds and impact their way of being and acting in society. Moreover, we suggest implications for the design and practice in comparable courses to maximize constructive, sustainable effects, such as improved active listening, presentation skills, and openness to other perspectives. These are essential (not only) for computer science professionals. Our findings suggest that the course provided significant learning for the vast majority of respondents, including aspects such as presenting while keeping the other side in mind, managing one's stress, and becoming less shy to speak up. All in all, we aim to contribute an evidence-based source of motivation for instructors in technically focused curricula who hold a student-centered stance.
The vast variety of information on web forums makes them a valuable resource for various purposes such as scam detection, national security protection and sentiment analysis. However, it is challenging to extract useful information from web forums accurately and efficiently. First, several page types exist in web forums and content is presented in different formats in these pages. Second, the content on the forum pages is stored in the form of data blocks. For the information to be meaningful, it is necessary to extract the relevant data blocks separately. The main problem with generic content extraction systems is that they cannot distinguish among various pages nor extract information with the required granularity. Although, several content extraction methods exist for web forums, these methods either do not satisfy the above requirements or use heuristics based approaches (such as assumptions on standard visual appearances, etc., resulting in limited applicability to different varieties of forum). In this paper, we propose a general and efficient content extraction method using the properties of links present in forum pages. The effectiveness of our proposed method is shown through our experimental results.
Purpose - Illegal wildlife trade (IWT) is a transnational organized crime that generates billions in criminal proceeds each year. Yet, it is not regarded by many countries as a serious crime. There is also no general consensus on its recognition as a predicate offence for money laundering. In this regard, banks are misused in different ways to facilitate financial flows linked to IWT. This paper aims to illustrate the importance of the banking sector in combating money laundering relating to IWT. It also aims to demonstrate the need for a general recognition of IWT as a predicate offence for money laundering. Design/methodology/approach - This study investigates the implementation of money laundering controls by banks in the illegal-wildlife-trade context. As background to this investigation, it provides an overview of IWT, which is followed by an exploration of some of the general characteristics of the banking sector, before discussing the relevant Financial Action Task Force (FATF) recommendations. Findings - This study finds that the banking sector is well-placed to combat money laundering relating to the IWT and is, by virtue of its international nature and strong focus on compliance, able to be effective in preventing the use of the proceeds of IWT as well as in identifying broader trafficking networks. Moreover, the banking sector is well-equipped to develop appropriate platforms to facilitate the swift, easy and effective sharing of financial intelligence between banks at the local, regional and especially international level. Research limitations/implications - This study draws on publicly available information on financial flows relating to IWT. Little data and research are available on the financial flows and consequently the money laundering techniques used in cases suspected of IWT. Originality/value - There has been little scholarly research on the relationship between money laundering and the IWT as well as the financial flows of IWT in general. This study highlights some of the money laundering techniques used in relation to IWT by drawing on the works of various international organizations, including the FATF.
"A literature survey shows that the number of malware attacks is gradually growing over the years due to the growing trend of Internet of Medical Things (IoMT) devices. To detect and classify malware attacks, automated malware detection and classification is an essential subsystem in healthcare cyber-physical systems. This work proposes an attention-based multidimensional deep learning (DL) approach for a cross-architecture IoMT malware detection and classification system based on byte sequences extracted from Executable and Linkable Format (ELF; formerly named Extensible Linking Format) files. The DL approach automates the feature design and extraction process from unstructured byte sequences. In addition, the proposed approach facilitates the detection of the central processing unit (CPU) architecture of the ELF file. A detailed experimental analysis and its evaluation are shown on the IoMT cross-architecture benchmark dataset. In all the experiments, the proposed method showed better performance compared with those obtained from several existing methods with an accuracy of 95% for IoMT malware detection, 94% for IoMT malware classification, and 95% for CPU architectures classification. The proposed method also suggests a similar performance with an accuracy of 94% on the Microsoft malware dataset. Experimental results on two malware datasets indicate that the proposed method is robust and generalizable in cross-architecture IoMT malware detection, classification, and CPU architectures classification in healthcare cyber-physical systems."
Malware detectors based on machine learning (ML) have been shown to be susceptible to adversarial malware examples. However, current methods to generate adversarial malware examples still have their limits. They either rely on detailed model information (gradient-based attacks), or on detailed outputs of the model - such as class probabilities (score-based attacks), neither of which are available in real-world scenarios. Alternatively, adversarial examples might be crafted using only the label assigned by the detector (label-based attack) to train a substitute network or an agent using reinforcement learning. Nonetheless, label-based attacks might require querying a black-box system from a small number to thousands of times, depending on the approach, which might not be feasible against malware detectors. This work presents a novel query-free approach to craft adversarial malware examples to evade ML-based malware detectors. To this end, we have devised a GAN-based framework to generate adversarial malware examples that look similar to benign executables in the feature space. To demonstrate the suitability of our approach we have applied the GAN-based attack to three common types of features usually employed by static ML-based malware detectors: (1) Byte histogram features, (2) API-based features, and (3) String-based features. Results show that our model-agnostic approach performs on par with MalGAN, while generating more realistic adversarial malware examples without requiring any query to the malware detectors. Furthermore, we have tested the generated adversarial examples against state-of-the-art multimodal and deep learning malware detectors, showing a decrease in detection performance, as well as a decrease in the average number of detections by the antimalware engines in VirusTotal.
Built into Microsoft Windows is the ability for the operating system to track user window viewing preferences specific to Windows Explorer. This information, which is called ShellBag'' information, is stored in several locations within the Windows Registry in the Windows Operating System. This paper introduces a novel method to examine ShellBag information within Registry snapshots to reconstruct user activities. It compares different states of ShellBag information within consecutive Registry snapshots in order to detect ShellBag-related user actions. Nine detection rules are proposed on the basis of analyzing the causality between user actions and updated ShellBag information. This approach can be used to prove that certain interactions between the user and system must have, or must not have happened during a certain time period. (C) 2009 Digital Forensic Research Workshop. Published by Elsevier Ltd. All rights reserved.
The dataset contains intrusion detection alerts obtained via an alert sharing platform (SABU) for one week. A plethora of heterogeneous intrusion detection systems deployed across several organizations contributed to the sharing platform. The alerts are stored in the intrusion Detection Extensible Alert (IDEA) format and categorized using the eCSIRT.net Incident Taxonomy. Dataset can be used in several areas of cybersecurity research for the analysis of intrusion detection alerts including temporal and spatial correlations, reputation scoring, attack scenario reconstruction, and attack projection. The network identifiers (e.g., IP addresses, hostnames) are anonymized. However, the list of interesting features (e.g., presence on blacklists, geolocation) of such entities at the time of data collection is provided. (C) 2020 The Authors. Published by Elsevier Inc.
Against the tide of a huge amount of data, law enforcement agencies barely manage to focus on undiscovered child abuse materials (CAMs) to identify victims, abusers, and crime scenes. Currently, to alleviate this burden to some extent, there are international and national repositories of CAMs in place, such as Interpol's International Child Sexual Exploitation Database (ICSE DB), NCMEC's Child Recognition and Identification System (CRIS), and the UK's Child Abuse Image Database (CAID). This article introduced these global, regional, and national repositories and other related transnational initiatives comprehensively and also critiques the current trend of setting up more repositories and additional initiatives at local and regional levels. Later, the author elaborates on a plan for a single global repository governed by an international organization. Despite a significant decrease in the costs for a country to be actively involved in the fight against online child sexual abuse and emergence of new possibilities for more effective prevention strategies and investigational methods, such an idea of global centralization might face insurmountable opposition from current stakeholders, particularly because of organizational concerns and technological challenges.
"Recent progress in pose-estimation methods enables the extraction of sufficiently-precise 3D human skeleton data from ordinary videos, which offers great opportunities for a wide range of applications. However, such spatio-temporal data are typically extracted in the form of a continuous skeleton sequence without any information about semantic segmentation or annotation. To make the extracted data reusable for further processing, there is a need to access them based on their content. In this paper, we introduce a universal retrieval approach that compares any two skeleton sequences based on temporal order and similarities of their underlying segments. The similarity of segments is determined by their content-preserving low-dimensional code representation that is learned using the Variational AutoEncoder principle in an unsupervised way. The quality of the proposed representation is validated in retrieval and classification scenarios; our proposal outperforms the state-of-the-art approaches in effectiveness and reaches speed-ups up to 64x on common skeleton sequence datasets."
The emergence of the internet as a global, borderless communication platform afforded a wide range of social and economic opportunities to people throughout the world. Criminals have exploited the ability to communicate instantaneously around the globe to facilitate crossjurisdictional cyber-fraud and subsequently, online money laundering. Coordinating international fraud and money laundering schemes requires a medium of communication, such as online hacking and carding forums, where offenders meet to exchange information and to engage in their illegal business. For the study presented in this paper, publicly available online carding and hacking forums were downloaded and keywords of interest pertaining to online money laundering were extracted. This study undertakes an analysis of two large Russian-speaking hacking and carding forums by qualitatively analyzing and quantifying contexts of keyword usage. Findings indicate that cyber-fraudsters are primarily interested in cashing out digitally stolen funds and do so mainly by resorting to the services of money mules and virtual casinos.
Generating automated cyber resilience policies for real-world settings is a challenging research problem that must account for uncertainties in system state over time and dynamics between attackers and defenders. In addition to understanding attacker and defender motives and tools, and identifying relevant system and attack data, it is also critical to develop rigorous mathematical formulations representing the defender's decision-support problem under uncertainty. Game-theoretic approaches involving cyber resource allocation optimization with Markov decision processes (MDP) have been previously proposed in the literature. However, as is the case in strategic card games such as poker, research challenges using game-theoretic approaches for practical cyber defense applications include equilibrium solvability, existence, and possible multiplicity. Moreover, mixed uncertainties associated with player payoffs also need to be accounted for within game settings. This paper proposes an agent-centric approach for cybersecurity decision-support with partial system state observability. Multiple partially observable MDP (POMDP) problems are formulated and solved from a cyber defender's perspective, against a fixed attacker type, using synthetic (notional) system and attack parameters estimated from a Monte Carlo based sampling scheme. The agent-centric problem formulation helps address equilibrium related research challenges and represents a step toward automated and dynamic cyber resilience policy generation and implementation.
"Security by design is an up and coming paradigm which seeks to decrease the opportunity for corruption and disruption, and to increase the inherent stability, dependability and resilience of systems. Cyber experts need to be involved in the design phase as Finagle's Law, interpreted for cybersecurity, is 'if it can be hacked, it will - at the worst possible time'. Testing is designed to counter Murphy's Law and reduce resistentialism. Defensive programming, original cybersecurity, carries an overhead, for which there is often no demonstrable return. The priority of the design stage cybersecurity expert is to plan for contingencies and think like a hacker. It is about risk management; understanding this dictates security by design requirements in the knowledge that interconnected systems' security is only as strong as its weakest link. Definition and audit of Service Level Agreements (SLA) are an essential part of cybersecurity, as is the audit of any third party suppliers of componentry of the system under design. Governance and policy definitions, exciting for some, are integral to cybersecurity. Beyond this, the cybersecurity expert must consider system failure. Recent ransomware attacks have demonstrated the necessity for business continuity plans as recovery has still taken time. The Blue Team Field Manual has an impressive list of necessary documentation and actions required in this event, but glosses over the effort required. While the above is rolling maintenance, threat hunting differs. Every vulnerability or threat must be evaluated for consequential impact. Either a passionate interest in psychology or an extremely jaundiced view of the world is a necessary attribute for cybersecurity. Reality is so different from the aspirations of potential pen testers, incident responders, and AI security engineers facing 3 - 5 post-graduation years to proficiency, it is no wonder disillusion results in a shortage of 3.5 million cybersecurity experts."
Motion capture data digitally represent human movements by sequences of 3D skeleton configurations. Such spatio-temporal data, often recorded in the stream-based nature, need to be efficiently processed to detect high-interest actions, for example, in human-computer interaction to understand hand gestures in real time. Alternatively, automatically annotated parts of a continuous stream can be persistently stored to become searchable, and thus reusable for future retrieval or pattern mining. In this paper, we focus on multi-label detection of user-specified actions in unsegmented sequences as well as continuous streams. In particular, we utilize the current advances in recurrent neural networks and adopt a unidirectional LSTM model to effectively encode the skeleton frames within the hidden network states. The model learns what subsequences of encoded frames belong to the specified action classes within the training phase. The learned representations of classes are then employed within the annotation phase to infer the probability that an incoming skeleton frame belongs to a given action class. The computed probabilities are finally compared against a learned threshold to automatically determine the beginnings and endings of actions. To further enhance the annotation accuracy, we utilize a bidirectional LSTM model to estimate class probabilities by considering not only the past frames but also the future ones. We extensively evaluate both the models on the three use cases of real-time stream annotation, offline annotation of long sequences, and early action detection and prediction. The experiments demonstrate that our models outperform the state of the art in effectiveness and are at least one order of magnitude more efficient, being able to annotate 10 k frames per second.
This paper describes a methodology for the reconstruction of digital events by comparing states captured in time. Microsoft Windows Restore Point data is used to illustrate how to organize captured state information into a useful timeline of user and system events. It is shown that by comparing consecutive states, events can be uncovered that would otherwise be unknown by analysis of the current system state alone. (C) 2009 Elsevier Ltd. All rights reserved.
SPRING is a family of pseudo-random functions that aims to combine the guarantees of security reductions with good performance on a variety of platforms. Preliminary software implementations for small-parameter instantiations of SPRING were proposed at FSE 2014, and have been demonstrated to reach throughputs within small factors of those of AES. In this paper, we complement these results and investigate the hardware design space of these types of primitives. Our first (pragmatic) contribution is the first FPGA implementation of SPRING in a counter-like mode. We show that the rounded product operations in our design can be computed efficiently, reaching throughputs in the hundreds of megabits/second range within only 4% of the resources of a modern (Xilinx Virtex-6) reconfigurable device. Our second (more prospective) contribution is to discuss the properties of SPRING hardware implementations for side-channel resistance. We show that a part of the design can be very efficiently masked (with linear overhead), while another part implies quadratic overhead due to non-linear operations (similarly to what is usually observed, e. g., for block ciphers). Yet, we argue that for this second part of the design, resistance against simple power analysis may be sufficient to obtain concrete implementation security. We suggest ways to reach this goal very efficiently, via shuffling. We believe that such hybrid implementations, where each part of the design is protected with adequate solutions, is a promising topic for further investigation.
EU and cyber security Securing network and information systems in the EU is essential to ensure prosperity and to keep the online economy running. The European Union works on a number of fronts to ensure cybersecurity in Europe, from providing the delivery of better internet for kids to implementing the international cooperation on cybersecurity and cybercrime. As societies, governments and businesses become increasingly reliant on the Internet for the normal functioning of every-day activities and the supply of essential services, protecting cyberspace from malicious activities has become a critical action point for policymakers globally. While digital and networked technologies promise much, the implications of successful attacks can be huge. The continued rapid development of information and communication technologies, globalization, the drastic increase in data volumes and the growing number of different types of equipment connected to data networks have an impact on daily life, the economy and the functioning of the state. On the one hand, this level of ICT development will contribute to the improved availability and usability of services, enhance transparency and citizen participation in governance, and cut public as well as private sector costs. On the other hand, the increasing importance of technology is accompanied by an increase in the state's growing dependence on already entrenched e-solutions, and cements the expectation of technology operating eamlessly. Social processes are also becoming increasingly dependent on a growing number of information technology resources, and in the future attention must be drawn to the fact that society at large, and each individual in particular, will be able to maintain control over the corresponding processes. Otherwise, there is potential for information technologies to reduce the role of humans in the decision making process, and processes may become self-regulatory (technological singularity). The number of state actors in cyberspace that are involved in cyber espionage targeted at computers connected to the Internet as well as closed networks continues to grow, with their aim being to collect information on both national security as well as economic interests. The amount and activeness of states capable of cyber-attacks are increasing. In addition to the activation of state actors, the ability of politically motivated individuals and groups with limited means to organize their activities using social networks and carry out denial of service and other types of attacks is growing as well. Meaningful and effective cooperation between the public and private sector in the development of cyber security organization as well as in preventing and resolving cyber incidents is becoming increasingly unavoidable. National defense and internal security are dependent on the private sector's infrastructure and resources, while at the same time the state can assist vital service providers and guarantors of national critical information infrastructure as a coordinator and balancer of various interests.
"Malicious networks of botnets continue to grow in strength as millions of new users and devices connect to the internet each day, many becoming unsuspectingly complicit in cyber-attacks or unwitting accomplices to cybercrimes. Both states and nonstate actors use botnets to surreptitiously control the combined computing power of infected devices to engage in espionage, hacking, and to carry out distributed denial of service attacks to disable internet-connected targets from businesses and banks to power grids and electronic voting systems. Although cybersecurity professionals have established a variety of best practices to fight botnets, many important questions remain concerning why levels of botnet infections differ sharply from country to country, as relatively little empirical testing has been done to establish which policies and approaches to cybersecurity are actually the most effective. Using newly available time-series data on botnets, this article outlines and tests the conventionally held beliefs and cybersecurity strategies at every level-individual, technical, isolationist, and multilateral. This study finds that wealthier countries are more vulnerable than less wealthy countries; that technical solutions, including patching software, preventing spoofing, and securing servers, consistently outperform attempts to educate citizens about cybersecurity; and that countries which favor digital isolation and restrictions on internet freedom are not actually better protected than those who embrace digital freedom and multilateral approaches to cybersecurity. This latter finding is of particular importance as China's attempts to fundamentally reshape the internet via the Digital Silk Road component of the Belt and Road Initiative will actually end up making both China and the world less secure. Due to the interconnected nature of threats in cyberspace, states should instead embrace multilateral, technical solutions to better govern this global common and increase cybersecurity around the world."
The goal of this paper was to analyze hacker forums to better understand the threats they pose to Canadian critical systems specifically and cyber-security more generally. To facilitate the data collection, a customized web-crawler was developed to specifically capture the structured content posted to forums. Three hacker forums were selected for analysis that represented different facets of the hacker community: carding (data theft), coding (malware development and deployment), and security (distribution of vulnerabilities). We identified and geolocated user disclosed IP addresses to try to identify critical systems and determine the extent as well as context in which critical systems were openly discussed by forum users. In total, 311,501 analyzable IP addresses were extracted from the data with 3,168 (1%) geolocated to Canada. The prevalence of Canadian IP addresses does not indicate their potential for exploitation, although it does highlight a perceived heightened interest in Canadian critical systems by hacker forum users. Potential at-risk systems included government agencies, universities across Canada, and private industries within the transportation network, namely aviation and shipping firms.
The increasing ubiquitous use of ICT has changed the way the society operates. When used effectively, ICT has provided people not only with access to information but also opportunities to participate in the global economy. Unfortunately, the widespread use of ICT presents a wide range of social and ethical issues from online safety to security and misuse of information. It is important that users are equipped with appropriate knowledge and skills to operate their devices securely. Therefore, countries should address the issue of Information Security to minimize the risk and bring it to an acceptable level whilst still exploiting the opportunities offered by ICT. An in-depth analysis has shown that effective implementation of information security and especially education is not adhering to paper based requests from government policies and strategies. The lack of cyber laws/regulations, shortage of ICT security skills and poorly secured networks are just some of the challenges affecting policy development and implementation. There is an urgent need for action within all key stakeholders to promote responsible and safe use of ICT. The issue of how to secure cyberspace is important to all nations including developing and least developed countries because cyber insecurity has international ramifications. An attack on one vector could affect the rest of cyberspace. The fact that criminals can commit crime anonymously with minimum effort, and minimum risk of being caught makes cybercrime the favourable tool for many criminals and also a great concern to all. North-South collaboration and support is needed in promoting cyber-security and especially maintaining the concept of solidarity in information security education. Failure to act now means, we are allowing the cyber criminals to take over our networks for their use. The current situations in cyber-security in Kenya, Rwanda and Nepal are presented and models of clever low cost implementation for awareness and educational are shared. The term cyber-wellness (in regard to legislation/regulations, national curriculum, and other educational initiatives) is defined and analysed in the three countries. Furthermore, cyber-wellness is compared with advanced countries to identify the gaps. Literature analysis, questionnaire field research and workshop discussions have identified relevant gaps, but also hope for effective implementation models on strong education and solidarity to assist the policy makers and other stakeholders in development and integration of ICT security in education curriculum at all stages. This paper discusses current ICT security policy implementation issues in education and how this can be improved by sharing knowledge and successful implementation models. The research and development is made in the frame of the Information Security Education and Solidarity Initiative (ISES), a project sponsored by UNESCO Participation Programme submitted through Technical Committee No. 3 Education of the International Federation on Information Processing (IFIP).
The National Institute of Standards and Technology (NIST) published the NIST Framework for Improving Critical Infrastructure Cybersecurity of 2014, followed by an updated version in 2017. The Framework, which was developed as a joint effort between the U.S. Federal Government and the private sector, serves only as a guideline and is not mandated by any legal authority. Currently, adoption of the Framework is voluntary. The Financial Sector, one of sixteen Department of Homeland Security critical infrastructure sectors, should be incentivized to adopt the framework, based on inconsistency and accountability of best practices implementation across the sector. Global cyber attack opportunists used the 2020 COVID-19 pandemic to exploit cybersecurity vulnerabilities and gaps in the U.S. Financial Sector. The NIST Cybersecurity Framework provides guidelines for strengthening cybersecurity and identifies areas of potential cyber attack impacts. This paper is a summary of the author's published 2021 doctoral dissertation, which includes research and analysis of reported Financial Sector risks, failures and impacts due to weak or lack of cybersecurity controls. The study also provides analysis of success stories of Financial Sector and other entities which have adopted the NIST Cybersecurity Framework. Dr. Goodwin is a Senior Member of IEEE.
As online forums contain a vast amount of information that can aid in the early detection of fraud and extremist activities, accurate and efficient information extraction from forum sites is very important. In this paper, we discuss the limitations of existing works in the extraction of information from generic web sites and forum sites. We also identify the need for better suited, generalized and lightweight algorithms to carry out a more accurate and efficient information extraction while eliminating noisy data from forum sites. In this paper, we propose three generalized and lightweight algorithms to carry out accurate thread and post content extraction from web forums. We evaluate our algorithms based on two strict criteria and to the granularity of the (DOM tree) node level correctness. We consider a thread or post as successfully extracted by our algorithms only if (i) all the contents in its text and anchor nodes are extracted correctly, and (ii) each content node is grouped correctly according to its respective thread or post. Our experiments on ten different forum sites show that our proposed thread extraction algorithm achieves an average recall and precision rate of 100% and 98.66%, respectively, while our core post extraction algorithm achieves an average recall and precision rate of 99.74% and 99.79%, respectively.
The dark web is a concealed portion of the Internet that can only be accessed through specialized software. Although multiple dark web technologies exist, with a common trait of using encryption to enforce anonymity, the Tor network remains the most prominent dark web network. To visit websites on the network, the user must use a heavily modified Firefox browser. The use of encryption to achieve anonymity poses a significant challenge for law enforcement that wishes to monitor users and content for illicit activity. This study examines Tor by focusing on the network structures created between websites via hyperlinks. Examining hyperlinks can provide insight into how virtual communities form on a network. We explore traditional social disorganization principles as a basis to draw comparisons between these virtual communities and real-life crime-prone neighborhoods. Automated data collection techniques were used to leverage the interconnected nature of domains on Tor. Using social network analysis, website hyperlinks are examined and core sites are identified. The analysis shows that these core sites forma significant portion of all connections made on the network with a density of 0.132. This core serves a critical function and has implications for detecting how users connect on Tor.
Identity theft has been known for some centuries whereby falsified identity documents were misused as well as offences such as impersonating others were common in the society. However, the advent of technology changed the method used for conducting this crime, whereby through the use of the Internet, personal information is can be stolen and misused by criminals. The crime has its causes originating from human error and judgement to failure of computing and networking systems that allow unauthorized access to personal information. In order to provide a better tool of investigating this crime, there is the need to explore the causes of the crime thereby providing a better framework for investigating Identity theft crimes. This study uses Root Cause Analysis (RCA) as a preliminary tool that serves to provide a depicted identification of the causes of Identity theft paving the way into investigating the crime and creating incident response plans.
In this paper, we first identify the need to be equipped with the capability to perform raw volatile memory data acquisition from live smartphones. We then investigate and discuss the potential of different approaches to achieve this task on Symbian smartphones. Based on our initial analysis, we propose a simple, flexible and portable approach which can have a full-coverage view of the memory space, to acquire the raw volatile memory data from commercial Symbian smartphones. We develop the tool to conduct the proof-of-concept experiments on the phones, and are able to acquire the volatile memory data successfully. A discussion on the problems we have encountered, the solutions we have proposed and the observations we have made in this research is provided. With the acquired data, we conduct an analysis on the memory images of the identified memory regions of interest, and propose a methodology for the purpose of in-depth malware security and forensics analysis.
Source Camera identification of digital images can be performed by matching the sensor pattern noise (SPN) of the images with that of the camera reference signature. This paper presents a non-decimated wavelet based source camera identification method for digital images. The proposed algorithm applies a non-decimated wavelet transform on the input image and split the image into its wavelet sub-bands. The coefficients within the resulting wavelet high frequency sub-bands are filtered to extract the SPN of the image. Cross correlation of the image SPN and the camera reference SPN signature is then used to identify the most likely source device of the image. Experimental results were generated using images of ten cameras to identify the source camera of the images. Results show that the proposed technique generates superior results to that of the state of the art wavelet based source camera identification.
This study sought to design a framework to enhance database security in state-owned universities. Database security has been a cause for concern for state-owned universities following many reported cases of state-owned universities that were repeatedly hacked locally and the fact that internationally and there seems to be no strategy in place to guide database security mechanisms in state-owned universities. The study followed a positivist research philosophy using a hypothetical model to test various hypotheses. Through the lens of activity, Social Technology System and deterrence theories, using a survey method to gather the information, the hypotheses were tested and analyzed to further understand security vulnerabilities, security controls and ideal architectural requirements of database security. Quantitative data were collected from 104 respondents using questionnaires. A deductive approach was employed to ensure that the proposed strategy was designed after collecting the necessary data. The quantitative data were analyzed using SPSS version 22. The research findings highlighted the following issues that affect database security: human factor, work environment and technology in use. These issues lead to data exposure, data theft, data corruption, loss of revenue and loss of goodwill. Finally, a framework was proposed that would enhance database security in state-owned universities.
This survey paper provides a comprehensive overview of recent research and development in network security that uses graphs and graph-based data representation and analytics. The paper focuses on the graph-based representation of network traffic records and the application of graph-based analytics in intrusion detection and botnet detection. The paper aims to answer several questions related to graph-based approaches in network security, including the types of graphs used to represent network security data, the approaches used to analyze such graphs, the metrics used for detection and monitoring, and the reproducibility of existing works. The paper presents a survey of graph models used to represent, store, and visualize network security data, a survey of the algorithms and approaches used to analyze such data, and an enumeration of the most important graph features used for network security analytics for monitoring and botnet detection. The paper also discusses the challenges and limitations of using graph-based approaches in network security and identifies potential future research directions. Overall, this survey paper provides a valuable resource for researchers and practitioners in the field of network security who are interested in using graph-based approaches for analyzing and detecting malicious activities in networks.
In recent years, ransomware attacks have led to disastrous consequences for victims, not just due to the payment ransom amount but also due to the recovery costs associated with these attacks. So far only a few empirical studies have analysed the financial impact of ransomware attacks. This study aims to understand the expected financial gains for attackers and financial losses of victims after a ransomware attack. To do so, we build a dataset based on 453 ransomware attack investigation reports in the Netherlands reported to the Dutch Police between 2019 and 2022. Using rational choice model of crime (RCM) and crime scripting we hypothesise that the effort of an attacker, victim characteristics and context variables influence not only the ransom requested by an attacker but also the financial losses reported by victims. We use generalised linear models to evaluate and quantify this influence. Our results show that attacker's efforts such as using Ransomware-as-a-Service (RaaS) and victim characteristics such as industry sector, contribute to the ransom requested by attackers and financial losses reported by victims. We also show that the availability of recoverable backups explains the likelihood of victims paying the ransom. A limitation of the present study is the interpretation of the results due to selection bias of victims willing to report to the police. Despite this limitation, we argue that our methodology and results lay the groundwork for future large-scale empirical studies and add to our understanding of attacker and victim behaviour.
The rise of the internet or, more specifically, of services offered and conducted online has led to a dramatic rise in frauds and scams. This study is a systematic review of the literature on the use of crime script analysis in the field of fraud facilitated by the internet to identify stages of the crime commission process across different forms of fraud and examine ways to disrupt those crimes. The scripts for different forms of fraud shared three common elements: communicating with the victim, recruiting enablers, and using money mules. These common elements suggest possible prevention measures. Future applications of crime scripts in the field of fraud and financial crime more broadly are discussed.
In June 2020 India opted for banning 59 Chinese Apps. Contrary to the apprehension of banning gaming apps including PUBG, the government did not do so when the Chinese apps were banned. But soon PUBG also got banned in India. Several stakeholders had raised their concerns in this regard as PUBG may affect the mental health of children who are the major users of this app. But on the other hand, it had been observed that PUBG had remained the biggest respite for children who were confined to homes due to COVID-19. S.69A of the Information Technology Act 2000 (amended in 2008) had been the main tool by the government to block access to Chinese apps. PUBG developers had however tried to outsmart S.69A of the Information Technology Act, 2000 (amended in 2008), which had been used primarily to block the website and by using Artificial Intelligence for preventing children below 18 from being engaged for a continuous period, being exposed to nudity, etc. This article aims to analyse the tussle between the scope and inherent meaning of S.69A and 79 of the Indian Information technology Act, 2000 (amended in 2008) and the ever-developing due diligence of the game developers with special concern for child safety and mental health of the children, which may make the later a winner or a loser in the legal battleground.
Cybercrimes are evolving on a regular basis and as such these crimes are becoming a greater threat day by day. Earlier these threats were very general and unorganized. In the last decade, these attacks have become highly sophisticated in nature. This higher level of coordination is possible mainly due to botnets, which are clusters of infected hosts controlled remotely by an attacker (botmaster). The number of infected machines is continuously rising, thereby resulting in botnets with over a million infected machines. This powerful capability gives the botmaster a lethal weapon to launch various security attacks. As a result, botnet detection techniques received greater research focus. The Domain Name System (DNS) is a large scale distributed database on the Internet, which is being abused as a botnet communication channel. While there are numerous survey and review papers on botnet detection, there are two survey papers on DNS-based botnet detection which are neither comprehensive nor take into consideration various parameters vital for effective comparison. This survey presents a new classification for DNS-based botnet detection techniques and provides a deep analysis of each technique within the category. (C) 2019 Elsevier Ltd. All rights reserved.
Browser extensions have by and large become a normal and accepted omnipresent feature within modern browsers. However, since their inception, browser extensions have remained under scrutiny for opening vulnerabilities for users. While a large amount of effort has been dedicated to patching such issues as they arise, including the implementation of extension sandboxes and explicit permissions, issues remain within the browser extension ecosystem through user-scripts. User-scripts, or micro-script extensions hosted by a top-level extension, are largely unregulated but inherit the permissions of the top-level application manager, which popularly includes extensions such as Greasemonkey, Tampermonkey, or xStyle. While most user-scripts are docile and serve a specific beneficial functionality, due to their inherently open nature and the unregulated ecosystem, they are easy for malicious parties to exploit. Common attacks through this method involve hijacking of DOM elements to execute malicious javascript and/or XSS attacks, although other more advanced attacks can be deployed as well. User-scripts have not received much attention, and this vulnerability has persisted despite attempts to make browser extensions more secure. This ongoing vulnerability remains an unknown threat to many users who employ user-scripts, and circumvents security mechanisms otherwise put in place by browsers. This paper discusses this extension derivative vulnerability as it pertains to current browser security paradigms.
An evidence base of what works using high-quality evaluations in tackling societal problems has become the norm in many spheres, including tackling traditional crime. Yet, as we show in the example of fraud faced by organisations, high-quality evaluations are not always possible, or even necessary for tackling problems effectively. Drawing on a review of over 400 research studies exploring the prevention of fraud, this paper finds a paucity of studies meeting the highest quality of standards of evaluation using the Maryland Scale. This is largely because of the barriers to implementing the Maryland Scale, given the challenges of measuring fraud, rather than because of a low quality of research per se. In the absence of high-quality evaluations, this paper uses a novel alternative to the Maryland scale to identify a range of effective tools that organisations can use to prevent fraud. Finally, the paper provides practical and theoretical reflections upon a broader problem of how and to what extent scientific evaluations of high-quality evidence are necessary in combating fraud effectively.
"As violent extremists continue to surface in online discussion forums, counter-extremism agencies search for new and innovative ways of uncovering their digital indicators. Using a sample of approximately 1 million posts and 26,000 unique users across four Islamic-based discussion forums, this study proposed a method of identifying the most radical users on the Dark Web. Several characteristics of each user's postings were analyzed using Parts of Speech (POS) tagging, a custom openNLP based tagger, sentiment analysis, and a novel algorithm called Sentiment-based Identification of Radical Authors (SIRA). POS tagging was used to develop a list of the 400 most frequently cited nouns across the discussion forums. With this list, sentiment analysis provided the context surrounding users' posts, and each post was assigned a polarity value. Radical scores were calculated using SIRA, which is an algorithm that accounts for a user's percentile score for average sentiment score, volume of negative posts, severity of negative posts, and duration of negative posts. Results did not suggest that a simple typology best described the most radical users in the Dark Web; however, the findings indicated that SIRA was flexible enough to evaluate several combinations of online activity that could identify the most radical users in the discussion forums. In addition, SIRA identified the same user across two separate discussion forums as the most radical, thus providing validation for the algorithm. This particular user was linked to an extremist website that supported terrorists. Lastly, the results revealed that the Gawaher and Islamic Awakening web forums hosted the highest volume of most radical users in the sample."
Cybersecurity students need to develop practical skills such as using command-line tools. Hands-on exercises are the most direct way to assess these skills, but assessing students' mastery is a challenging task for instructors. We aim to alleviate this issue by modeling and visualizing student progress automatically throughout the exercise. The progress is summarized by graph models based on the shell commands students typed to achieve discrete tasks within the exercise. We implemented two types of models and compared them using data from 46 students at two universities. To evaluate our models, we surveyed 22 experienced computing instructors and qualitatively analyzed their responses. The majority of instructors interpreted the graph models effectively and identified strengths, weaknesses, and assessment use cases for each model. Based on the evaluation, we provide recommendations to instructors and explain how our graph models innovate teaching and promote further research. The impact of this paper is threefold. First, it demonstrates how multiple institutions can collaborate to share approaches to modeling student progress in hands-on exercises. Second, our modeling techniques generalize to data from different environments to support student assessment, even outside the cybersecurity domain. Third, we share the acquired data and open-source software so that others can use the models in their classes or research.
Cybercriminals use domain generation algorithms (DGAs) to prevent their servers from being potentially blacklisted or shut down. Existing reverse engineering techniques for DGA detection is labor intensive, extremely time-consuming, prone to human errors, and have significant limitations. Hence, an automated real-time technique with a high detection rate is warranted in such applications. In this article, we present a novel technique to detect randomly generated domain names and domain name system (DNS) homograph attacks without the need for any reverse engineering or using nonexistent domain (NXDomain) inspection using deep learning. We provide an extensive evaluation of our model over four large, real-world, publicly available datasets. We further investigate the robustness of our model against three different adversarial attacks: DeepDGA, CharBot, and MaskDGA. Our evaluation demonstrates that our method is effectively able to identify DNS homograph attacks and DGAs and also is resilient to common evading cyberattacks. Promising results show that our approach provides a more effective detection rate with an accuracy of 0.99. Additionally, the performance of our model is compared against the most popular deep learning architectures. Our findings highlight the essential need for more robust detection models to counter adversarial learning.
"The article considers the problems associated with the development of new state approaches to ensure the security of critical information infrastructure (hereinafter the CII) in the context of the existence of threats to their information security, including computer attacks in its regard. We analyzed the main provisions of the Federal Law No. 187-FZ dated July 26, 2017 On the Safety of the CII of the Russian Federation. We disclosed the content and essence of the concept of security of the CII. It is justified that the security of the CII shall be based on the principles and methodology of ensuring national security. We have developed proposals to classify part of the subjects of economic activity as the CII subjects, as well as offered some additional mechanisms to increase the security of the CII. We proposed to develop and implement: the federal state standard of higher education in the direction of safety of the CII; retraining and advanced training courses in the direction of safety of the CII; a mechanism for improving the qualifications of officials of the CII subjects on various issues of ensuring its security; security insurance mechanism for the CCI; a mechanism for organizing international, all-Russian, regional and sectoral cyber orders at the CII objects. It has been established that the security of the CII directly depends on the correctness of decision-making in countering computer attacks, the speed and effectiveness of the actions of their entities. It is proved that the criminal law norm on liability for unlawful influence on the CII of the Russian Federation shall be changed."
Despite the constant evolution of similarity searching research, it continues to face challenges stemming from the complexity of the data, such as the curse of dimensionality and computationally expensive distance functions. Various machine learning techniques have proven capable of replacing elaborate mathematical models with simple linear functions, often gaining speed and simplicity at the cost of formal guarantees of accuracy and correctness of querying. The authors explore the potential of this research trend by presenting a lightweight solution for the complex problem of 3D protein structure search. The solution consists of three steps - (i) transformation of 3D protein structural information into very compact vectors, (ii) use of a probabilistic model to group these vectors and respond to queries by returning a given number of similar objects, and (iii) a final filtering step which applies basic vector distance functions to refine the result.
Contrary to the early days in which freedom of access and knowledge for all was a fundamental tenet of the burgeoning internet, today, the internet is a hostile environment. This paper represents that practices of trusted autonomous systems and zero trust must reside both organisationally and in any connected device, and that they will be insufficient to secure any hyperconnected system. For example, in the area of Connected and Autonomous Vehicles (CAV), the emphasis of standards associated with cybersecurity is to protect vehicles from an external attack, particularly where it may have an impact on safety. The Internet of Vehicles (IoV) is a subset of the IoT. Security of the IoT has not been standardised and is applied by proprietary organisations. Little consideration has been given to the IoV not being the target of an attack, but a means to an end. Vehicles necessarily communicate with the infrastructure and other vehicles. Additionally, outsourced organisational systems and third-party components within a complex, interconnected, communicating system renders it impossible to define and secure all endpoints. Malware in any part of the hyperconnected systems, provides opportunities for hackers. The security of the systems is only as strong as its weakest link. As an example of an onward attack, a DDOS attack is a debilitating exhaustion of resources attack that disables intended operation of a system. Formation of a botnet from the IoV for use in onward attacks is hypothesised. Such a botnet could have a global reach. Research has indicated that the complete cleansing of a computer-based botnet could take between 5 and 15 years. However, with decentralised command and control and peer to peer communication, the botnet could remain persistent. It is shown that persistent botnet formation software is readily available on the dark market and that specialist software can be commissioned as Crime as a Service. Organised crime groups have already reverse engineered vehicle systems. Mobile attack platform swarms are an attractive proposition to the malefactor, and with the potential for global reach, perhaps the next step on from ransomware. Intermittent disruption from mobile sources is hard to trace and there is an existing pool of 1.2 billion vehicles.
In this short paper, we take a first step towards empirically assessing Internet-wide malicious activities generated from and targeted towards Internet-scale business sectors (i.e., financial, health, education, etc.) and critical infrastructure (i.e., utilities, manufacturing, government, etc.). Facilitated by an innovative and a collaborative large-scale effort, we have conducted discussions with numerous Internet entities to obtain rare and private information related to allocated IP blocks pertaining to the aforementioned sectors and critical infrastructure. To this end, we employ such information to attribute Internet-scale maliciousness to such sectors and realms, in an attempt to provide an in-depth analysis of the global cyber situational posture. We draw upon close to 16.8 TB of darknet data to infer probing activities (typically generated by malicious/infected hosts) and DDoS backscatter, from which we distill IP addresses of victims. By executing week-long measurements, we observed an alarming number of more than 11,000 probing machines and 300 DDoS attack victims hosted by critical sectors. We also generate rare insights related to the maliciousness of various business sectors, including financial, which typically do not report their hosted and targeted illicit activities for reputation-preservation purposes. While we treat the obtained results with strict confidence due to obvious sensitivity reasons, we postulate that such generated cyber threat intelligence could be shared with sector/critical infrastructure operators, backbone networks and Internet service providers to contribute to the overall threat remediation objective.
A negative outcome of the nascent cashless policy in Nigeria has been persistent electronic banking fraud (e-fraud). Fraud occurrence in any financial space indicates insecurity and loopholes being exploited by fraudsters. This underscores the importance of trust governance in electronic banking and its centrality in a transiting cashless economy like Nigeria. Against this background, we investigated e-banking fraud and the role trust governance plays in both the adoption and refusal to migrate and use electronic banking in Nigeria. Using qualitative methods (in-depth and key informant interviews) of data collection, 30 participants were purposively selected and in some instances reached through the snowball and referral methods. Findings showed internal, external, and collaborative dimensions of e-fraud. Experiences of fraud reportedly affected adoption and migration of bank customers to e-banking platforms. Although weak governance mechanism was reported, banks nonetheless are embracing security mechanisms such as sending SCAM alert messages to customers, while shaming and sack of compromised staff were employed as within-bank measures to secure the confidence of customers in the evolving financial ecosystem.
This research utilizes social network analysis to determine the success of three different disruption strategies on a child exploitation network extracted from the public internet. Using a custom-written web-crawler called LECEN, data from a set of hyperlinked child-exploitation websites was collected from the Internet. From these data, two types of networks were coded: the nodes of the first network consisted of only website domains, while the nodes of the second were generated using the registrant data, where the nodes represented the legal owners of those same domains. Three attack scenarios were carried out on these two networks: two types of hub attacks (one focused on in-degree and one focused on out-degree) and a bridge attack. Using these disruption strategies, it was found that bridge attacks were more suitable for disrupting the domain networks, while both hub-attacks could be favored when disrupting the network of registrants. These findings have implications for law enforcement, as it provides real-world applications to disruption where registrants may be targeted directly.
Purpose The purpose of this study is to focus on organisation's cybersecurity strategy and propose a high-level programme for cybersecurity education and awareness to be used when targeting small- and medium-sized enterprises/businesses (SMEs/SMBs) at a city-level. An essential component of an organisation's cybersecurity strategy is building awareness and education of online threats and how to protect corporate data and services. This programme is based on existing research and provides a unique insight into an ongoing city-based project with similar aims. Design/methodology/approach To structure this work, a scoping review was conducted of the literature in cybersecurity education and awareness, particularly for SMEs/SMBs. This theoretical analysis was complemented using a case study and reflecting on an ongoing, innovative programme that seeks to work with these businesses to significantly enhance their security posture. From these analyses, best practices and important lessons/recommendations to produce a high-level programme for cybersecurity education and awareness were recommended. Findings While the literature can be informative at guiding education and awareness programmes, it may not always reach real-world programmes. However, existing programmes, such as the one explored in this study, have great potential, but there can be room for improvement. Knowledge from each of these areas can, and should, be combined to the benefit of the academic and practitioner communities. Originality/value The study contributes to current research through the outline of a high-level programme for cybersecurity education and awareness targeting SMEs/SMBs. Through this research, literature in this space was examined and insights into the advances and challenges faced by an on-going programme were presented. These analyses allow us to craft a proposal for a core programme that can assist in improving the security education, awareness and training that targets SMEs/SMBs.
An analysis of a host behavior is an essential key for modern network management and security. A robust behavior profile enables the network managers to detect anomalies with high accuracy, predict the host behavior, or group host to clusters for better management. Hence, host profiling methods attract the interest of many researchers, and novel methods for host profiling are being introduced. However, these methods are frequently developed on preprocessed and small datasets. Therefore, they do not reflect the real-world artifacts of the host profiling, such as missing observations, temporal patterns, or variability in the profile characteristics in time. To provide the needed insight into the artifacts of host profiling in real-world settings, we present a study of the host behavior in a network conducted on a one-year-long real-world network dataset. In the study, we inspect the availability of the data for host profiling, identify the temporal patterns in host behavior, introduce a method for stable labeling of the hosts, and assess the variability of the host characteristics in the course of the year using the coefficient of variance. Moreover, we make the one-year dataset containing nine characteristics used for host behavior analysis available for public use and further research, including selected use cases representing host profiling caveats. We also share the record of analysis presented in the paper.
In this paper, researchers provide a preliminary analysis on the forensic implications of cloud computing reference architecture, on the segregation of duties of cloud actors in cloud investigations, forensic artifacts on all layers of cloud system stack, cloud actors interaction scenarios in cloud investigations, and forensic implications of all cloud deployment models. The analysis serves as feedback and input for integrating forensic considerations into cloud standardization processes from early stage, and specifies requirements and directions for further standardization efforts.
Access control has been proposed asthe solution to prevent unauthorized accesses to sensitive system resources. Historically, access control models use a two-valued decision set to indicate whether an access should be granted or denied. Many access control models have extended the two-valued decision set to indicate, for instance, whether a policy is applicable to an access query or an error occurred during policy evaluation. Decision sets are often coupled with operators for combining decisions from multiple applicable policies. Although a larger decision set is more expressive, it may be necessary to reduce it to a smaller set in order to simplify the complexity of decision making or enable comparison between access control models. Moreover, some access control mechanisms like XACML v3 uses more than one decision set. The projection from one decision set to the other may result in a loss of accuracy, which can affect the final access decision. In this paper, we present a formal framework for the analysis and comparison of decision sets centered on the notion of decision reduction. In particular, we introduce the notion of safe reduction, which ensures that a reduction can be performed at any level of policy composition without changing the final decision. We demonstrate the framework by analyzing XACML v3 against the notion of safe reduction. From this analysis, we draw guidelines for the selection of the minimal decision set with respect to a given set of combining operators.
Most research on internet-frauds has focused on victims' cognitive and personality vulnerabilities and ignored that scammers often have been victims of financial cyber-crimes [1]. These victim-offenders expressed retaliation as a motive to offend and highlight the overlooked emotions and social learning contributing to cyberscams [2]. A broader understanding of the motives, emotions and knowledge of victim-offenders, solely offenders and solely victims might improve awareness campaigns and security training [3]. In this talk, I use a life-course perspective of social learning [4] to examine media and social sources, prior victimization, and knowledge and attitudes about relationships contribute to committing internet frauds. Data are drawn from two large self-report surveys of victimization and perpetration of a wide range of internet frauds. Deviant friends and family members, mentors, online discussions, and contacts in the dark web increase support for retaliation and provide praise for perpetrating internet-fraud. Those who attended victim support groups and have knowledge of dating app etiquette have more accurate knowledge about suspicious communications on dating apps. Beyond low self-control, psychopathy and committing frauds in the real world, those with higher rates of victimization more often perpetrated cyber-frauds. The life-course perspective suggests a broader view of the emotional and social context of offending might improve the content and focus of awareness campaigns and security training. These campaigns and training often ignore how scammers learn manipulative tactics from friends, family, media, and online sources. This focus also might enhance AI tools to detect and intercept fraudulent messages on dating and social media sites.
"In recent years, widespread internet usage, increasing numbers of IoT devices, and vast data on social media have increased cyber attack vectors. Parallel to this trend, the demand for the cyber security workforce has augmented; however, employers could not fill these positions due to the shortage in the field. This anomaly, in turn, has placed companies in a vulnerable situation by being exposed to an increased level of cyber security threats/incidents. Relevant authorities underscore the importance of a comprehensive cybersecurity workforce framework to overcome this problem. Given this context, this study offers a CyberSec Labs framework to improve the skills of potential cybersecurity actors. The framework extensively benefits from hands-on exercises like cloud computing and networking, OSINT, data-driven cybersecurity approach, vulnerability and penetration test analysis, and volatility analysis. The overall goal of this open-source material is to prepare students for their future cybersecurity roles, which contributes to overcoming the shortage of skilled human capital in the cybersecurity field."
"The total number of webpages has grown substantially since the birth of the Internet. So too have the number of webpages dedicated to radical yet subtle content. As these new circumstances have necessitated a guided data collection method, one that can sidestep the laborious manual methods that have been classically utilized, simple keyword analysis has not been sufficient to identify radical sites on Web 1.0 pro-extremist, anti-extremist, and news sites, for example, may use the same keywords to discuss the same event but have a very different motivation. In an effort to explore this problem, we completed an exercise involving the use of a web-crawler to collect 20,000 webpages from five sentiment-based classes to assess their differences: (1) radical Right sites; (2) radical Islamic sites; (3) anti-extremist sites; (4) news source sites discussing extremism; and (5) sites that did not discuss extremism. Parts-of-Speech (POS) tagging was used to identify 198 of the most frequent keywords within the data, and the sentiment value for each of these keywords was calculated for each webpage using sentiment analysis. With these values, a decision tree was applied to three classification models. Results suggest that radical Islamic text can be classified at a much higher rate of success than radical Right text."
The Scientific and Technical Revolution 5.0 and WEB 3.0 technologies create conditions for the renovation of various forms of social relations with the use of virtual and augmented reality technologies in the metaverse. According to the proposed theory, the legal regulation of social relations in the metaverse requires the development of a comprehensive electronic jurisdiction based on the latest basic legislation. The formation of legal regulation of the metaverse is a prerequisite for the need to form an electronic jurisdiction of the metaverse, which will include sectoral Metaverse Codes. The metaverse, as the electronic society of the future, does not yet have clear legal boundaries, and the task of scholars is to predict and outline with sufficient certainty the future contours of legal authority for virtual environments. Today, discussions in the scientific community about the feasibility and necessity of legal regulation of the metaverse often revolve around several key issues. First, there is the question of what legal framework should be applied in the metaverse and how conflicts between different legal systems should be resolved. Second, there is a debate about whether current regulatory bodies in the physical world have the capacity to effectively regulate the metaverse through existing laws and regulations. Third, there is the question of how to deal with offences committed in the virtual environment, and whether they should be dealt with under existing tort or criminal law, or whether a separate cross-border electronic jurisdiction should be created. The regulation of social relations in the metaverse should focus on one central goal: to clearly define the status of electronic entities, subjects and objects, to establish their rights, duties and responsibilities, and to define the different types of relations between virtual entities, subjects and objects within a given metaverse, as well as between different metaverses within an electronic jurisdiction and in a cross-border context. An essential component of the Metaverse Electronic Jurisdiction is a Metaverse Model Criminal Code that will outline the norms and offences applicable to analogue, hybrid and electronic jurisdictions. This code will define the types of socially harmful acts or crimes and the corresponding criminal penalties that will be applied within the metaverse. The formation of the electronic jurisdiction of the metaverse and the development of a Metaverse Model Criminal Code is a current scientific and legal issue.
"PurposeThe metaverse is the new buzzword. With the phenomenal growth of the metaverse comes accounting, taxation and jurisdictional challenges, which business and governments have yet to fully address. This paper aims to highlight and rationalise the lack of regulatory framework and multiplicity of jurisdictions on metaverse transactions. This paper addresses some of the complications with respect to accounting and taxation in virtual environments. Design/methodology/approachThis study relies on secondary data and emerging literature to understand the multiplicity of jurisdiction and complexity of the accounting transactions. The concept of the metaverse is rapidly evolving, and this study uses extant literature to provide the foundation for understanding the key challenges relating to accounting and taxation. FindingsConcepts of revenue recognition and deferment are challenged by the transactions in the metaverse. There are novel applications, underpinned by emerging technologies and blockchain supporting new crypto assets, such as non-fungible tokens and other decentralised finance (DeFi) tools; however, the caveats of anonymity and jurisdictional issues persist. The paper suggests that the industry must adapt to the unique reporting requirements of these assets and develop new standards for evaluating their value for financial reporting purposes. The paper emphasises the need for a case-based approach in the absence of standardised regulations for the accounting industry in the metaverse. Originality/valueThis paper adds original contributions to extant literature of the metaverse and advances ongoing debates into the accounting and taxation issues pertinent to the metaverse and DeFi."
In this paper we present a shortened version of the Cloud Forensic Maturity Model (CFMM). It composes of two inter-related parts, i.e., the Cloud Forensic Investigative Architecture (CFIA) and the Cloud Forensic Capability Matrix (CFCM). The CFMM is developed in order to create a reference model to evaluate and improve cloud forensic maturity. It is a part an on-going project, and is evaluted by a panel of experts and practitioners as a first step for further cloud forensic standardization efforts.
EU criminal policy making is a relatively new policy domain and its credibility is said to be undermined by the lack of an evidence base. Because the EU claims to pursue evidence based policy making, this justifies reviewing the mechanisms put in place to that end. To properly evaluate the evidence base in EU criminal policy making, an assessment is made of the availability of comparable crime statistics. Crime statistics, a vital data source for criminal policy making, are considered highly problematic at EU level due to (amongst other reasons) the differences in the definition of the offences. In spite of the good intentions that can be read into the repeated acknowledgement of the importance of crime statistics and the efforts to commonly define EU worthy offences, a thorough empirical analysis leads to the conclusion that we are still in search of valid EU level data with respect to the EU level offences. The EU as a policy maker does not take its responsibility to ensure the availability of the necessary comparable crime statistical data serious enough.
Network-based monitoring and intrusion detection systems generate a high number of alerts reporting the suspicious activity of IP addresses. The majority of alerts are dropped due to their low relevance, low priority, or due to high number of alerts themselves. We assume that these alerts still contain valuable information, namely, about the coordination of IP addresses. Knowledge of the coordinated IP addresses improves situational awareness and reflects the requirement of security analysts as well as automated reasoning tools to have as much contextual information as possible to make an informed decision. To validate our assumption, we introduce a novel method to discover the groups of coordinated IP addresses that exhibit a temporal correlation of their alerts. We evaluate our method on data from a real sharing platform reporting approximately 1.5 million alerts per day. The results show that our method can indeed discover groups of truly coordinated IP addresses.
The advent of the internet has unfortunately increased the scale and complexity of child exploitation material (CEM) with content increasingly moving online, forming online CEM networks through a series of websites that are hyperlinked to each other and lead consumers from one website to another. Extending on prior research focusing on examining network structure and network disruption strategies it was prudent to expand avenues to increase attack strategies. Geolocation and Whois data were utilized to map the prevalence of CEM globally. Differences in the Geolocation and Whois data were observed, suggesting both are critical pieces of information in generating accurate geo-mapping of CEM. These maps show how multi-jurisdictional attack strategies may be employed to attack these networks and remove this content.
In this study we used social network analysis of incel-related videos on YouTube to understand the recommendations, patterns, and dissemination of incel ideology on a popular multimedia platform, i.e., YouTube. Results revealed 12 distinct groups in the network (e.g., Female Hypergamy, Gynocentric Bias). Central videos in each group revealed the spread of ideological material on YouTube. Videos with the highest betweenness centrality scores were evaluated to map the pathways from groups with more innocuous video content to groups with more extremist incel-ideological content.
Recovering deleted information is one of the most important probative elements in a forensic investigation that involves a mobile phone. In this paper, we present a new tool implementing an innovative method, based on a low-level analysis, to recover deleted data from SQLite databases on Android devices, taking as an initial example text messages. The paper then proposes a generic framework for deleted data recovery that can be used with a range of SQLite databases on a variety of Android systems and devices. Indeed, although our initial aim was to recover deleted SMSs, we realized along the way that, with the appropriate changes, the initial implemented method can be applicable to the extraction of deleted information from any SQLite database file.
Artificial Intelligence is a growing field in technology that mimics the human neural network in order to deduct patterns based on specific datasets. Unlike conventional methods of programming where the code is told explicit rules, AI uses data to predict processes. However, due to AI's prediction of future behavior, it is highly susceptible to data tampering from adversaries who may flood the program with false information. Previous solutions have utilized random sampling, active learning, blockchain and human interaction in order to solve AI bias. In this paper we propose a scheme to address the AI bias by using a method of random sampling in order to mitigate the destruction done to hacked systems while maintaining prediction reliability.
This paper shares our three years of experience in conducting collaborative-based cybersecurity teaching involving industrial-expertise sharing and an authentic-learning environment. Penetration testing (pen-testing) is widely adopted in the cybersecurity industry. It requires a wide range of skillsets, including non-technical aspects, which are not easy to be acquired in a standard lecture-style setting. While the fundamentals of the skillsets could be taught separately in different modules, an integrated pen-testing module using real-world target applications will provide students with a bird's-eye view of security assessment in an authentic learning setting. There exist, however, challenges in providing a sustainable structured pen-testing module. These include the evolving industrial best practices and availability of authentic target environments. In this paper, we share our experience as well as best practices in designing and teaching a pen-testing module in our Bachelor of Computing degree program. The module unconventionally adopts a fruitful win-win collaborative paradigm. The students, guided along by professional pen-testers from the industry and academic instructors, pen-test our University's operational applications selected by the University IT Department. With the completed six semesters to date, our students have tested various applications, including our University's learning management system, student registration system, and student-hall dining system, which all manage sensitive data. We have received very positive feedback from the parties involved. This paper describes our module's rationale, involved parties and roles, class arrangements and activities, as well as grading considerations. The paper also discusses encountered issues and our adopted solutions related to University application selection, student contribution assessment, and activity arrangements during the COVID-19 outbreak. Some notes are additionally given for others who are keen to offer similar modules using the same teaching pedagogy. Our experience thus demonstrates that, while provisioning industrial collaboration and authentic learning in education needs to address several technical and administrative issues, a collaborative based teaching paradigm can work well in a sustainable manner.
The increasing demand of smart security systems has enhanced the demand for the proper identification and verification of a person. In this context, accurate estimation of age as well as proper identification of gender is highly significant. Therefore, in this work, we have implemented two separate methods with satisfactory runtime and efficiency to estimate both human age and gender using facial images. Our image processing based method involves comparison of some features extracted from the post-processed facial images of people of various age ranges followed by some edge-detection procedures, creation of binary masks and evaluation of wrinkle densities. Afterwards, thresholds were set via Naive Bayes Classification to estimate classes. For the assessment purpose, we developed a database namely BUET facial database, which consists of images of both male and female of diverse ages. For the developed database, our proposed algorithm exhibits 76.3% accuracy in the age group classification while it shows 86.6% accuracy in the gender classification. Apart from BUET facial database, our developed algorithm has also been tested in three other databases and compared its performance with the reported literature for these databases. The mean absolute error is almost below 5.0 for this work, whereas, others exceed 5.0 in most of the cases. Moreover, the proposed algorithm exhibits reasonably good accuracy under different lighting conditions of images as well. Our study would provide further insight into the choice of appropriate features for the efficient and accurate estimation of the age and the gender of a person.
In the past few years blockchains have been a major focus for security research, resulting in significant progress in the design, formalization, and analysis of blockchain protocols. However, the more general class of distributed ledgers, which includes not just blockchains but also prominent non-blockchain protocols, such as Corda and OmniLedger, cannot be covered by the state-of-the-art in the security literature yet. These distributed ledgers often break with traditional blockchain paradigms, such as block structures to store data, system-wide consensus, or global consistency. In this paper, we close this gap by proposing the first framework for defining and analyzing the security of general distributed ledgers, with an ideal distributed ledger functionality, called F-ledger, at the core of our contribution. This functionality covers not only classical blockchains but also non-blockchain distributed ledgers in a unified way. To illustrate F-ledger, we first show that the prominent ideal blockchain functionalities g(ledger) and g(PL) realize (suitable instantiations of) F-ledger, which captures their security properties. This implies that their respective implementations, including Bitcoin, Ouroboros Genesis, and Ouroboros Crypsinous, realize F-ledger as well. Secondly, we demonstrate that F-ledger is capable of precisely modeling also non-blockchain distributed ledgers by performing the first formal security analysis of such a distributed ledger, namely the prominent Corda protocol. Due to the wide spread use of Corda in industry, in particular the financial sector, this analysis is of independent interest. These results also illustrate that F-ledger not just generalizes the modular treatment of blockchains to distributed ledgers, but moreover helps to unify existing results.
Recent studies have shown the possible negative effects of children's early exposure to digital content with particular reference to screen time. In fact, excessive use of Digital Media (DM) can have important repercussions on development, learning and the quality of family life. In addition, Internet -connected devices represent the key to accessing digital subculture, the content of which can have a negative impact on children's psychological and physical development. Given the impact of DM and in particular of digital subculture on children, pre-adolescents and adolescents, we propose to broaden the concept of health by including not only the biological, psychological and social dimensions but also the digital dimension and thus the relationship with digital devices. In this work, we introduce the concept of Digital Health (DH), which refers to the well-being of all individuals, in particular of subjects of developmental age, exposed to digital devices, and we provide paediatricians with a new health procedure, called Evaluation of Digital Health (EDH). The EDH, aimed at assessing the digital habits, screen time and digital content viewed by the child, is carried out during periodic check-ups. In fact, we believe that paediatricians have a fundamental role in the protection of all-round health, including DH. The EDH would enable paediatricians, who follow growth from birth to adolescence, to prevent and/or promptly pick up on the signs of any risky digital behaviour displayed by parents and their children. From this perspective, the paediatrician may lead to the manifestation of appropriate digital behaviours, thus representing the first DH promotion service and fostering the development of digital awareness in the family.
In this paper, we introduce a concept to counter the current weakness of robust hashing with respect to cropping. We combine face detectors and robust hashing. By doing so, the detected faces become a subarea of the overall image which always can be found as long as cropping of the image does not remove the faces. As the face detection is prone to a drift effect altering size and position of the detected face, further mechanisms are needed for robust hashing. We show how face segmentation utilizing blob algorithms can be used to implement a face-based cropping robust hash algorithm.
In many developed countries, the usage of artificial intelligence (AI) and machine learning (ML) has become important in paving the future path in how data is managed and secured in the small and medium enterprises (SMEs) sector. SMEs in these developed countries have created their own cyber regimes around AI and ML. This knowledge is tested daily in how these countries' SMEs run their businesses and identify threats and attacks, based on the support structure of the individual country. Based on recent changes to the UK General Data Protection Regulation (GDPR), Brexit, and ISO standards requirements, machine learning cybersecurity (MLCS) adoption in the UK SME market has become prevalent and a good example to lean on, amongst other developed nations. Whilst MLCS has been successfully applied in many applications, including network intrusion detection systems (NIDs) worldwide, there is still a gap in the rate of adoption of MLCS techniques for UK SMEs. Other developed countries such as Spain and Australia also fall into this category, and similarities and differences to MLCS adoptions are discussed. Applications of how MLCS is applied within these SME industries are also explored. The paper investigates, using quantitative and qualitative methods, the challenges to adopting MLCS in the SME ecosystem, and how operations are managed to promote business growth. Much like security guards and policing in the real world, the virtual world is now calling on MLCS techniques to be embedded like secret service covert operations to protect data being distributed by the millions into cyberspace. This paper will use existing global research from multiple disciplines to identify gaps and opportunities for UK SME small business cyber security. This paper will also highlight barriers and reasons for low adoption rates of MLCS in SMEs and compare success stories of larger companies implementing MLCS. The methodology uses structured quantitative and qualitative survey questionnaires, distributed across an extensive participation pool directed to the SMEs' management and technical and non-technical professionals using stratify methods. Based on the analysis and findings, this study reveals that from the primary data obtained, SMEs have the appropriate cybersecurity packages in place but are not fully aware of their potential. Secondary data collection was run in parallel to better understand how these barriers and challenges emerged, and why the rate of adoption of MLCS was very low. The paper draws the conclusion that help through government policies and processes coupled together with collaboration could minimize cyber threats in combatting hackers and malicious actors in trying to stay ahead of the game. These aspirations can be reached by ensuring that those involved have been well trained and understand the importance of communication when applying appropriate safety processes and procedures. This paper also highlights important funding gaps that could help raise cyber security awareness in the form of grants, subsidies, and financial assistance through various public sector policies and training. Lastly, SMEs' lack of understanding of risks and impacts of cybercrime could lead to conflicting messages between cross-company IT and cybersecurity rules. Trying to find the right balance between this risk and impact, versus productivity impact and costs, could lead to UK SMES getting over these hurdles in this cyberspace in the quest for promoting the usage of MLCS. UK and Wales governments can use the research conducted in this paper to inform and adapt their policies to help UK SMEs become more secure from cyber-attacks and compare them to other developed countries also on the same future path.
A workflow is resilient when the unavailability of some users does not force to choose between a violation of the security policy or an early termination of the workflow. Although checking for the resiliency of a workflow is a well-studied problem, solutions usually only provide a binary answer to the problem, leaving a workflow designer with little help when the workflow is not resilient. We propose in this paper to provide instead a measure of quantitative resiliency, indicating how much a workflow is likely to terminate for a given security policy and a given user availability model. We define this notion by encoding the resiliency problem as a decision problem, reducing the finding of an optimal user-task assignment to that of solving a Markov Decision Process. We illustrate the flexibility of our encoding by considering different measures of resiliency, and we empirically analyse them, showing the existence of a trade-off between multiple aspects such as success rate, expected termination step and computation time, thus providing a toolbox that could help a workflow designer to improve or fix a workflow.
Linux is considered to be less prone to malware compared to other operating systems, and as a result Linux users rarely run anti-malware. However, many popular software applications released on other platforms cannot run natively on Linux. Wine is a popular compatibility layer for running Windows programs on Linux. The level of security risk that Wine poses to Linux users is largely undocumented. This project was conducted to assess the security implications of using Wine, and to determine if any specific types of malware or malware behavior have a significant effect on the malware being successful in Wine. Dynamic analysis (both automated and manual) was applied to 30 malware samples both in a Windows environment and Linux environment running Wine. Behavior analyzed included file system, registry, and network access, and the spawning of processes, and services. The behavior was compared to determine malware success in Wine. The study results provide evidence that Wine can pose serious security implications when used to run Windows software in a Linux environment. Five samples of Windows malware were run successfully through Wine on a Linux system. No significant relationships were discovered between the success of the malware and its high-level behavior or malware type. However, certain API calls could not be recreated in a Linux environment, and led to failure of malware to execute via Wine. This suggests that particular malware samples that utilize these API calls will never run completely successfully in a Linux environment. As a consequence, the success of some samples can be determined from observing the API calls when run within a Windows environment.
This paper describes the Registration Data Access Protocol (RDAP) with a focus on relevance to digital forensic investigators. RDAP was developed as the successor to the aging WHOIS system and is intended to eventually replace WHOIS as the authoritative source for registration information on IP addresses, Domain Names, Autonomous Systems, and more. RDAP uses a RESTful interface over HTTP and introduces a number of new features related to security, internationalization, and standardized query/response definitions. It is important for digital forensic investigators to become familiar with RDAP as it will play an increasingly important role in Internet investigations requiring the search and collection of registration data as evidence. (C) 2017 Elsevier Ltd. All rights reserved.
"To assess the efficacy of routine activity theory (RAT) for explaining phishing victimization and guide evidence-based policy, we launched two phishing attacks via a university Listserv (N = 25,875). The first email offered access to a pdf file; the second offered free concert tickets. Several interesting findings emerged demonstrating phishing victimization results from network users' routine behaviors. Students were significantly less likely to open the phishing email sharing a pdf but more likely to open the email offering free concert tickets. Moreover, students were more likely to click the malicious link embedded within the phishing email in both studies, often using mobile devices. Conversely, employees were more likely to click the link while connected to the university network, thus exposing the network to greater levels of risk. Finally, the email offering concert tickets was opened at a frequency more than double the email containing the pdf. Theoretical and policy implications are discussed."
Probabilistic and stochastic models are routinely used in performance, dependability and security evaluation, and determining appropriate values for model parameters is a long-standing problem in the practical use of such models. With the increasing emphasis on human aspects and business considerations, data collection to estimate parameter values often gets prohibitively expensive, since it may involve questionnaires, costly audits or additional monitoring and processing. In this paper we articulate a set of optimization problems related to data collection, and provide efficient algorithms to determine the optimal data collection strategy for a model. The main idea is to model the uncertainty of data sources and determine its influence on output accuracy by solving the model. This approach is particularly natural for data sources that rely on sampling, such as questionnaires or monitoring, since uncertainty can be expressed using the central limit theorem. We pay special attention to the efficiency of our optimization algorithm, using ideas inspired by importance sampling to derive optimal strategies for a range of parameter values from a single set of experiments.
"Research SummaryThis article investigates the contributing factors (or triggers) to a realization of romance fraud victimization, based on 1015 reports lodged with Scamwatch (Australian online reporting portal for fraud) between July 2018 and July 2019 (inclusive). The article examines the free text narrative of each report to propose five discernible trigger categories: further requests for money; characteristics of communications; verification checks; an offender's action(s); and being told by a third party.Policy ImplicationsBased on a comprehensive understanding of these five categories, the article advocates for broader messaging approaches to encompass financial literacy and well-being, cyberliteracy and critical thinking skills, cybersecurity practices, and respectful and healthy relationships. Leveraging the use of these wider education and awareness campaigns could improve recognition of romance fraud. Importantly, it is suggested that banks and other financial institutions are in a strong position to focus their efforts on some of these broader messages to effect positive change."
Today's requirements for visualization of geospatial data are continually rising. Visualization authoring tools provide only limited support for this purpose. The ability to create geovisualizations by non-programmers is often reliant on template editing or visualization authoring tools. However, these tools are often limited either in configuring visual parameters or interaction capabilities. In our work, we identify the main limitations of current tools. Then, we propose design requirements and describe the implementation of Geovisto-a toolkit combining capabilities of the React, Leaflet, and D3.js frameworks in order to provide tools for processing generic geospatial data and creating multilayered reusable map widgets. We demonstrate our approach on two usage scenarios from conceptually different application areas (DDoS attacks from a network monitoring system and COVID-19 pandemics open data). Finally, we discuss the pros and cons of our approach and outline our future work.
As organized criminals use instant messengers, it becomes increasingly important to obtain digital evidence from instant messengers. Recently, instant messengers apply end-to-end encryption, so all digital evidence can only be obtained from your mobile device. However, some instant messengers encrypt and store database and multimedia files, making forensic analysis of mobile devices difficult. In this paper, we present a methodology for analyzing the decryption algorithm of the messenger, and apply this methodology to Signal, Wickr, and Threema. We extracted data from both unrooted and rooted devices and performed static and dynamic analysis. As a result, we succeeded in decrypting all the encrypted database, multimedia, log, and preferences files of three messengers. We describe the decryption algorithms and disclose all decryption scripts.(c) 2022 Elsevier Ltd. All rights reserved.
An intrusion and attack detection system usually focuses on classifYing a record as either normal or abnormal. In some cases such as insider attacks, attackers rely on feedback from the attacked system, which enables them to gradually manipulate their attempts in order to avoid detection. This paper proposes the notion of accumulative manipulation that can be observed through a number of attempts accomplished by the attacker, which forms the basis of the Attacker Learning Curve (ALC). Based on a controlled experiment, we first show that the ALC for three different attack strategies are consistent between two different groups of subjects. We then define a strategy detection mechanism, which is experimentally shown to be accurate more than 70% of the time.
As the data generated on the internet exponentially increases, developing guided data collection methods become more and more essential to the research process. This paper proposes an approach to building a self-guiding web-crawler to collect data specifically from extremist websites. The guidance component of the web-crawler is achieved through the use of sentiment-based classification rules which allow the crawler to make decisions on the content of the webpage it downloads. First, content from 2,500 webpages was collected for each of the four different sentiment based classes: pro-extremist webs ites, anti-extremist websites, neutral news sites discussing extremism and finally sites with no discussion of extremism. Then parts of speech tagging was used to find the most frequent keywords in these pages. Utilizing sentiment software in conjunction with classification software a decision tree that could effectively discern which class a particular page would fall into was generated. The resulting tree showed an 80% success rate on differentiating between the four classes and a 92% success rate at classifying specifically extremist pages. This decision tree was then applied to a randomly selected sample of pages for each class. The results from the secondary test showed similar results to the primary test and hold promise for future studies using this framework.
The Internet of Things (IoT) nowadays plays an essential and critical role in our lives and is beginning to dominate Internet communications. The twenty-first century will continue to witness exponential growth and dependence on IoT systems in all aspects of life from smart homes, smart vehicles, e-health, smart cities, and smart grid to wearables, and medical devices that are becoming more microscopic due to the increase in processing ability and communications capabilities. The massive deployment of IoT devices in various domains has increased security threats to IoT devices and systems and created internet of vulnerabilities. Different bodies have proposed several fundamental architectural models for IoT. Privacy and security requirements and challenges of IoT are different from any other types of communications. Building trust between different entities and systems is the motive behind this work. This paper proposes a novel security architecture framework for IoT.
The increasing interconnectedness of countries and national critical infrastructures in today's global network society has ushered the world into what has been aptly described as an age of interdependence where each nation's security is also dependent on the actions of the other nations of the world[1]. This state of affairs emphasizes the need for the collective responsibility of states for global cybersecurity. This paper examines some strategies towards enhancing the collective responsibility of states to address the challenge of cyber terrorism. It particularly suggests the need for a state to be held accountable where its failure to establish regulatory measures to deter or prosecute cybercrimes or cyber terrorism within it territories has allowed the perpetration of such that produced trans-boundary effects in other states.
This Chapter begins by outlining the problem in defining and understanding the interrelationship between privacy and data protection law in Australia, India, Indonesia, Japan, Malaysia, Singapore, Thailand and the European Union. This Chapter will demonstrate and discuss how the concept of privacy is considered an important feature of the modern era. In other words, it is argued that there has been wide acceptance and a convergence of privacy that now transcends, government, countries, cultures religion over the Internet. This convergence of the concept of privacy, has resulted in nation states adopting to varying degrees, data protection and privacy laws. However, it will be highlighted that the current day approach needs further development and greater convergence and harmonization of data protection law and policy at the international level. This will be important as the trade in personal data continues to grow. It will be argued in this Chapter that the privacy and data protection law of these jurisdictions is far from settled. It is further argued that data protection and privacy law has two dimensions. First, is to protect personal data and information of individuals, as a human right. Second, is balancing the protection of personal data with current and future economic activity (trade) of personal data. Moreover, data protection and privacy cannot be restricted to a single country or region of the world. It is international, and has been underpinned by Internet technology and infrastructure that knows no [national] borders. Thus, these laws, while being developed by nation states for their own particular sovereign needs, the internationalization of the Internet poses significant challenges to the future law and policy in this area. They are likely to continue to be challenged and require reviewing and updating, as technology continues to change. Being a recent addition to the law, data protection is also challenging and is arguably in conflict with other areas of the law, such as intellectual property, competition, transnational commercial contract law, and cybercrime-security law. This Chapter also highlights the structure of the overall book in recognizing and responding to these differences in data protection and privacy law. It argues that data protection is a tool of Internet privacy. At the recent June 2019 meeting of the G20, the leaders' declaration called for respect of national and international regulation of data and technology. The importance of this declaration highlights the importance for governments to balance innovation with protection of personal data. To achieve this, the book reinforces the G20 leaders position, and goes a step further, by recommending that an international Model Law be developed, similar to international trade law. Also, consideration to an international treaty or convention will support any model law and go some way to closing the gaps and tensions between country data protection law. Thus, this book calls for greater legal convergence and harmonization in this emerging and complex area of law and policy. The book also identifies that personal data being afforded an intellectual property right. It also highlights the tension between data protection and competition law and cybersecurity/crime law. It will also be argued that data protection can fall within the current transnational international contract legal framework. Adopting data protection within these areas of law, provide valuable tools to strengthen the governance, control and regulation of personal data.
Social scientists have long been interested in the motives of hackers, particularly financially motivated attackers. This article analyzes web defacements, a less studied and more public form of cyberattack, in which the content of a web page is deliberately substituted with unwanted text and graphics chosen by the perpetrator. These attacks use a variety of strategies and are performed for a variety of motives, including political and ideological goals. The proliferation of such attacks has resulted in vast amounts of data that open new opportunities for qualitative and quantitative analysis. This article explores the usefulness of machine learning techniques to better understand attacker strategies and motivations. To detect overall attack patterns, this analysis utilized a sample of 40,000 images posted on defaced websites analyzed through deep machine learning methods. The approach demonstrates the potential of machine learning approaches for the study of cyberattacks, but it also reveals the considerable challenges that need to be overcome.
"The purpose of this study is to investigate the application of forensic accounting techniques in relation to fraud risk mitigation. This study employed an explanatory research design and a qualitative approach accompanied with a purposive sampling method. A primary data source was devised with a focus on the 17 licensed commercial banks registered in South Africa. By obtaining a true reflection of the situations in the banks, a conclusion was drawn following the outcome of the inferential statistical analysis. The research was conducted at the individual and organisational levels, with the bank consultants presenting their views. One hypothesis was formulated and non-parametric statistical analyses involving the use of Chi-square test, Fisher's Exact test and Spearman's correlation were carried out. The results obtained substantiate that the loopholes created as a result of non-effective application of forensic techniques are partly responsible for some cyberfraud incidents in the banking industry. There is no sufficient evidence to ascertain whether the fraud risk assessment and management in the banking industry has a relationship with the effective application of forensic accounting techniques in terms of the identified causes of cyberfraud. However, the findings establish a positive correlation between fraud risk assessment and management as it relates to forensic accounting implementation. This study provides an insight into the significance of forensic accounting applications for fraud risk mitigation. There is still a death of information regarding the forensic accounting for fraud risk mitigation; hence, it is envisaged that this study will add to the existing literature in this regard."
This articles analyses a contemporary problem, which has not been thoroughly analysed in scientific literature - Estimation of Digital Shadow Economy through a modified MIMIC model. It is the first pilot research of such type, which allows to reveal the need of deeper data analysis and data collection. Received results show, that three causal factors (internet access, and PC availability for households, non-cash payments, placement of innovative financial instruments on a market) and three indicators (non-cash transfers through internet payment platforms, volume of payments in cryptocurrencies and parcels, which are tax free at the customs) are not enough in order to perform interpretations of economic results. Additionally, the data set should cover longer-term data, however the limitation appears due to relatively short existence of innovative financial products and the lack of information accumulation about necessary data in statistical databases.
This paper starts with describing some methods of teaching the history of malware and the response to adult learners visiting an online malware museum. We also discuss some of the methodologies and tools that a student can use to see live malware processes on his/her laptop while enrolled in a malware class. The paper also discusses how to learn more about the activity of the malware, methods to remove it, and some organizations that the student may join in order to network with corporate, academic, and law enforcement personnel to get the latest information on certain types of malware and their variants.
This work addresses the definition and identification of key elements of robustness and resilience in the context of sustainable digital investigation capacity. After a review of prior work, we describe the results of a structured questionnaire that was sent to 72 law enforcement agencies and subject-matter experts in both online and oral formats (app. response rate 29%). Based on an in-depth analysis of the feedback received, key elements for robustness and resilience of digital investigation capacity are identified and discussed at the strategic and operational levels, including Digital Forensics Strategy, Forensic Discipline, Standardisation, Continuous Education and Training, Research and Development, Co-operation, and Human Resources. (C) 2015 The Authors. Published by Elsevier Ltd.
The advances of the ICT industry in recent years has led to huge popularity of Cloud Computing Services. Due to the fact that the Cloud is distributed and hosts numerous users, its use to commit crimes becomes a critical issue. Proactive cloud forensics becomes a matter of urgency: its capability to collect critical data before crimes happen, thus saving time and energy for the investigations is its primary objective. In this paper, we discuss the basis of Cloud Forensic Readiness, because we believe that such a system is of huge necessity. We begin by carefully defining Digital Forensic Readiness in the Cloud Computing context. We propose a reference architecture for a Cloud Forensic Readiness System (CFRS) together with its features, components, and challenges.
We propose in this paper a formal model for soft enforcement, where a decision-maker is influenced towards a decision, rather than forced to select that decision. This novel type of enforcement is particularly useful when the policy enforcer cannot fully control the environment of the decision-maker, as we illustrate in the context of attribute-based access control, by limiting the control over attributes. We also show that soft enforcement can improve the security of the system when the influencer is uncertain about the environment, and when neither forcing the decision-maker nor leaving them make their own selection is optimal. We define the general notion of optimal influencing policy, that takes into account both the control of the influencer and the uncertainty in the system.
This article reviews Braithwaite's recent work Crime as a Cascade Phenomenon and offers insights into its potential to transform criminological theory and practice. In his article Braithwaite argues that the cascade phenomenon is useful to understanding the spread of crime prevention interventions across time and space. The cascade concept is used as a framework to discuss the macro-level factors that affect action and inaction at the community level. Through an incorporation of self-efficacy and collective efficacy within the cascade framework Braithwaite argues that individuals and communities are the mediums through which crime and crime prevention flow. These mediums are also important factors in the sustenance and proliferation of crime prevention cascades, particularly when those cascades include elements of reintegrative approaches to the destigmatisation of criminal histories. Cascades of crime prevention and non-violence reach their greatest potential when collective efficacy is used to create socially positive rather than exclusionary practices.
Smart technologies, such as the Internet of Things (IoT), cloud computing, and artificial intelligence (AI), are being adopted in cities and transforming them into smart cities. In smart cities, various network technologies, such as the Internet and IoT, are combined to exchange real-time information, making the everyday lives of their residents more convenient. However, there is a lack of systematic research on cybersecurity and cyber forensics in smart cities. This paper presents a comprehensive review and survey of cybersecurity and cyber forensics for smart cities. We analysed 154 papers that were published from 2015 to 2022 and proposed a new framework based on a decade of related research papers. We identified four major areas and eleven sub-areas for smart cities. We found that smart homes and the IoT were the most active research areas within the cybersecurity field. Additionally, we found that research on cyber forensics for smart cities was relatively limited compared to that on cybersecurity. Since 2020, there have been many studies on the IoT (which is a technological component of smart cities) that have utilized machine learning and deep learning. Due to the transmission of large-scale data through IoT devices in smart cities, ML and DL are expected to continue playing critical roles in smart city research.
"A significant percentage of security research that is conducted suffers from common issues that prevent wide-scale adoption. Common snags of such proposed methods tend to include (i) introduction of additional nodes within the communication architecture, breaking the simplicity of the typical client-server model, or fundamental restructuring of the Internet ecosystem; (ii) significant inflation of responsibilities or duties for the user and/or server operator; and (iii) adding increased risks surrounding sensitive data during the authentication process. Many schemes seek to prevent brute-forcing attacks; they often ignore either partially or holistically the dangers of other cyber-attacks such as MiTM or replay attacks. Therefore, there is no incentive to implement such proposals, and it has become the norm instead to inflate current username/password authentication systems. These have remained standard within client-server authentication paradigms, despite insecurities stemming from poor user and server operator practices, and vulnerabilities to interception and masquerades. Besides these vulnerabilities, systems which revolve around secure authentication typically present exploits of two categories; either pitfalls which allow MiTM or replay attacks due to transmitting data for authentication constantly, or the storage of sensitive information leading to highly specific methods of data storage or facilitation, increasing chances of human error. This paper proposes a more secure method of authentication that retains the current structure of accepted paradigms, but minimizes vulnerabilities which result from the process, and does not inflate responsibilities for users or server operators. The proposed scheme uses a hybrid, layered encryption technique alongside a two-part verification process, and provides dynamic protection against interception-based cyber-attacks such as replay or MiTM attacks, without creating additional vulnerabilities for other attacks such as bruteforcing. Results show the proposed mechanism outperforms not only standardized methods, but also other schemes in terms of deployability, exploit resilience, and speed."
"The purpose of this article is to report from a qualitative Scandinavian study with the aim of shedding some light on how investigators in the Scandinavian police services perceive the use of information from social media in investigative police work. Based on 12 group interviews and 49 informants from Danish, Norwegian, and Swedish police services, we present three overarching themes mirroring the general perception amongst the interviewed investigators that: (1) information from social media is valuable in almost all types of crime investigation; (2) the use of social media information is fraught with technical pitfalls resulting in a general fear of making mistakes; (3) the legal frameworks governing digital investigative action are vague, leading to a feeling amongst the investigators of working in a grey zone. Overall, the informants express the view that this seemingly unregulated part of investigative work requires a major overhaul."
Under Sitakeboge's Stress reaction and equilibrium recovery theory to Social distribution equilibrium failure, this paper proposes a host load balancing control model to each node based on Sitakeboge model under cloud computing. Based on CloudSim simulation software to Job scheduling algorithm under Sitakeboge model and Min-Min algorithm, the result approves the first algorithm is more Effectively guarantee the node load balancing than the next.
Several fields of digital forensics (i.e. file carving, memory forensics, network forensics) require the reliable data type classification of digital fragments. Up to now, a multitude of research papers proposing new classification approaches have been published. Within this paper we comprehensively review existing classification approaches and classify them into categories. For each category, approaches are grouped based on shared commonalities. The major contribution of this paper is a novel taxonomy of existing data fragment classification approaches. We highlight progress made by previous work facilitating the identification of future research directions. Furthermore, the taxonomy can provide the foundation for future knowledge-based classification approaches.
As more businesses and users adopt cloud computing services, security vulnerabilities will be increasingly found and exploited. There are many technological and political challenges where investigation of potentially criminal incidents in the cloud are concerned. Security experts, however, must still be able to acquire and analyze data in a methodical, rigorous and forensically sound manner. This work applies the STRIDE asset-based risk assessment method to cloud computing infrastructure for the purpose of identifying and assessing an organization's ability to respond to and investigate breaches in cloud computing environments. An extension to the STRIDE risk assessment model is proposed to help organizations quickly respond to incidents while ensuring acquisition and integrity of the largest amount of digital evidence possible. Further, the proposed model allows organizations to assess the needs and capacity of their incident responders before an incident occurs.
Article 2(4) of the UN Charter provides that nation-states will refrain from the threat or use of force against the territorial integrity or political independence of any state. It is doubtful whether it will deter states from waging war in cyberspace. Cyber warfare is a perplexing and contentious issue within the ambit of international law. Discussions have focused on whether the existing rules and principles may be extended to cyberspace or whether new treaty law on cyber warfare must be drafted. Against this background the International Group of Experts drafted the Tallinn Manual on the International Law Applicable to Cyber Warfare at the invitation of the NATO Cooperative Cyber Defense Centre of Excellence. The Tallinn Manual provides rules in respect of cyber warfare. In the absence of a multilateral treaty it may be asked whether the Tallinn Manual will achieve acceptance on a global level as rules governing cyber warfare.
With the increasing popularity of digital media and the ubiquitous availability of media editing software, innocuous multimedia are easily tampered for malicious purposes. Copy-move forgery is one important category of image forgery, in which a part of an image is duplicated, and substitutes another part of the same image at a different location. Many schemes have been proposed to detect and locate the forged regions. However, these schemes fail when the copied region is affected by post-processing operations before being pasted. To rectify the problem and further improve the detection accuracy, we propose a robust copy-move forgery detection method based on dual-transform to detect such specific artifacts, in which a cascade of Radon transform (RT) and Discrete Cosine Transform (DCT) is used. It will be shown that the dual-transform coefficients well conform the efficient assumption and therefore leads to more robust feature extraction results. Experimental results demonstrate that our method is robust not only to noise contamination, blurring, and JPEG compression, but also to region scaling, rotation and flipping, respectively.
The recovery of deleted files is an important task frequently carried out by professionals in digital forensics and data recovery. When carried out without information from the file system, this process is called file carving. The techniques implemented in today's file carvers are mostly sufficient for non-fragmented files. Fragmented files, on the contrary, are not well supported. In this paper we present a general process model for the recovery of fragmented files. This model is then applied to the JPEG file format which is the de facto standard for digital photographs. Moreover, we evaluate popular open source carvers and compare them with our proposed approach.
"Lawful interception has evolved over the past decades from the target based monitoring and interception of telecomm conversations, to the monitoring and interception of packet switched communications. The lawful monitoring and interception of both telecomm and packet switched communications is regulated by law enforcement agencies, with the cooperation, under the Lawful Interception regulation and legislation, of the service providers. Social networks are also a means of communicating; but the nature of communication therein is extremely more complex, as it allows both for linear communication (one to one) and broadcasting of information (one to many/ crowd) to selected groups and communities. Social networks are a haven for criminals and insurgents. The open nature of the media provides criminals with ample access to potential victims and provides insurgents with a virtual Hyde Park, where they can openly voice their opinions, gain followers and instigate and organize disruptive activities. The nature of the communication within social networks, the ease of establishing fake identities therein, the fact that the client-server communication can be encrypted, and the huge amount of data that passes through these networks on a daily basis - however, are far from law-enforcement friendly. Furthermore, the fact that social networks are operated by commercial, usually foreign based, companies, which do not necessarily adhere to the local Lawful Interception legislation and regulation, increases the challenge of monitoring of communication with social media. The paper will discuss the technological challenges that law-enforcement agencies face when trying to monitor social networks, and the technological, regulatory and legislative provisions that can and should be put in place, by the operators of the Social Network Services and local law enforcement agencies, in order to prevent social network services from continuing to foster criminals and insurgents."
In order to efficiently manage and operate industrial-level production, an increasing number of industrial devices and critical infrastructure (CI) are now connected to the internet, exposed to malicious hackers and cyberterrorists who aim to cause significant damage to institutions and countries. Throughout the various stages of a cyber-attack, Open-source Intelligence (OSINT) tools could gather data from various publicly available platforms, and thus help hackers identify vulnerabilities and develop malware and attack strategies against targeted CI sectors. The purpose of the current study is to explore and identify the types of OSINT data that are useful for malicious individuals intending to conduct cyber-attacks against the CI industry. Applying and searching keyword queries in four open-source surface web platforms (Google, YouTube, Reddit, and Shodan), search results published between 2015 and 2020 were reviewed and qualitatively analyzed to categorize CI information that could be useful to hackers. Over 4000 results were analyzed from the open-source websites, 250 of which were found to provide information related to hacking and/or cybersecurity of CI facilities to malicious actors. Using thematic content analysis, we identified three major types of data malicious attackers could retrieve using OSINT tools: indirect reconnaissance data, proof-of-concept codes, and educational materials. The thematic results from this study reveal an increasing amount of open-source information useful for malicious attackers against industrial devices, as well as the need for programs, training, and policies required to protect and secure industrial systems and CI.
While a growing body of evidence suggests that the Internet is a key facilitator of violent extremism, research in this area has rarely incorporated former extremists' experiences with the Internet when they were involved in violent extremism. To address this gap, in-depth interviews were conducted with ten Canadian former right-wing extremists who were involved in violent racist skinhead groups, with interview questions provided by thirty Canadian law enforcement officials and ten community activists. Participants were asked about their use of the Internet and the connection between their on- and offline worlds during their involvement in the violent right-wing extremist movement. Overall, our study findings highlight the interplay between the Internet and violent extremism as well as the interactions between the on- and offline worlds of violent extremists. We conclude with a discussion of study limitations and avenues for future research.
A significant, and likely dominant, proportion of fraud is now conducted online. Police struggle to integrate this emerging reality into their processes, while expectations of this institution are high. These types of cybercrimes alter the volume and complexity of problems compared to how they previously manifested and require profound transformations of crime analysis methods to address them proactively. These developments face many difficulties, such as the quality of accessible data, the lack of existing analytical models and the need to increase police knowledge of fraud mechanisms within every level of organisations. We suggest methods to overcome these obstacles, which consist of implementing an approach integrating theories from various fields in criminology and forensic intelligence to examine the digital transformations of certain criminal processes. We take, as an example, how a generic script expressing the anatomy of an existing type of fraud can be used to interpret their new digital forms. This modelling activity both provides new insight into specific frauds and highlights relevant dimensions that are useful in orienting the development of crime analysis systems.
Handling forensic investigations gets more and more difficult as the amount of data one has to analyze is increasing continuously. A common approach for automated file identification are hash functions. The proceeding is quite simple: a tool hashes all files of a seized device and compares them against a database. Depending on the database, this allows to discard non-relevant (whitelisting) or detect suspicious files (blacklisting). One can distinguish three kinds of algorithms: (cryptographic) hash functions, bytewise approximate matching and semantic approximate matching (a. k. a perceptual hashing) where the main difference is the operation level. The latter one operates on the semantic level while both other approaches consider the byte-level. Hence, investigators have three different approaches at hand to analyze a device. First, this paper gives a comprehensive overview of existing approaches for bytewise and semantic approximate matching (for semantic we focus on images functions). Second, we compare implementations and summarize the strengths and weaknesses of all approaches. Third, we show how to integrate these functions based on a sample use case into one existing process model, the computer forensics field triage process model.
"eCrime is now the typical volume property crime in the United Kingdom impacting more of the public than traditional acquisitive crimes such as burglary and car theft (Anderson et al, 2012). It has become increasingly central to the National Security Strategy of several countries; in the United Kingdom becoming a Tier One threat. While it is apparent to some governments that cybercrimes are now as much of a 'problem' as some forms of organised crime, little is known about the perceptions of the broad network of what we call public and private sector 'eCrime controllers' in the United Kingdom. A survey of 104 members of the UK Information Assurance community garnered data on the perceptions of the eCrime problem. The results showed an association of cooperation and consumption of data sources with perceptions. It is likely that perceptions within non-specialist corporate and public domains (non-IT and Finance) will begin to change as new cooperation arrangements are introduced as part of the UK Cyber Security Strategy. These findings call for a more in-depth qualitative understanding of the cooperation between eCrime controllers and their data consumption practices. Ascertaining what shapes this cooperation (and non-cooperation) and how perceptions compare with 'actual' threats and risks is necessary if we are to better understand the 'social construction' of the problem and subsequent policy and operational outcomes."
There is increasing enthusiasm for, and recognition of, the benefits that artificial intelligence (AI) can provide to society. The emphasis has been on the positive, but AI and deep learning can be used for negative purposes. Modular Neural Networks (MNN) are capable of independent learning and have been targeted at evolutionary, complex financial systems. If the goal of an MNN were to be defined as system penetration, there is no reason why an algorithm could not run in the background. There are a resource requirements, but organised crime groups, technology companies, nation states and individuals with a curious bent are all capable of such. Ordered society and security requires a degree of certainty that systems on which society depends will remain recognisable, dependable and resilient. Under current conditions, security is difficult enough. It is suggested that limitations may be required before release of certain AI systems, in the knowledge of their potential for detriment to society. An AI system capable of independent learning, permits undefined emergent behaviours. That the results of any emergent properties may be benign or malign is irrelevant. Scientific history is littered with developments whose uses were redirected away from the benign.Such concern could be interpreted as fear of the unknown, standing in the way of technological advances. Unless society wishes to become machine-driven, the power and control of systems should be defined and limited by society, not accidently sprung on humanity or based on a ruthless logic that may drive a system to an unacceptable conclusion. Currently there are sophisticated botnet forming methods ensuring botnet persistence. If combined with the concepts of AI, there is a possibility that botnets could exist in perpetuity, with no one able to predict emergence, and no time limits on evolution. Whither cyber defence in the face of the unstoppable, increasingly intelligent, goal directed systems?
Cybersecurity adopts data mining for its ability to extract concealed and indistinct patterns in the data, such as for the needs of alert correlation. Inferring common attack patterns and rules from the alerts helps in understanding the threat landscape for the defenders and allows for the realization of cyber situational awareness, including the projection of ongoing attacks. In this article, we explore the use of data mining, namely sequential rule mining, in the analysis of intrusion detection alerts. We employed a dataset of 12 million alerts from 34 intrusion detection systems in 3 organizations gathered in an alert sharing platform, and processed it using our analytical framework. We execute the mining of sequential rules that we use to predict security events, which we utilize to create a predictive blacklist. Thus, the recipients of the data from the sharing platform will receive only a small number of alerts of events that are likely to occur instead of a large number of alerts of past events. The predictive blacklist has the size of only 3% of the raw data, and more than 60% of its entries are shown to be successful in performing accurate predictions in operational, real-world settings.
For digital forensics, eliminating the uninteresting is often more critical than finding the interesting. We define uninteresting as containing no useful information about users of a drive, a definition which applies to most criminal investigations. Matching file hash values to those in published hash sets is the standard method, but these sets have limited coverage. This work compared nine automated methods of finding additional uninteresting files: (1) frequent hash values, (2) frequent paths, (3) frequent filename-directory pairs, (4) unusually busy times for a drive, (5) unusually busy weeks for a corpus, (6) unusually frequent file sizes, (7) membership in directories containing mostly-known files, (8) known uninteresting directories, and (9) uninteresting extensions. Tests were run on an international corpus of 83.8 million files, and after removing the 25.1 % of files with hash values in the National Software Reference Library, an additional 54.7 % were eliminated that matched two of our nine criteria, few of whose hash values were in two commercial hash sets. False negatives were estimated at 0.1 % and false positives at 19.0 %. We confirmed the generality of our methods by showing a good correlation between results obtained separately on two halves of our corpus. This work provides two kinds of results: 8.4 million hash values of uninteresting files in our own corpus, and programs for finding uninteresting files on new corpora.
This work presents a method for the measurement of the accuracy of evidential artifact extraction and categorization tasks in digital forensic investigations. Instead of focusing on the measurement of accuracy and errors in the functions of digital forensic tools, this work proposes the application of information retrieval measurement techniques that allow the incorporation of errors introduced by tools and analysis processes. This method uses a 'gold standard' that is the collection of evidential objects determined by a digital investigator from suspect data with an unknown ground truth. This work proposes that the accuracy of tools and investigation processes can be evaluated compared to the derived gold standard using common precision and recall values. Two example case studies are presented showing the measurement of the accuracy of automated analysis tools as compared to an in-depth analysis by an expert. It is shown that such measurement can allow investigators to determine changes in accuracy of their processes over time, and determine if such a change is caused by their tools or knowledge.
New versions of Windows come equipped with mechanisms, such as EFS and BitLocker, which are capable of encrypting data to an industrial standard on a Personal Computer. This creates problems if the computer in question contains electronic evidence. BitLocker, for instance, provides a secure way for an individual to hide the contents of their entire disk, but as with most technologies, there are bound to be weaknesses and threats to the security of the encrypted data. It is conceivable that this technology, while appearing robust and secure, may contain flaws, which would jeopardize the integrity of the whole system. As more people encrypt their hard drives, it will become harder and harder for forensic investigators to recover data from Personal Computers. This paper documents the Bitlocker Drive Encryption System (version 2) in Windows 7. In particular it describes how to forensically decrypt and load a FAT disk or image which is bitlocked, if the keys are provided.
"This research presents two developed approaches for the forensic acquisition of an Amazon Kindle Fire HD. It describes the forensic acquisition and analysis of the Amazon Kindle Fire HD device. Two developed methods of acquisition are presented; one requiring a special cable to reflash the boot partition of the device with a forensic acquisition environment (Method A), and the other exploiting a vulnerability in the device's Android operating system (Method B). A case study is then presented showing the various digital evidence that can be extracted from the device. The results indicate that Method A is more favorable because it utilizes a general methodology that does not exploit a vulnerability that could potentially be patched by Amazon in future software updates."
Image conversion of malicious binaries, or binary visualisation, is a relevant approach in the security community. Recently, it has exceeded the role of a single-file malware analysis tool and has become a part of Intrusion Detection Systems (IDSs) thanks to the adoption of Convolutional Neural Networks (CNNs). However, there has been little effort toward image segmentation for the converted images. In this study, we propose a novel method that serves a dual purpose: (a) it enhances colour and pattern segmentation, and (b) it achieves a sparse representation of the images. According to this, we considered the R, G, and B colour values of each pixel as respective fuzzy sets. We then performed a-cuts as a defuzzification method across all pixels of the image, which converted them to sparse matrices of 0s and 1s. Our method was tested on a variety of dataset sizes and evaluated according to the detection rates of hyperparameterised ResNet50 models. Our findings demonstrated that for larger datasets, sparse representations of intelligently coloured binary images can exceed the model performance of unprocessed ones, with 93.60% accuracy, 94.48% precision, 92.60% recall, and 93.53% f-score. This is the first time that a-cuts were used in image processing and according to our results, we believe that they provide an important contribution to image processing for challenging datasets. Overall, it shows that it can become an integrated component of image-based IDS operations and other demanding real-time practices.
It remains unknown if taking commonly used preventive actions is related to identity theft. In the current study, we use a dataset featuring over 220,000 respondents to the National Crime Victimization Survey Identity Theft Supplement (NCVS ITS). The survey was conducted by the Bureau of Justice Statistics (BJS) first in 2012, then again in 2014, and once more in 2016. The findings reported here suggest that demographic variables (e.g., gender, income) and types of online activities (e.g., frequency of online shopping) are significantly related to identity theft victimization. An interesting additional finding is that among seven distinct types of preventive actions listed in the NCVS ITS survey (frequently checking credit reports, frequently changing passwords for financial accounts, employing purchase credit monitoring, shredding documents containing personal information, monitoring bank statements for suspect charges, using security software programs, and purchasing identity theft protection), shredding documents with personal information ALONE is significantly negatively related to identity theft victimization. All six other preventive actions are either positively related or unrelated to identity theft victimization. These findings generate practical implications and, most importantly, raise the question of whether some newly-fashioned preventive actions might provide better protection from identity theft protection.
Ordered society and nation states are dependent on interconnected systems, the defence of which is largely in private hands whose actions are driven by need for oligopolistic market dominance, protection of assets, and their monetisation models. This paper queries the responsibility of the nation state for the protection of itself and its citizenry. By some definitions, corporations are conducting cyberwarfare and, in cyberspace, are virtual nation states with ownership and rights over the data they hold and the intelligence it yields. The financial challenge for market dominance could drive an internecine war among the major technology corporations, and an assertion that the rights over the data they control are superior to those of the nation state. As functional monopolists, data they have acquired is not available from any other source. The intelligence from analytics exercised over that data, and the data itself is proprietary. These corporations exercise monopolist characteristics in the areas of data, information and intelligence. The aggregate value of the top 5 technology corporations, colloquially known as Big Tech is equivalent to third in projected global GDP rankings for 2021. This represents an equivalent expression of power in/over cyberspace. Cloud service providers (CSP) are often offshoots of Big Tech and have a high compound annual growth rate, thereby revealing the motivation for protection of market dominance and potential threat to user/customers. By concentrating on traditional cyber warfare and defence, there is limited consideration on policing or guarding against the rise of these virtual supranational powers driven by strict market agenda. What consideration there is regarding potential threats is driven by an economic perspective and anti-trust initiatives. Whether judged by the nation state as benign or malign, Big Tech has an impact on the nature and direction of society as currently understood and the question must be raised whether both citizens, organisations, and states need protection from it.
Security is a key challenge in the deployment and broader acceptance of cloud computing services, and existing research efforts include evaluating the effectiveness of various security solutions such as security policy implementations and technological solutions. Attacks on cloud environment may be considered from the criminological perspective, and crime theories be used to protect the cloud. This paper introduces a conceptual cloud security model utilizing the concept of situational crime prevention (SCP). Using SCP techniques, it may be possible to design process and technology-based steps to modifying the cloud computing environment to make it less attractive to crime.
An extensive study of the current practice of online payment using credit and debit cards reveals the intrinsic security challenges caused by differences in how payment sites operate.
This paper explores some of the key barriers to Open Government Data (OGD) that responsible civil servants in the UK face as they try to comply with the UK-led OGD initiative. Empirically, we provide a quantitative analysis of the resources published on the government's central OGD portal, data.gov.uk, and a unique insight into the publishing of OGD in the UK based on 22 interviews with responsible individuals at the operational level of publishing OGD. Our findings reveal that while the barriers to open government information have been substantially reduced, the barriers to open government data persist. Even the most enthusiastic responsible individuals face considerable obstacles in publishing OGD. Further, a key barrier to OGD in the UK is its impression management strategy based on its informational rather than data orientation. Due to the UK's pioneering position in the OGD initiative, these findings are relevant to understanding and improving OGD programmes at local, national and international levels. The findings may subsequently lead to evidence-based strategies and policies.
This edited volume celebrates the significant contributions of Peter Grabosky to the field of Criminology, and in particular, his work developing and adapting regulatory theory to the study of policing and security. Over the past three decades, his path-breaking theoretical and empirical research has contributed to a burgeoning literature on the myriad ways regulatory systems drive state and non-state interactions in an effort to control crime. This collection of essays showcase Grabosky's pioneering treatment of key regulatory concepts as they relate to such interactions, and illustrate how his work has been instrumental in shaping contemporary scholarship and practice around the governance of security.
This study provides a snapshot of the availability of weapons across eight omnibus or 'High Street' and 12 specialist darknet or illicit cryptomarkets between July and December 2019. Overall, 2,124 weapons were identified, of which 11 percent were found on niche markets. On all markets, weapons for sale included 1,497 handguns, 218 rifles, 41 submachine guns and 34 shotguns. Also available were ammunition (n=79), explosives (n=37) and accessories such as silencers (n=24). Omnibus markets also sold other weapons (n=70) such as tasers, pepper spray and knives, and digital products (n=112), mostly DIY weapon manuals, as well as chemical, biological, nuclear and radiological weapons (n=12). The data allowed for estimates of the cost of weapons and some description of the 215 vendors identified, 18 (8.4%) of whom were active across more than one market.
"This study sheds light on the match-fixing ecosystem, with particular focus on those entities engaged in protecting the integrity of competitions. It analyzes the characteristics of match-fixers, as perceived by some anti-match-fixing stakeholders; the known processes of match-fixing and their evolution; and the interactions among stakeholders in the match-fixing ecosystem. Results show that while anti-match-fixing actors seem to have only a fuzzy idea of match-fixers' characteristics, they appear to know quite well how fraudsters manipulate games. Meanwhile, fixers seem to have adapted their processes across time in response to harsher countermeasures according to two main strategies: layering, in which mules are used to place bets on multiple operators to conceal suspicious betting pattern algorithms, and diversification/displacement, in which fixers decide to pursue new betting options or to target minor leagues or other, less monitored sports. Some fixers did not have to adapt since in many countries police investigations remain rare and criminal sanction mild. Further research should encourage comparative perspectives beyond the European dimension, develop a more systematic effort to establish a sport fraudsters typology, and focus on non-betting-related sports manipulations."
"The Internet of Things (IoT) has experienced constant growth in the number of devices deployed and the range of applications in which such devices are used. They vary widely in size, computational power, capacity storage, and energy. The explosive growth and integration of IoT in different domains and areas of our daily lives has created an Internet of Vulnerabilities (IoV). In the rush to build and implement IoT devices, security and privacy have not been adequately addressed. IoT devices, many of which are highly constrained, are vulnerable to cyber attacks, which threaten the security and privacy of users and systems. This survey provides a comprehensive overview of IoT in regard to areas of application, security architecture frameworks, recent security and privacy issues in IoT, as well as a review of recent similar studies on IoT security and privacy. In addition, the paper presents a comprehensive taxonomy of attacks on IoT based on the three-layer architecture model; perception, network, and application layers, as well as a suggestion of the impact of these attacks on CIA objectives in representative devices, are presented. Moreover, the study proposes mitigations and countermeasures, taking a multi-faceted approach rather than a per layer approach. Open research areas are also covered to provide researchers with the most recent research urgent questions in regard to securing IoT ecosystem."
In this paper, we propose a generic specification framework for argumentation dialogue protocols in an open multi-agent system. The specification framework is based on reusable elements - dialogue templates - which are realized as an open-source implementation. We provide operational semantics and show formally how templates can be used to determine the possible dialogues. Furthermore, for open multi-agent systems we need to be able to specify peer-to-peer dialogues, where the agents themselves are in a position to know whether their dialogue actions are legal according to the protocol without relying on central entities, institutes or middleware. We prove that all protocols that can be specified in our framework are peer-to-peer suitable.
In cyber security education, hands-on training is a common type of exercise to help raise awareness and competence, and improve students' cybersecurity skills. To be able to measure the impact of the design of the particular courses, the designers need methods that can reveal hidden patterns in trainee behavior. However, the support of the designers in performing such analytic and evaluation tasks is ad-hoc and insufficient. With unsupervised machine learning methods, we designed a tool for clustering the trainee actions that can exhibit their strategies or help pinpoint flaws in the training design. By using a k-means++ algorithm, we explore clusters of trainees that unveil their specific behavior within the training sessions. The final visualization tool consists of views with scatter plots and radar charts. The former provides a two-dimensional correlation of selected trainee actions and displays their clusters. In contrast, the radar chart displays distinct clusters of trainees based on their more specific strategies or approaches when solving tasks. Through iterative training redesign, the tool can help designers identify improper training parameters and improve the quality of the courses accordingly. To evaluate the tool, we performed a qualitative evaluation of its outcomes with cybersecurity experts. The results confirm the usability of the selected methods in discovering significant trainee behavior. Our insights and recommendations can be beneficial for the design of tools for educators, even beyond cyber security.
"Criminal activities are widely facilitated by online means; so are sex crimes. Online dating, also referred to as e-dating, enables people to get in touch with potential romantic partners through digital means. Unfortunately, sex criminals also exploit online dating platforms to find victims. Several e-dating applications have been developed for computers and mobile phones, but few, if any, efforts have focused on retrieving evidence from e-dating applications. This chapter describes forensic methods for retrieving evidence from two popular e-dating applications - Tinder and Coffee Meets Bagel - by examining iPhone backups created via iTunes on Windows and Macintosh personal computers."
PurposeThis paper presents a qualitative study of penetration testing, the practice of attacking information systems to find security vulnerabilities and fixing them. The purpose of this paper is to understand whether and to what extent penetration testing can reveal various socio-organisational factors of information security in organisations. In doing so, the paper innovates theory by using Routine Activity Theory together with phenomenology of information systems concepts.Design/methodology/approachThe articulation of Routine Activity Theory and phenomenology emerged inductively from the data analysis. The data consists of 24 qualitative interviews conducted with penetration testers, analysed with thematic analysis.FindingsThe starting assumption is that penetration testers are akin to offenders in a crime situation, dealing with targets and the absence of capable guardians. A key finding is that penetration testers described their targets as an installed base, highlighting how vulnerabilities, which make a target suitable, often emerge from properties of the existing built digital environments. This includes systems that are forgotten or lack ongoing maintenance. Moreover, penetration testers highlighted that although the testing is often predicated on planned methodologies, often they resort to serendipitous practices such as improvisation.Originality/valueThis paper contributes to theory, showing how Routine Activity Theory and phenomenological concepts can work together in the study of socio-organisational factors of information security. This contribution stems from considering that much research on information security focuses on the internal actions of organisations. The study of penetration testing as a proxy of real attacks allows novel insights into socio-organisational factors of information security in organisations.
Relation extraction from text is a well-known and extensively studied topic in Natural Language Processing research. However, the implementation of relation extraction approaches in real-world application scenarios raises various methodological considerations which are often left implicit in existing research. This paper explores these considerations using a real-world dataset of user-generated police reports in Dutch. The use of linguistic features based on dependency trees is investigated, including an ablation analysis of the importance of individual features. The construction of negative examples for machine learning models is discussed, as well as the construction of a baseline model. The methodological implications of using a small dataset are discussed in terms of the design and performance of a Long Short Term Memory network as well as a Support Vector Machine. In general the models perform well, however the definition of the classification task, and in particular the construction of negative examples, are shown to have a large impact on classification accuracy and subsequently on the interpretation of the evaluation results.
Understanding commuting behavior among socioeconomic groups is essential in promoting equity across the housing and transportation system. This field is well-studied, but existing research failed to reach a complete agreement on the relationship between socioeconomic groups and commuting distance. Our study establishes a framework to model distance deterrence impacts on various socio-economic groups of commuters by adapting a commonly used gravity model approach. We apply this framework to explore commuting distance on income groups in 12 major U.S cities, delineate how the deterrence impact increases with distance, and demonstrate a significant discrepancy in it between the low- and high-income groups. Results indicate that high-income commuters are less sensitive to increasing distance. And this inequity is more severe in cities that have higher population density and more clustered job locations. Our findings provide insights for equity analysis in public transit infrastructures and land use.
The vast increase in the use of social networks and other internet-based communication tools contributed to the escalation of the problem of exchanging child pornographic material over the internet. The problem of dissemination of child pornographic material could be addressed using dedicated image detection algorithms capable of rating the inappropriateness level of images exchanged through computer networks so that images with inappropriate content involving children are blocked. However, the complexity of the image detection task coupled with the nonexistence of suitable datasets, inhibit the development of efficient algorithms that can be used for detecting offensive images containing children. To deal with the problem, we propose a methodological approach that can be used for supporting the development of child pornography detectors through the generation of synthetic datasets and through the decomposition of the task into a set of simpler tasks for which training data is available. Preliminary results show the promise of the proposed approach.
The combined success of social networking sites and smartphones has changed the way people communicate. It is now possible to publish and track contents in real time at any time and from anywhere. The large number of users on social platforms constitutes an unprecedented opportunity for attack for malicious users. Social engineering techniques, spammers, phishing and malicious attacks are examples of threats that can lead to data loss, data theft, identity theft, etc. The detection of suspicious messages or profiles is mainly covered in the literature as a binary and static classification problem. In this paper, we propose a dynamic behavioral framework for identifying suspicious profiles on social networking sites. This approach is based on three indicators: balance, energy and anomaly, synthesized from daily activities of users. We demonstrate that sensing users regularly, even on few indicators, enables suspicious behaviour to be predicted with a high level of accuracy. The low calculation costs of the approach makes it embeddable into smartphones of social networking users for inferring trust scores to their contacts.
A snapshot of the sale of fentanyl and its analogues across several popular darknet markets between 2 January and 27 March 2019 reveals the amount, types and physical forms available. Of the 127,541 unique drug listings identified, 13,135 were opioids (10.3% of all drugs), of which 1,118 (0.876% of all drugs) were fentanyl or its analogues. Between 27.3 and 39.3 kilograms of fentanyl and its derivatives were available over the period. The average price of fentanyl was A$99 per gram, while carfentanil was A$26.8 per gram. The shipping methods, cross-market operations and product specialisation of the 303 active fentanyl vendors on these darknet markets are also described.
"Utilizing interviews with 42 current and ex-street offenders, this study explores the relationship between street gang organization and robbery. Robbery type is affected by level of organization exhibited by the gang. For recreational and territorial young street gangs, robbery is opportunistic, occurring in a diffuse manner, and conducted individually, even when others are present as backup'. For criminal gangs, robbery is often planned in advance with proceeds of crime divided more evenly amongst group members. Serious Organized Crime gangs are typically more specialized; thus, robbery may often be the gang's main occupation'. For organized crime groups, robbery most often occurs in the illegitimate market, but can be aimed at legitimate and highly profitable institutions. We make sense of these findings with reference to street capital theory and present implications for future research and practice."
Choosing an optimal investment in information security is an issue most companies face these days. Which security controls to buy to protect the IT system of a company in the best way? Selecting a subset of security controls among many available ones can be seen as a resource allocation problem that should take into account conflicting objectives and constraints of the problem. In particular, the security of the system should be improved without hindering productivity, under a limited budget for buying controls. In this work, we provide several possible formulations of security controls subset selection problem as a portfolio optimization, which is well known in financial management. We propose approaches to solve them using existing single and multiobjective optimization algorithms. (C) 2015 The Authors. Published by Elsevier B.V.
In this work, a new approach to selection in multiobjective evolutionary algorithms (MOEAs) is proposed. It is based on the portfolio selection problem, which is well known in financial management. The idea of optimizing a portfolio of investments according to both expected return and risk is transferred to evolutionary selection, and fitness assignment is reinterpreted as the allocation of capital to the individuals in the population, while taking into account both individual quality and population diversity. The resulting selection procedure, which unifies parental and environmental selection, is instantiated by defining a suitable notion of (random) return for multiobjective optimization. Preliminary experiments on multiobjective multidimensional knapsack problem instances show that such a procedure is able to preserve diversity while promoting convergence towards the Pareto-optimal front.
Online users continuously come across privacy policies for the service they use. Due to the complexity and verbosity of policies, majority of the users skip the tedious task of reading the policy and accept it. Without reading and evaluating the document users risk giving up all kinds of rights to their personal data and for the most part, are unaware of the data sharing and handling process. Efforts have been made to address the complex and lengthy structure of privacy policies by creating a standardized machine-readable format of privacy policies for the web browsers to process it automatically, a repository of crowdsourced summarized versions of some privacy policies, or by using natural language processing to summarize the policies. PirvacyInterpreter is one unique tool that acknowledges human-centric factors while summarising the policy. Thus, it generates a personalised summary of the privacy policy for the user providing relevant information to appease their privacy concerns. This paper presents the conceptualization of PrivacyInterpreter and implements a proof-of-concept model using configured RoBERTa(base) model to classify a privacy policy and produce a summary based on privacy aspects that reflect users' privacy concerns.
"The 40th Anniversary Edition of Taylor, Walton and Young's New Criminology, published in 2013, opened with these words: 'The New Criminology was written at a particular time and place, it was a product of 1968 and its aftermath; a world turned upside down'. We are at a similar moment today. Several developments have been, and are turning, our 21st century world upside down. Among the most profound has been the emergence of a new earth, that the 'Anthropocene' references, and 'cyberspace', a term first used in the 1960s, which James Lovelock has recently termed a 'Novacene', a world that includes both human and artificial intelligences. We live today on an earth that is proving to be very different to the Holocene earth, our home for the past 12,000years. To appreciate the Novacene one need only think of our 'smart' phones. This world constitutes a novel domain of existence that Castells has conceived of as a terrain of 'material arrangements that allow for simultaneity of social practices without territorial contiguity' - a world of sprawling material infrastructures, that has enabled a 'space of flows', through which massive amounts of information travel. Like the Anthropocene, the Novacene has brought with it novel 'harmscapes', for example, attacks on energy systems. In this paper, we consider how criminology has responded to these harmscapes brought on by these new worlds. We identify 'lines of flight' that are emerging, as these challenges are being met by criminological thinkers who are developing the conceptual trajectories that are shaping 21st century criminologies."
This study examined the feasibility of routine activity theory in predicting online harassment victimization of people aged 15 to 30 years in the USA, Finland, Germany, and the UK. Logistic regression models controlled for socio-demographic factors, exposure to offender, target suitability, and absence of guardianship. According to the results, between 15 percent and 20 percent of respondents reported having been victims of online harassment. Of routine activity theory variables tested, only exposure to offenders was statistically significant in each of the four countries. Females were more likely to be victims than males in Finland, but not in other countries. Those with an immigrant background had a higher likelihood of being victims in Germany, but not in the other countries, whereas the protective role of guardianship was supported in the USA and Germany. Our findings indicate that while routine activity theory is a useful tool for predicting online victimization, its feasibility varied across countries.
"Since the inception of the World Wide Web, security agencies, researchers, and analysts have focused much of their attention on the sentiment found on hate-inspired web-forums. Here, one of their goals has been to detect and measure users' affects that are expressed in the forums as well as identify how users' affects change over time. Manual inspection has been one way to do this; however, as the number of discussion posts and sub-forums increase, there has been a growing need for an automated system that can assist humans in their analysis. The aim of this paper, then, is to detect and measure a number of affects expressed in written text on Stormfront. org, the most visited hate forum on the Web. To do this, we used a machine learning approach where we trained a model to recognize affects on three sub-forums: Ideology and Philosophy, For Stormfront Ladies Only, and Stormfront Ireland. The training data consisted of manual annotated data and the affects we focused on were racism, aggression, and worries. Results indicate that even though measuring affects is a subjective process, machine learning is a promising way forward to analyze and measure the presence of different affects on hate forums."
Honeypots have been a key tool in controlling and understanding digital crime for several decades. The tool has traditionally been deployed against actors who are attempting to hack into systems or as a discovery mechanism for new forms of malware. This paper presents a novel approach to using a honeypot architecture in conjunction with social networks to respond to non-technical digital crimes. The tool is presented within the context of Child Exploitation Material (CEM), and to support the goal of taking an educative approach to Internet users who are developing an interest in this material. The architecture that is presented in the paper includes multiple layers, including recruitment, obfuscation, and education. The approach does not aim to collect data to support punitive action, but to educate users, increasing their knowledge and awareness of the negative impacts of such material.
In many applications of forensic image analysis, state-of-the-art results are nowadays achieved with machine learning methods. However, concerns about their reliability and opacity raise the question whether such methods can be used in criminal investigations. So far, this question of legal compliance has hardly been discussed, also because legal regulations for machine learning methods were not defined explicitly. To this end, the European Commission recently proposed the artificial intelligence (AI) act, a regulatory framework for the trustworthy use of AI. Under the draft AI act, high-risk AI systems for use in law enforcement are permitted but subject to compliance with mandatory requirements. In this paper, we review why the use of machine learning in forensic image analysis is classified as high-risk. We then summarize the mandatory requirements for high-risk AI systems and discuss these requirements in light of two forensic applications, license plate recognition and deep fake detection. The goal of this paper is to raise awareness of the upcoming legal requirements and to point out avenues for future research.
With the increasing complexity of software permeating critical domains such as autonomous driving, new challenges are emerging in the ways the engineering of these systems needs to be rethought. Autonomous driving is expected to continue gradually overtaking all critical driving functions, which is adding to the complexity of the certification of autonomous driving systems. As a response, certification authorities have already started introducing strategies for the certification of autonomous vehicles and their software. But even with these new approaches, the certification procedures are not fully catching up with the dynamism and unpredictability of future autonomous systems, and thus may not necessarily guarantee compliance with all requirements imposed on these systems. In this paper, we identified a number of issues with the proposed certification strategies, which may impact the systems substantially. For instance, we emphasize the lack of adequate reflection on software changes occurring in constantly changing systems, or low support for systems' cooperation needed for the management of coordinated moves. Other shortcomings concern the narrow focus of the awarded certification by neglecting aspects such as the ethical behaviour of autonomous software systems. The contribution of this paper is threefold. First, we discuss the motivation for the need to modify the current certification processes for autonomous driving systems. Second, we analyze current international standards used in the certification processes towards requirements derived from the requirements laid on dynamic software ecosystems and autonomous systems themselves. Third, we outline a concept for incorporating the missing parts into the certification procedure.
